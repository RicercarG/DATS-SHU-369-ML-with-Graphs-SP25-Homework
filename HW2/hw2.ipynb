{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATS369: Machine Learning with Graphs Homework 2\n",
    "Due: Mar 29 23:59, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, let's dive into graph neural networks! Try running your code on NYU Greene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device at the very beginning\n",
    "import torch \n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Node Classification based on GNNs (65 points)\n",
    "## 1.1 Node Feature Preparation (5 points)\n",
    "Show your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricercar/miniconda3/envs/gnn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# for CLIP \n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "def embed_clip(batch, model=clip_model, processor=clip_processor):\n",
    "    inputs = processor(images=batch, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embs = model.get_image_features(**inputs)\n",
    "    \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN-based Image Classifier \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # import files from HW1\n",
    "\n",
    "from HW1.Q3 import CNN, MVModel\n",
    "\n",
    "class CNNFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.features = None \n",
    "    \n",
    "    def hook(self, module, input, output):\n",
    "        self.features = output \n",
    "\n",
    "extractor = CNNFeatureExtractor()\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "ckpt_path = os.path.join(parent_dir, \"HW1/checkpoints/cnn-e30-bs32-lr0.0001/epoch=29-val_acc=0.18.ckpt\")\n",
    "cnn_model = MVModel.load_from_checkpoint(ckpt_path, model_type='cnn')\n",
    "\n",
    "pen_layer = cnn_model.model.model[-2] # locate thhe penultimate layer\n",
    "pen_layer.register_forward_hook(extractor.hook)\n",
    "\n",
    "cnn_model = cnn_model.to(device)\n",
    "    \n",
    "def embed_cnn(batch, model=cnn_model, extractor=extractor):\n",
    "    with torch.no_grad():\n",
    "        _ = model(batch)\n",
    "        embs = extractor.features\n",
    "    \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are loaded from cache\n"
     ]
    }
   ],
   "source": [
    "# get the features\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "feature_cache_dir = \"features\"\n",
    "os.makedirs(feature_cache_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    features_clip = torch.load(os.path.join(feature_cache_dir, \"clip.pt\"))\n",
    "    features_cnn = torch.load(os.path.join(feature_cache_dir, \"cnn.pt\"))\n",
    "    print(\"Features are loaded from cache\")\n",
    "except:\n",
    "    # load the data and compute features\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    images_dir = os.path.join(parent_dir, \"HW1/data/sub_images\")\n",
    "\n",
    "    image_names = sorted(os.listdir(images_dir), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor()\n",
    "                            ])\n",
    "\n",
    "    images = []\n",
    "    for image_name in tqdm(image_names, desc=\"Loading images\"):\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        image = transform(image)\n",
    "        images.append(image)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    features_clip = []\n",
    "    features_cnn = []\n",
    "\n",
    "    for batch_idx in tqdm(range(0, len(images), batch_size), desc=\"Getting feature embeddings\"):\n",
    "        batch = images[batch_idx: batch_idx+batch_size]\n",
    "        clip = embed_clip(batch)\n",
    "        cnn = embed_cnn(batch)\n",
    "\n",
    "        features_clip.append(clip)\n",
    "        features_cnn.append(cnn)\n",
    "\n",
    "    features_clip = torch.cat(features_clip, dim=0)\n",
    "    features_cnn = torch.cat(features_cnn, dim=0)\n",
    "\n",
    "    torch.save(features_clip, os.path.join(feature_cache_dir, \"clip.pt\"))\n",
    "    torch.save(features_cnn, os.path.join(feature_cache_dir, \"cnn.pt\"))\n",
    "    print(\"Features saved to cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Implementing GCN for Product Classification (20 points)\n",
    "## Code implementation (15 points)\n",
    "Given the adjacency matrix $A$ and node feature matrix $X$, the layer-wise GCN updating rule is: $H=\\text{ReLU}(AXW)$. $W$ is the model parameter, $H$ represents the hidden representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to install the following packages\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"GCN layer: refer to https://github.com/tkipf/pygcn\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    in_features : input feature dimension\n",
    "    out_features : output dimension\n",
    "    with_bias: bias term \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, with_bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if with_bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self): \n",
    "        \"\"\"param initialization\"\"\"\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"forward pass, AXW\n",
    "        x: node feature tensor\n",
    "        adj: sparse tensor\n",
    "        \"\"\"\n",
    "\n",
    "        ######################################\n",
    "        # write your code here (don't forget to include the bias term with self.bias argument)\n",
    "\n",
    "        h = torch.mm(x, self.weight)\n",
    "        output = torch.spmm(adj, h)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "\n",
    "        ######################################\n",
    "        return output # AXW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\" 2 GNN layers\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    nfeat : input dimension\n",
    "    nhid : hidden dimension\n",
    "    nclass : output dimension, or number of class\n",
    "    dropout : dropout ratio\n",
    "    with_bias: bias term\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.nfeat = nfeat\n",
    "        self.nclass = nclass\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid, with_bias=with_bias)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass, with_bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        ######################################\n",
    "        # write your code here \n",
    "        # hint: you can use F.dropout() function to define dropout \n",
    "        h = F.relu(self.gc1(x, adj))\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        predicts = self.gc2(h, adj)\n",
    "\n",
    "        ######################################\n",
    "        return F.log_softmax(predicts, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, dense_to_sparse\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch \n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "dataset_dir = os.path.join(parent_dir, \"HW1/data/Movies.pt\")\n",
    "\n",
    "graph = torch.load(dataset_dir, weights_only=False) # create a \"Movies\" folder and download the Movies dataset from the google drive. \n",
    "adj = graph['adj']\n",
    "label = graph['label']\n",
    "train_index = graph['train']\n",
    "val_index = graph['val']\n",
    "test_index = graph['test']\n",
    "\n",
    "# # hint: to use torch_geometric package, the graph input should follow the standard formats. You can try with \"Cora\" dataset below to understand how to transform Amazon Movies to the required format\n",
    "# dataset = Planetoid(root='./data', name='Cora') # dataset download automatically\n",
    "# data = dataset[0]\n",
    "# adj = to_scipy_sparse_matrix(data.edge_index)\n",
    "# features = data.x\n",
    "# labels = data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(adj).nonzero().t().contiguous()\n",
    "\n",
    "data_clip = Data(\n",
    "    x=features_clip,\n",
    "    y=torch.tensor(label),\n",
    "    edge_index=edge_index,\n",
    "    train_mask=train_index,\n",
    "    val_mask=val_index,\n",
    "    test_mask=test_index\n",
    ")\n",
    "\n",
    "data_cnn= Data(\n",
    "    x=features_cnn,\n",
    "    y=torch.tensor(label),\n",
    "    edge_index=edge_index,\n",
    "    train_mask=train_index,\n",
    "    val_mask=val_index,\n",
    "    test_mask=test_index\n",
    ")\n",
    "\n",
    "dataset = {\n",
    "    \"data_clip\": data_clip,\n",
    "    \"data_cnn\": data_cnn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj = coo_matrix(adj, dtype=np.float32) # turn our adj to scipy sparse matrix\n",
    "# labels = torch.tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build GCN, we need to normalize the adj ï¼š \n",
    "\n",
    "$A \\leftarrow A + I$\n",
    "\n",
    "$\\hat{A}= D^{-1/2}{A}D^{-1/2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"normalization: A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
    "    \"\"\"\n",
    "    mx = mx + sp.eye(mx.shape[0]) # A = A + I\n",
    "    rowsum = np.array(mx.sum(1)) \n",
    "    r_inv = np.power(rowsum, -1/2).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0. \n",
    "    r_mat_inv = sp.diags(r_inv) \n",
    "    ######################################\n",
    "    # write your code here \n",
    "\n",
    "    mx = r_mat_inv @ mx @ r_mat_inv\n",
    "    ######################################\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"convert scipy.sparse matrix to torch sparse tensor,\n",
    "       we need three arugments, row, col and data, coresponding to\n",
    "       edge_index and edge_weight\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    sparserow = torch.LongTensor(sparse_mx.row).unsqueeze(1) # \n",
    "    sparsecol = torch.LongTensor(sparse_mx.col).unsqueeze(1) # \n",
    "    sparseconcat = torch.cat((sparserow, sparsecol), 1) # \n",
    "    sparsedata = torch.FloatTensor(sparse_mx.data)  # \n",
    "    return torch.sparse.FloatTensor(sparseconcat.t(), sparsedata, torch.Size(sparse_mx.shape)) # construct sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, data, adj, lr=0.01, weight_decay=5e-4, epochs=200):\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     labels = data.y\n",
    "#     train_mask = data.train_mask\n",
    "\n",
    "#     for i in range(epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data.x, adj)\n",
    "\n",
    "#         loss = F.nll_loss(output[train_mask], labels[train_mask]) # compute training loss\n",
    "\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if i % 10 == 0:\n",
    "#             print('Epoch {}, training loss: {}'.format(i, loss.item()))\n",
    "\n",
    "def train(model, data, adj, lr=0.01, weight_decay=5e-4, epochs=200, patience=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    labels = data.y\n",
    "    train_mask = data.train_mask \n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_val_epoch = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, adj)\n",
    "\n",
    "        loss = F.nll_loss(output[train_mask], labels[train_mask]) # compute training loss\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validate each epoch\n",
    "        _, _, acc_val, loss_val = validation(model, data, adj, verbose=False)\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {i}, training loss: {loss.item()}, val loss: {loss_val}, val_acc: {acc_val}\")\n",
    "            # print('Epoch {}, training loss: {}, '.format(i, loss.item()))\n",
    "\n",
    "        # early stopping\n",
    "        if acc_val > best_val_acc:\n",
    "            best_val_acc = acc_val\n",
    "            best_val_epoch = i\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {best_val_epoch} with validation accuracy {best_val_acc:4f}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None: \n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, adj):\n",
    "    \"\"\"test the performance\"\"\"\n",
    "    model.eval() # eval() \n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model(data.x, adj) #  \n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) #  \n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() #  \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(model, data, adj, verbose=True):\n",
    "    \"\"\"add a validation for early stopping\"\"\"\n",
    "    model.eval() # eval() \n",
    "    val_mask = data.val_mask\n",
    "    labels = data.y \n",
    "    output = model(data.x, adj) #  \n",
    "    loss_val = F.nll_loss(output[val_mask], labels[val_mask])\n",
    "    preds = output[val_mask].argmax(1) #  \n",
    "    acc_val= preds.eq(labels[val_mask]).cpu().numpy().mean() #  \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Val set results:\",\n",
    "            \"loss= {:.4f}\".format(loss_val.item()),\n",
    "            \"accuracy= {:.4f}\".format(acc_val))\n",
    "    return preds, output, acc_val.item(), loss_val.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GCN model and test your model and report your results (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN with data_clip\n",
      "Epoch 0, training loss: 35.99372100830078, val loss: 30.00778579711914, val_acc: 0.04\n",
      "Epoch 10, training loss: 8.85456657409668, val loss: 7.337459087371826, val_acc: 0.04363636363636364\n",
      "Epoch 20, training loss: 3.043886661529541, val loss: 3.012892007827759, val_acc: 0.030303030303030304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4c/h1pj207x7bx25b8wfdz4fg8w0000gn/T/ipykernel_97954/2683752846.py:11: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:646.)\n",
      "  return torch.sparse.FloatTensor(sparseconcat.t(), sparsedata, torch.Size(sparse_mx.shape)) # construct sparse tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, training loss: 3.004514455795288, val loss: 3.0093605518341064, val_acc: 0.05454545454545454\n",
      "Epoch 40, training loss: 3.0041542053222656, val loss: 3.008765697479248, val_acc: 0.05454545454545454\n",
      "Epoch 50, training loss: 3.0033345222473145, val loss: 3.008046865463257, val_acc: 0.05454545454545454\n",
      "Epoch 60, training loss: 3.0030012130737305, val loss: 3.0072085857391357, val_acc: 0.05454545454545454\n",
      "Epoch 70, training loss: 3.0028178691864014, val loss: 3.0068724155426025, val_acc: 0.05454545454545454\n",
      "Epoch 80, training loss: 3.0021421909332275, val loss: 3.0061416625976562, val_acc: 0.05454545454545454\n",
      "Epoch 90, training loss: 3.0013113021850586, val loss: 3.0055134296417236, val_acc: 0.05454545454545454\n",
      "Epoch 100, training loss: 3.001465082168579, val loss: 3.004864454269409, val_acc: 0.05454545454545454\n",
      "Epoch 110, training loss: 2.9988272190093994, val loss: 3.0029895305633545, val_acc: 0.05454545454545454\n",
      "Epoch 120, training loss: 3.0006885528564453, val loss: 3.0035643577575684, val_acc: 0.05454545454545454\n",
      "Epoch 130, training loss: 3.000560760498047, val loss: 3.0029988288879395, val_acc: 0.05454545454545454\n",
      "Epoch 140, training loss: 2.9999141693115234, val loss: 3.00235915184021, val_acc: 0.05454545454545454\n",
      "Epoch 150, training loss: 2.998140811920166, val loss: 3.0013270378112793, val_acc: 0.05454545454545454\n",
      "Epoch 160, training loss: 2.994719982147217, val loss: 2.9997105598449707, val_acc: 0.055757575757575756\n",
      "Epoch 170, training loss: 2.992558002471924, val loss: 2.995645761489868, val_acc: 0.05333333333333334\n",
      "Epoch 180, training loss: 2.9869649410247803, val loss: 2.991973638534546, val_acc: 0.05454545454545454\n",
      "Epoch 190, training loss: 2.979768753051758, val loss: 2.9863338470458984, val_acc: 0.05454545454545454\n",
      "Epoch 200, training loss: 2.9770913124084473, val loss: 2.9834327697753906, val_acc: 0.05454545454545454\n",
      "Epoch 210, training loss: 2.9666659832000732, val loss: 2.9861207008361816, val_acc: 0.06666666666666667\n",
      "Epoch 220, training loss: 2.965203285217285, val loss: 2.9767305850982666, val_acc: 0.06181818181818182\n",
      "Epoch 230, training loss: 2.972184419631958, val loss: 2.9745161533355713, val_acc: 0.06060606060606061\n",
      "Epoch 240, training loss: 2.9604220390319824, val loss: 2.971601963043213, val_acc: 0.06303030303030303\n",
      "Epoch 250, training loss: 2.9640212059020996, val loss: 2.9761502742767334, val_acc: 0.0703030303030303\n",
      "Epoch 260, training loss: 2.955702066421509, val loss: 2.9695472717285156, val_acc: 0.06303030303030303\n",
      "Epoch 270, training loss: 2.955936908721924, val loss: 2.9683830738067627, val_acc: 0.06303030303030303\n",
      "Epoch 280, training loss: 2.951077938079834, val loss: 2.969681978225708, val_acc: 0.06909090909090909\n",
      "Epoch 290, training loss: 2.9644408226013184, val loss: 2.9681243896484375, val_acc: 0.06909090909090909\n",
      "Epoch 300, training loss: 2.9577019214630127, val loss: 2.9713213443756104, val_acc: 0.06909090909090909\n",
      "Epoch 310, training loss: 2.952444314956665, val loss: 2.964118480682373, val_acc: 0.06424242424242424\n",
      "Epoch 320, training loss: 2.9531188011169434, val loss: 2.9628818035125732, val_acc: 0.06303030303030303\n",
      "Epoch 330, training loss: 2.9688167572021484, val loss: 2.975344181060791, val_acc: 0.03878787878787879\n",
      "Epoch 340, training loss: 2.9556286334991455, val loss: 2.9678704738616943, val_acc: 0.04\n",
      "Epoch 350, training loss: 2.9484195709228516, val loss: 2.961970806121826, val_acc: 0.055757575757575756\n",
      "Epoch 360, training loss: 2.970247983932495, val loss: 2.9641032218933105, val_acc: 0.05333333333333334\n",
      "Epoch 370, training loss: 2.9457380771636963, val loss: 2.9728522300720215, val_acc: 0.04606060606060606\n",
      "Epoch 380, training loss: 2.9522392749786377, val loss: 2.9620420932769775, val_acc: 0.05090909090909091\n",
      "Epoch 390, training loss: 2.9562501907348633, val loss: 2.963949203491211, val_acc: 0.04242424242424243\n",
      "Epoch 400, training loss: 2.9535939693450928, val loss: 2.961426258087158, val_acc: 0.044848484848484846\n",
      "Epoch 410, training loss: 2.9606986045837402, val loss: 2.9617185592651367, val_acc: 0.04242424242424243\n",
      "Epoch 420, training loss: 2.9456374645233154, val loss: 2.962625503540039, val_acc: 0.041212121212121214\n",
      "Epoch 430, training loss: 2.9545555114746094, val loss: 2.964817523956299, val_acc: 0.041212121212121214\n",
      "Epoch 440, training loss: 2.948997974395752, val loss: 2.9617621898651123, val_acc: 0.044848484848484846\n",
      "Epoch 450, training loss: 2.9445083141326904, val loss: 2.9618592262268066, val_acc: 0.04606060606060606\n",
      "Epoch 460, training loss: 2.945857286453247, val loss: 2.9634034633636475, val_acc: 0.04363636363636364\n",
      "Epoch 470, training loss: 2.9519290924072266, val loss: 2.9639735221862793, val_acc: 0.04606060606060606\n",
      "Epoch 480, training loss: 2.9500346183776855, val loss: 2.9644856452941895, val_acc: 0.04606060606060606\n",
      "Epoch 490, training loss: 2.9524595737457275, val loss: 2.9640884399414062, val_acc: 0.04363636363636364\n",
      "Epoch 500, training loss: 2.9325621128082275, val loss: 2.963040590286255, val_acc: 0.04727272727272727\n",
      "Epoch 510, training loss: 2.9398746490478516, val loss: 2.9621973037719727, val_acc: 0.04363636363636364\n",
      "Epoch 520, training loss: 2.940617561340332, val loss: 2.961962938308716, val_acc: 0.04606060606060606\n",
      "Epoch 530, training loss: 2.939358949661255, val loss: 2.965426206588745, val_acc: 0.052121212121212124\n",
      "Epoch 540, training loss: 2.9448862075805664, val loss: 2.9608747959136963, val_acc: 0.05696969696969697\n",
      "Epoch 550, training loss: 2.9495604038238525, val loss: 2.963805675506592, val_acc: 0.05333333333333334\n",
      "Epoch 560, training loss: 2.938549280166626, val loss: 2.9617340564727783, val_acc: 0.059393939393939395\n",
      "Epoch 570, training loss: 2.952089548110962, val loss: 2.9634149074554443, val_acc: 0.048484848484848485\n",
      "Epoch 580, training loss: 2.9499309062957764, val loss: 2.9632394313812256, val_acc: 0.04606060606060606\n",
      "Epoch 590, training loss: 2.966223955154419, val loss: 2.9645638465881348, val_acc: 0.05454545454545454\n",
      "Epoch 600, training loss: 2.9392411708831787, val loss: 2.9649577140808105, val_acc: 0.05090909090909091\n",
      "Epoch 610, training loss: 2.940985918045044, val loss: 2.9653749465942383, val_acc: 0.05454545454545454\n",
      "Epoch 620, training loss: 2.9404423236846924, val loss: 2.963068723678589, val_acc: 0.05090909090909091\n",
      "Epoch 630, training loss: 2.9546890258789062, val loss: 2.9641432762145996, val_acc: 0.05090909090909091\n",
      "Epoch 640, training loss: 2.9471371173858643, val loss: 2.965269088745117, val_acc: 0.041212121212121214\n",
      "Epoch 650, training loss: 2.934297800064087, val loss: 2.961474657058716, val_acc: 0.059393939393939395\n",
      "Epoch 660, training loss: 2.9610469341278076, val loss: 2.964501142501831, val_acc: 0.06060606060606061\n",
      "Epoch 670, training loss: 2.9410531520843506, val loss: 2.963008165359497, val_acc: 0.06424242424242424\n",
      "Epoch 680, training loss: 2.9510042667388916, val loss: 2.9653432369232178, val_acc: 0.04363636363636364\n",
      "Epoch 690, training loss: 2.945067882537842, val loss: 2.962733745574951, val_acc: 0.05090909090909091\n",
      "Epoch 700, training loss: 2.9516868591308594, val loss: 2.962170362472534, val_acc: 0.05696969696969697\n",
      "Epoch 710, training loss: 2.9322052001953125, val loss: 2.962153434753418, val_acc: 0.0496969696969697\n",
      "Epoch 720, training loss: 2.936349630355835, val loss: 2.960488796234131, val_acc: 0.05696969696969697\n",
      "Epoch 730, training loss: 2.9431545734405518, val loss: 2.9602408409118652, val_acc: 0.05818181818181818\n",
      "Epoch 740, training loss: 2.938013792037964, val loss: 2.960253953933716, val_acc: 0.06060606060606061\n",
      "Epoch 750, training loss: 2.937103271484375, val loss: 2.9624948501586914, val_acc: 0.05696969696969697\n",
      "Epoch 760, training loss: 2.9415223598480225, val loss: 2.9629454612731934, val_acc: 0.05818181818181818\n",
      "Epoch 770, training loss: 2.933424472808838, val loss: 2.96209454536438, val_acc: 0.059393939393939395\n",
      "Epoch 780, training loss: 2.941789388656616, val loss: 2.9618635177612305, val_acc: 0.05696969696969697\n",
      "Epoch 790, training loss: 2.946645498275757, val loss: 2.961569309234619, val_acc: 0.05696969696969697\n",
      "Epoch 800, training loss: 2.9319677352905273, val loss: 2.962364673614502, val_acc: 0.05818181818181818\n",
      "Epoch 810, training loss: 2.930040121078491, val loss: 2.962968349456787, val_acc: 0.06060606060606061\n",
      "Epoch 820, training loss: 2.939512252807617, val loss: 2.964402437210083, val_acc: 0.05090909090909091\n",
      "Epoch 830, training loss: 2.9425230026245117, val loss: 2.9632680416107178, val_acc: 0.052121212121212124\n",
      "Epoch 840, training loss: 2.939052104949951, val loss: 2.9648537635803223, val_acc: 0.05454545454545454\n",
      "Epoch 850, training loss: 2.9394264221191406, val loss: 2.963954448699951, val_acc: 0.06060606060606061\n",
      "Epoch 860, training loss: 2.9358248710632324, val loss: 2.964596748352051, val_acc: 0.05818181818181818\n",
      "Epoch 870, training loss: 2.949007749557495, val loss: 2.9643657207489014, val_acc: 0.05333333333333334\n",
      "Epoch 880, training loss: 2.932555913925171, val loss: 2.9631409645080566, val_acc: 0.052121212121212124\n",
      "Epoch 890, training loss: 2.9311444759368896, val loss: 2.96345853805542, val_acc: 0.055757575757575756\n",
      "Epoch 900, training loss: 2.9363858699798584, val loss: 2.9643919467926025, val_acc: 0.05454545454545454\n",
      "Epoch 910, training loss: 2.9394874572753906, val loss: 2.9643666744232178, val_acc: 0.05333333333333334\n",
      "Epoch 920, training loss: 2.935570478439331, val loss: 2.963824510574341, val_acc: 0.055757575757575756\n",
      "Epoch 930, training loss: 2.948131799697876, val loss: 2.9638290405273438, val_acc: 0.05696969696969697\n",
      "Epoch 940, training loss: 2.9400458335876465, val loss: 2.9636926651000977, val_acc: 0.05696969696969697\n",
      "Epoch 950, training loss: 2.9331560134887695, val loss: 2.9634218215942383, val_acc: 0.059393939393939395\n",
      "Epoch 960, training loss: 2.9390180110931396, val loss: 2.963070869445801, val_acc: 0.052121212121212124\n",
      "Epoch 970, training loss: 2.9364380836486816, val loss: 2.96321439743042, val_acc: 0.05333333333333334\n",
      "Epoch 980, training loss: 2.942755699157715, val loss: 2.966216802597046, val_acc: 0.05818181818181818\n",
      "Epoch 990, training loss: 2.934185743331909, val loss: 2.9638311862945557, val_acc: 0.052121212121212124\n",
      "Epoch 1000, training loss: 2.940988302230835, val loss: 2.9641432762145996, val_acc: 0.05333333333333334\n",
      "Epoch 1010, training loss: 2.923760414123535, val loss: 2.9671883583068848, val_acc: 0.0496969696969697\n",
      "Epoch 1020, training loss: 2.9374451637268066, val loss: 2.970395565032959, val_acc: 0.05333333333333334\n",
      "Epoch 1030, training loss: 2.9518890380859375, val loss: 2.975660800933838, val_acc: 0.04606060606060606\n",
      "Epoch 1040, training loss: 2.9318768978118896, val loss: 2.9646356105804443, val_acc: 0.05090909090909091\n",
      "Epoch 1050, training loss: 2.9329423904418945, val loss: 2.961012363433838, val_acc: 0.05818181818181818\n",
      "Epoch 1060, training loss: 2.9321353435516357, val loss: 2.9616355895996094, val_acc: 0.059393939393939395\n",
      "Epoch 1070, training loss: 2.9138994216918945, val loss: 2.9639501571655273, val_acc: 0.06424242424242424\n",
      "Epoch 1080, training loss: 2.9348859786987305, val loss: 2.9608564376831055, val_acc: 0.05818181818181818\n",
      "Epoch 1090, training loss: 2.9271891117095947, val loss: 2.9629287719726562, val_acc: 0.04606060606060606\n",
      "Epoch 1100, training loss: 2.9307937622070312, val loss: 2.960383892059326, val_acc: 0.052121212121212124\n",
      "Epoch 1110, training loss: 2.9253554344177246, val loss: 2.9594624042510986, val_acc: 0.05696969696969697\n",
      "Epoch 1120, training loss: 2.9335384368896484, val loss: 2.957857847213745, val_acc: 0.06424242424242424\n",
      "Epoch 1130, training loss: 2.929105043411255, val loss: 2.9579811096191406, val_acc: 0.06060606060606061\n",
      "Epoch 1140, training loss: 2.9179913997650146, val loss: 2.956943988800049, val_acc: 0.05818181818181818\n",
      "Epoch 1150, training loss: 2.9260268211364746, val loss: 2.9562180042266846, val_acc: 0.06181818181818182\n",
      "Epoch 1160, training loss: 2.920006275177002, val loss: 2.9553442001342773, val_acc: 0.06787878787878789\n",
      "Epoch 1170, training loss: 2.9252798557281494, val loss: 2.953697919845581, val_acc: 0.06909090909090909\n",
      "Epoch 1180, training loss: 2.916142463684082, val loss: 2.9526984691619873, val_acc: 0.06303030303030303\n",
      "Epoch 1190, training loss: 2.931365966796875, val loss: 2.951390266418457, val_acc: 0.06909090909090909\n",
      "Epoch 1200, training loss: 2.919646739959717, val loss: 2.952155590057373, val_acc: 0.05696969696969697\n",
      "Epoch 1210, training loss: 2.931018352508545, val loss: 2.948699712753296, val_acc: 0.06303030303030303\n",
      "Epoch 1220, training loss: 2.9261093139648438, val loss: 2.956908941268921, val_acc: 0.05818181818181818\n",
      "Epoch 1230, training loss: 2.9240705966949463, val loss: 2.9487545490264893, val_acc: 0.06303030303030303\n",
      "Epoch 1240, training loss: 2.9278929233551025, val loss: 2.9485037326812744, val_acc: 0.059393939393939395\n",
      "Epoch 1250, training loss: 2.909531593322754, val loss: 2.9454147815704346, val_acc: 0.06181818181818182\n",
      "Epoch 1260, training loss: 2.9181342124938965, val loss: 2.943467140197754, val_acc: 0.06303030303030303\n",
      "Epoch 1270, training loss: 2.912766695022583, val loss: 2.9424965381622314, val_acc: 0.059393939393939395\n",
      "Epoch 1280, training loss: 2.9168760776519775, val loss: 2.9431726932525635, val_acc: 0.06303030303030303\n",
      "Epoch 1290, training loss: 2.9194607734680176, val loss: 2.9458625316619873, val_acc: 0.06060606060606061\n",
      "Epoch 1300, training loss: 2.9110491275787354, val loss: 2.94020414352417, val_acc: 0.06060606060606061\n",
      "Early stopping at epoch 302 with validation accuracy 0.073939\n",
      "====================\n",
      "Training GCN with data_cnn\n",
      "Epoch 0, training loss: 683.8055419921875, val loss: 648.2606201171875, val_acc: 0.059393939393939395\n",
      "Epoch 10, training loss: 297.9418029785156, val loss: 271.69964599609375, val_acc: 0.05333333333333334\n",
      "Epoch 20, training loss: 113.06562042236328, val loss: 97.32575988769531, val_acc: 0.055757575757575756\n",
      "Epoch 30, training loss: 27.03208351135254, val loss: 21.694602966308594, val_acc: 0.05696969696969697\n",
      "Epoch 40, training loss: 3.3580076694488525, val loss: 3.2174997329711914, val_acc: 0.05333333333333334\n",
      "Epoch 50, training loss: 3.0537006855010986, val loss: 3.041679620742798, val_acc: 0.05818181818181818\n",
      "Epoch 60, training loss: 3.0192370414733887, val loss: 3.0331642627716064, val_acc: 0.05818181818181818\n",
      "Epoch 70, training loss: 3.011688470840454, val loss: 3.0326316356658936, val_acc: 0.05818181818181818\n",
      "Epoch 80, training loss: 3.0150043964385986, val loss: 3.033137321472168, val_acc: 0.05818181818181818\n",
      "Epoch 90, training loss: 3.0078394412994385, val loss: 3.0300698280334473, val_acc: 0.059393939393939395\n",
      "Epoch 100, training loss: 3.0140864849090576, val loss: 3.029839277267456, val_acc: 0.059393939393939395\n",
      "Epoch 110, training loss: 3.010542392730713, val loss: 3.028083562850952, val_acc: 0.059393939393939395\n",
      "Epoch 120, training loss: 3.0083446502685547, val loss: 3.0296339988708496, val_acc: 0.059393939393939395\n",
      "Epoch 130, training loss: 3.001636028289795, val loss: 3.0265939235687256, val_acc: 0.059393939393939395\n",
      "Epoch 140, training loss: 3.003847122192383, val loss: 3.027064800262451, val_acc: 0.059393939393939395\n",
      "Epoch 150, training loss: 3.007497787475586, val loss: 3.0255484580993652, val_acc: 0.059393939393939395\n",
      "Epoch 160, training loss: 3.0084609985351562, val loss: 3.0264711380004883, val_acc: 0.059393939393939395\n",
      "Epoch 170, training loss: 2.9989500045776367, val loss: 3.024840831756592, val_acc: 0.059393939393939395\n",
      "Epoch 180, training loss: 2.9960367679595947, val loss: 3.02276349067688, val_acc: 0.05818181818181818\n",
      "Epoch 190, training loss: 3.0054073333740234, val loss: 3.0228869915008545, val_acc: 0.05818181818181818\n",
      "Epoch 200, training loss: 2.9925029277801514, val loss: 3.022136926651001, val_acc: 0.05818181818181818\n",
      "Epoch 210, training loss: 2.9968960285186768, val loss: 3.0208113193511963, val_acc: 0.05818181818181818\n",
      "Epoch 220, training loss: 2.9934473037719727, val loss: 3.020859956741333, val_acc: 0.05818181818181818\n",
      "Epoch 230, training loss: 2.9933176040649414, val loss: 3.0195088386535645, val_acc: 0.04606060606060606\n",
      "Epoch 240, training loss: 2.9826531410217285, val loss: 3.018854856491089, val_acc: 0.04606060606060606\n",
      "Epoch 250, training loss: 2.9873595237731934, val loss: 3.018630266189575, val_acc: 0.04606060606060606\n",
      "Epoch 260, training loss: 2.997572183609009, val loss: 3.0185365676879883, val_acc: 0.04606060606060606\n",
      "Epoch 270, training loss: 2.9902124404907227, val loss: 3.027468681335449, val_acc: 0.044848484848484846\n",
      "Epoch 280, training loss: 2.9786717891693115, val loss: 3.029031753540039, val_acc: 0.04606060606060606\n",
      "Epoch 290, training loss: 2.976613998413086, val loss: 3.0434229373931885, val_acc: 0.04727272727272727\n",
      "Epoch 300, training loss: 2.972302198410034, val loss: 3.0225002765655518, val_acc: 0.04606060606060606\n",
      "Epoch 310, training loss: 2.9679477214813232, val loss: 3.0679173469543457, val_acc: 0.048484848484848485\n",
      "Epoch 320, training loss: 2.9755823612213135, val loss: 3.0592713356018066, val_acc: 0.048484848484848485\n",
      "Epoch 330, training loss: 2.966938018798828, val loss: 3.041989326477051, val_acc: 0.0496969696969697\n",
      "Epoch 340, training loss: 2.958021879196167, val loss: 3.0731050968170166, val_acc: 0.055757575757575756\n",
      "Epoch 350, training loss: 2.9551467895507812, val loss: 3.0920321941375732, val_acc: 0.05696969696969697\n",
      "Epoch 360, training loss: 2.967495918273926, val loss: 3.0122785568237305, val_acc: 0.048484848484848485\n",
      "Epoch 370, training loss: 3.0003960132598877, val loss: 3.009403944015503, val_acc: 0.04606060606060606\n",
      "Epoch 380, training loss: 2.993495464324951, val loss: 3.0091066360473633, val_acc: 0.04606060606060606\n",
      "Epoch 390, training loss: 2.9854674339294434, val loss: 3.009594202041626, val_acc: 0.04606060606060606\n",
      "Epoch 400, training loss: 2.989323139190674, val loss: 3.0114147663116455, val_acc: 0.044848484848484846\n",
      "Epoch 410, training loss: 2.971601724624634, val loss: 3.0133628845214844, val_acc: 0.04727272727272727\n",
      "Epoch 420, training loss: 2.953890800476074, val loss: 3.130232095718384, val_acc: 0.06060606060606061\n",
      "Epoch 430, training loss: 2.954211950302124, val loss: 3.026895523071289, val_acc: 0.05454545454545454\n",
      "Epoch 440, training loss: 2.9389431476593018, val loss: 3.040992021560669, val_acc: 0.06060606060606061\n",
      "Epoch 450, training loss: 3.01631498336792, val loss: 3.0706727504730225, val_acc: 0.06060606060606061\n",
      "Epoch 460, training loss: 2.905733585357666, val loss: 3.1719703674316406, val_acc: 0.055757575757575756\n",
      "Epoch 470, training loss: 2.9216926097869873, val loss: 3.129147529602051, val_acc: 0.05696969696969697\n",
      "Epoch 480, training loss: 2.911156177520752, val loss: 3.1215980052948, val_acc: 0.05818181818181818\n",
      "Epoch 490, training loss: 2.902482032775879, val loss: 3.0669546127319336, val_acc: 0.06060606060606061\n",
      "Epoch 500, training loss: 2.9006004333496094, val loss: 3.0645835399627686, val_acc: 0.06060606060606061\n",
      "Epoch 510, training loss: 2.891650438308716, val loss: 3.0554888248443604, val_acc: 0.06060606060606061\n",
      "Epoch 520, training loss: 2.902550458908081, val loss: 3.075316905975342, val_acc: 0.059393939393939395\n",
      "Epoch 530, training loss: 2.896413803100586, val loss: 3.0290863513946533, val_acc: 0.06303030303030303\n",
      "Epoch 540, training loss: 2.8940365314483643, val loss: 3.0886130332946777, val_acc: 0.059393939393939395\n",
      "Epoch 550, training loss: 2.9057562351226807, val loss: 3.088498115539551, val_acc: 0.05818181818181818\n",
      "Epoch 560, training loss: 2.8808698654174805, val loss: 3.0869665145874023, val_acc: 0.05818181818181818\n",
      "Epoch 570, training loss: 2.8970863819122314, val loss: 3.0734665393829346, val_acc: 0.05818181818181818\n",
      "Epoch 580, training loss: 2.8973207473754883, val loss: 3.0111401081085205, val_acc: 0.06303030303030303\n",
      "Epoch 590, training loss: 2.8952414989471436, val loss: 3.0062620639801025, val_acc: 0.06303030303030303\n",
      "Epoch 600, training loss: 2.9028966426849365, val loss: 3.0166776180267334, val_acc: 0.055757575757575756\n",
      "Epoch 610, training loss: 2.875213146209717, val loss: 3.0314252376556396, val_acc: 0.055757575757575756\n",
      "Epoch 620, training loss: 2.8855764865875244, val loss: 3.0394821166992188, val_acc: 0.05696969696969697\n",
      "Epoch 630, training loss: 2.8784611225128174, val loss: 3.0401220321655273, val_acc: 0.08727272727272728\n",
      "Epoch 640, training loss: 2.8998193740844727, val loss: 3.0276010036468506, val_acc: 0.08727272727272728\n",
      "Epoch 650, training loss: 2.879216432571411, val loss: 3.04211163520813, val_acc: 0.08484848484848485\n",
      "Epoch 660, training loss: 2.8664071559906006, val loss: 3.029801368713379, val_acc: 0.08727272727272728\n",
      "Epoch 670, training loss: 2.8849434852600098, val loss: 3.0273900032043457, val_acc: 0.08727272727272728\n",
      "Epoch 680, training loss: 2.8650870323181152, val loss: 3.039497137069702, val_acc: 0.08484848484848485\n",
      "Epoch 690, training loss: 2.8677563667297363, val loss: 3.0475575923919678, val_acc: 0.08727272727272728\n",
      "Epoch 700, training loss: 2.874824285507202, val loss: 3.0355684757232666, val_acc: 0.08606060606060606\n",
      "Epoch 710, training loss: 2.8686840534210205, val loss: 3.0146679878234863, val_acc: 0.08606060606060606\n",
      "Epoch 720, training loss: 2.871899127960205, val loss: 3.0120320320129395, val_acc: 0.08606060606060606\n",
      "Epoch 730, training loss: 2.8920884132385254, val loss: 3.0110411643981934, val_acc: 0.08606060606060606\n",
      "Epoch 740, training loss: 2.8826823234558105, val loss: 2.995741367340088, val_acc: 0.08606060606060606\n",
      "Epoch 750, training loss: 2.8772077560424805, val loss: 3.004694938659668, val_acc: 0.08727272727272728\n",
      "Epoch 760, training loss: 2.8970258235931396, val loss: 2.992690086364746, val_acc: 0.08606060606060606\n",
      "Epoch 770, training loss: 2.875994920730591, val loss: 2.9824271202087402, val_acc: 0.08484848484848485\n",
      "Epoch 780, training loss: 2.8732430934906006, val loss: 2.99993896484375, val_acc: 0.08727272727272728\n",
      "Epoch 790, training loss: 2.8739266395568848, val loss: 3.039496421813965, val_acc: 0.08606060606060606\n",
      "Epoch 800, training loss: 2.8637547492980957, val loss: 3.034356117248535, val_acc: 0.08606060606060606\n",
      "Epoch 810, training loss: 2.8739407062530518, val loss: 3.020078182220459, val_acc: 0.08484848484848485\n",
      "Epoch 820, training loss: 2.8745250701904297, val loss: 3.0230939388275146, val_acc: 0.08606060606060606\n",
      "Epoch 830, training loss: 2.860769748687744, val loss: 3.0158026218414307, val_acc: 0.08727272727272728\n",
      "Epoch 840, training loss: 2.868361234664917, val loss: 3.022592067718506, val_acc: 0.08606060606060606\n",
      "Epoch 850, training loss: 2.864596366882324, val loss: 3.022364377975464, val_acc: 0.08727272727272728\n",
      "Epoch 860, training loss: 2.881190538406372, val loss: 3.0028393268585205, val_acc: 0.08727272727272728\n",
      "Epoch 870, training loss: 2.864208459854126, val loss: 3.0024333000183105, val_acc: 0.08727272727272728\n",
      "Epoch 880, training loss: 2.8710384368896484, val loss: 3.013213872909546, val_acc: 0.08606060606060606\n",
      "Epoch 890, training loss: 2.8598716259002686, val loss: 3.0154454708099365, val_acc: 0.06181818181818182\n",
      "Epoch 900, training loss: 2.8534419536590576, val loss: 3.0082151889801025, val_acc: 0.06303030303030303\n",
      "Epoch 910, training loss: 2.8633697032928467, val loss: 3.0144336223602295, val_acc: 0.06424242424242424\n",
      "Epoch 920, training loss: 2.852541923522949, val loss: 3.0132079124450684, val_acc: 0.06181818181818182\n",
      "Epoch 930, training loss: 2.862041711807251, val loss: 3.023094415664673, val_acc: 0.06303030303030303\n",
      "Epoch 940, training loss: 2.8730781078338623, val loss: 3.0150632858276367, val_acc: 0.06181818181818182\n",
      "Epoch 950, training loss: 2.8704800605773926, val loss: 3.0232722759246826, val_acc: 0.06545454545454546\n",
      "Epoch 960, training loss: 2.885246753692627, val loss: 3.006823778152466, val_acc: 0.06303030303030303\n",
      "Epoch 970, training loss: 2.8552772998809814, val loss: 3.007366180419922, val_acc: 0.06424242424242424\n",
      "Epoch 980, training loss: 2.873798131942749, val loss: 3.0088164806365967, val_acc: 0.06181818181818182\n",
      "Epoch 990, training loss: 2.863217830657959, val loss: 3.0182723999023438, val_acc: 0.06060606060606061\n",
      "Epoch 1000, training loss: 2.8631362915039062, val loss: 3.01399302482605, val_acc: 0.06181818181818182\n",
      "Epoch 1010, training loss: 2.865992784500122, val loss: 3.011971950531006, val_acc: 0.06181818181818182\n",
      "Epoch 1020, training loss: 2.8582656383514404, val loss: 3.014106512069702, val_acc: 0.06181818181818182\n",
      "Epoch 1030, training loss: 2.8535048961639404, val loss: 3.019683599472046, val_acc: 0.06545454545454546\n",
      "Epoch 1040, training loss: 2.83969783782959, val loss: 3.0147769451141357, val_acc: 0.06787878787878789\n",
      "Epoch 1050, training loss: 2.852736234664917, val loss: 3.0137991905212402, val_acc: 0.06303030303030303\n",
      "Epoch 1060, training loss: 2.8744406700134277, val loss: 3.0024502277374268, val_acc: 0.07515151515151515\n",
      "Epoch 1070, training loss: 2.849745750427246, val loss: 3.012639284133911, val_acc: 0.08969696969696969\n",
      "Epoch 1080, training loss: 2.8764779567718506, val loss: 2.999915599822998, val_acc: 0.08363636363636363\n",
      "Epoch 1090, training loss: 2.863384246826172, val loss: 3.0008416175842285, val_acc: 0.08484848484848485\n",
      "Epoch 1100, training loss: 2.9003026485443115, val loss: 2.9746181964874268, val_acc: 0.08727272727272728\n",
      "Epoch 1110, training loss: 2.8496391773223877, val loss: 2.995255470275879, val_acc: 0.08727272727272728\n",
      "Epoch 1120, training loss: 2.8492588996887207, val loss: 2.9941751956939697, val_acc: 0.08969696969696969\n",
      "Epoch 1130, training loss: 2.86303448677063, val loss: 2.9939827919006348, val_acc: 0.08969696969696969\n",
      "Epoch 1140, training loss: 2.869652032852173, val loss: 2.9924561977386475, val_acc: 0.08969696969696969\n",
      "Epoch 1150, training loss: 2.8519294261932373, val loss: 3.0321531295776367, val_acc: 0.09212121212121212\n",
      "Epoch 1160, training loss: 2.8595991134643555, val loss: 3.0318121910095215, val_acc: 0.09090909090909091\n",
      "Epoch 1170, training loss: 2.8530216217041016, val loss: 3.0177371501922607, val_acc: 0.09212121212121212\n",
      "Epoch 1180, training loss: 2.8453617095947266, val loss: 3.0180234909057617, val_acc: 0.09090909090909091\n",
      "Epoch 1190, training loss: 2.851864814758301, val loss: 3.0164356231689453, val_acc: 0.09212121212121212\n",
      "Epoch 1200, training loss: 2.859783887863159, val loss: 3.0207629203796387, val_acc: 0.08969696969696969\n",
      "Epoch 1210, training loss: 2.835991382598877, val loss: 3.0160374641418457, val_acc: 0.08969696969696969\n",
      "Epoch 1220, training loss: 2.8530938625335693, val loss: 2.998039484024048, val_acc: 0.08606060606060606\n",
      "Epoch 1230, training loss: 2.846662998199463, val loss: 3.0073161125183105, val_acc: 0.08727272727272728\n",
      "Epoch 1240, training loss: 2.8479480743408203, val loss: 3.00972843170166, val_acc: 0.08363636363636363\n",
      "Epoch 1250, training loss: 2.869109630584717, val loss: 3.0204694271087646, val_acc: 0.08848484848484849\n",
      "Epoch 1260, training loss: 2.8427653312683105, val loss: 3.0297482013702393, val_acc: 0.09333333333333334\n",
      "Epoch 1270, training loss: 2.8485682010650635, val loss: 3.041163921356201, val_acc: 0.08727272727272728\n",
      "Epoch 1280, training loss: 2.8816144466400146, val loss: 3.024721145629883, val_acc: 0.08848484848484849\n",
      "Epoch 1290, training loss: 2.8481385707855225, val loss: 3.0215041637420654, val_acc: 0.08848484848484849\n",
      "Epoch 1300, training loss: 2.863252878189087, val loss: 3.0125343799591064, val_acc: 0.09090909090909091\n",
      "Epoch 1310, training loss: 2.861328601837158, val loss: 3.0082931518554688, val_acc: 0.09090909090909091\n",
      "Epoch 1320, training loss: 2.8448193073272705, val loss: 3.0312399864196777, val_acc: 0.09575757575757576\n",
      "Epoch 1330, training loss: 2.832988739013672, val loss: 3.019213914871216, val_acc: 0.08727272727272728\n",
      "Epoch 1340, training loss: 2.848457098007202, val loss: 3.0185976028442383, val_acc: 0.09333333333333334\n",
      "Epoch 1350, training loss: 2.840974807739258, val loss: 3.0220425128936768, val_acc: 0.08848484848484849\n",
      "Epoch 1360, training loss: 2.8494632244110107, val loss: 3.0201237201690674, val_acc: 0.09333333333333334\n",
      "Epoch 1370, training loss: 2.8400208950042725, val loss: 3.0232346057891846, val_acc: 0.06545454545454546\n",
      "Epoch 1380, training loss: 2.836575746536255, val loss: 3.0265815258026123, val_acc: 0.06303030303030303\n",
      "Epoch 1390, training loss: 2.860363483428955, val loss: 3.032470941543579, val_acc: 0.05818181818181818\n",
      "Epoch 1400, training loss: 2.8476386070251465, val loss: 3.0418434143066406, val_acc: 0.06060606060606061\n",
      "Epoch 1410, training loss: 2.846179485321045, val loss: 3.0168957710266113, val_acc: 0.09212121212121212\n",
      "Epoch 1420, training loss: 2.837167978286743, val loss: 3.0256786346435547, val_acc: 0.08484848484848485\n",
      "Epoch 1430, training loss: 2.8423478603363037, val loss: 3.023597478866577, val_acc: 0.05818181818181818\n",
      "Epoch 1440, training loss: 2.829315185546875, val loss: 3.021348237991333, val_acc: 0.06545454545454546\n",
      "Epoch 1450, training loss: 2.825856924057007, val loss: 3.0243043899536133, val_acc: 0.059393939393939395\n",
      "Epoch 1460, training loss: 2.847205877304077, val loss: 3.025712251663208, val_acc: 0.06303030303030303\n",
      "Epoch 1470, training loss: 2.8517370223999023, val loss: 3.0191917419433594, val_acc: 0.06181818181818182\n",
      "Epoch 1480, training loss: 2.8585922718048096, val loss: 3.03507924079895, val_acc: 0.06909090909090909\n",
      "Epoch 1490, training loss: 2.843538284301758, val loss: 3.017261505126953, val_acc: 0.055757575757575756\n",
      "Epoch 1500, training loss: 2.834104299545288, val loss: 3.050737142562866, val_acc: 0.06303030303030303\n",
      "Epoch 1510, training loss: 2.824300527572632, val loss: 3.048454761505127, val_acc: 0.06060606060606061\n",
      "Epoch 1520, training loss: 2.8616368770599365, val loss: 3.00877046585083, val_acc: 0.055757575757575756\n",
      "Epoch 1530, training loss: 2.8376126289367676, val loss: 3.023465633392334, val_acc: 0.06424242424242424\n",
      "Epoch 1540, training loss: 2.8406624794006348, val loss: 3.024235248565674, val_acc: 0.055757575757575756\n",
      "Epoch 1550, training loss: 2.8433046340942383, val loss: 3.0396289825439453, val_acc: 0.0703030303030303\n",
      "Epoch 1560, training loss: 2.8354713916778564, val loss: 3.024132251739502, val_acc: 0.059393939393939395\n",
      "Epoch 1570, training loss: 2.8619630336761475, val loss: 3.033867597579956, val_acc: 0.059393939393939395\n",
      "Epoch 1580, training loss: 2.849647045135498, val loss: 3.016404867172241, val_acc: 0.059393939393939395\n",
      "Epoch 1590, training loss: 2.853869676589966, val loss: 3.0378150939941406, val_acc: 0.06424242424242424\n",
      "Epoch 1600, training loss: 2.8634650707244873, val loss: 2.990802764892578, val_acc: 0.055757575757575756\n",
      "Epoch 1610, training loss: 2.839921474456787, val loss: 2.9989705085754395, val_acc: 0.055757575757575756\n",
      "Epoch 1620, training loss: 2.856642961502075, val loss: 3.0230813026428223, val_acc: 0.06060606060606061\n",
      "Epoch 1630, training loss: 2.84436297416687, val loss: 3.034313678741455, val_acc: 0.06060606060606061\n",
      "Epoch 1640, training loss: 2.85298490524292, val loss: 3.016437530517578, val_acc: 0.06181818181818182\n",
      "Epoch 1650, training loss: 2.850423574447632, val loss: 3.0352680683135986, val_acc: 0.05818181818181818\n",
      "Epoch 1660, training loss: 2.8194010257720947, val loss: 3.0243031978607178, val_acc: 0.059393939393939395\n",
      "Epoch 1670, training loss: 2.825040578842163, val loss: 3.04590106010437, val_acc: 0.09575757575757576\n",
      "Epoch 1680, training loss: 2.8394198417663574, val loss: 3.039682149887085, val_acc: 0.08727272727272728\n",
      "Epoch 1690, training loss: 2.820096969604492, val loss: 3.024674415588379, val_acc: 0.08363636363636363\n",
      "Epoch 1700, training loss: 2.823625326156616, val loss: 3.0325121879577637, val_acc: 0.08606060606060606\n",
      "Epoch 1710, training loss: 2.824629068374634, val loss: 3.054058313369751, val_acc: 0.08848484848484849\n",
      "Epoch 1720, training loss: 2.8398852348327637, val loss: 3.0172760486602783, val_acc: 0.08121212121212121\n",
      "Epoch 1730, training loss: 2.816816568374634, val loss: 3.0438854694366455, val_acc: 0.08848484848484849\n",
      "Epoch 1740, training loss: 2.825824737548828, val loss: 3.0403640270233154, val_acc: 0.08848484848484849\n",
      "Epoch 1750, training loss: 2.8264219760894775, val loss: 3.0374913215637207, val_acc: 0.08848484848484849\n",
      "Epoch 1760, training loss: 2.8405847549438477, val loss: 3.0313382148742676, val_acc: 0.08848484848484849\n",
      "Epoch 1770, training loss: 2.8340423107147217, val loss: 3.035524606704712, val_acc: 0.08363636363636363\n",
      "Epoch 1780, training loss: 2.8365049362182617, val loss: 3.0194449424743652, val_acc: 0.08606060606060606\n",
      "Epoch 1790, training loss: 2.8412952423095703, val loss: 3.04040789604187, val_acc: 0.08848484848484849\n",
      "Epoch 1800, training loss: 2.827888011932373, val loss: 3.0279550552368164, val_acc: 0.08121212121212121\n",
      "Epoch 1810, training loss: 2.8355822563171387, val loss: 3.0329980850219727, val_acc: 0.08727272727272728\n",
      "Epoch 1820, training loss: 2.8161275386810303, val loss: 3.0605804920196533, val_acc: 0.08484848484848485\n",
      "Epoch 1830, training loss: 2.821138381958008, val loss: 3.0537211894989014, val_acc: 0.09333333333333334\n",
      "Epoch 1840, training loss: 2.833942413330078, val loss: 3.0455777645111084, val_acc: 0.09212121212121212\n",
      "Epoch 1850, training loss: 2.8523988723754883, val loss: 2.9727237224578857, val_acc: 0.08606060606060606\n",
      "Epoch 1860, training loss: 2.866626739501953, val loss: 3.100252628326416, val_acc: 0.08606060606060606\n",
      "Epoch 1870, training loss: 2.8217649459838867, val loss: 3.0742380619049072, val_acc: 0.06303030303030303\n",
      "Epoch 1880, training loss: 2.8290252685546875, val loss: 3.0649197101593018, val_acc: 0.08727272727272728\n",
      "Epoch 1890, training loss: 2.840484380722046, val loss: 3.068455457687378, val_acc: 0.08606060606060606\n",
      "Epoch 1900, training loss: 2.837055206298828, val loss: 3.097214698791504, val_acc: 0.06060606060606061\n",
      "Epoch 1910, training loss: 2.8447022438049316, val loss: 3.0892093181610107, val_acc: 0.08848484848484849\n",
      "Epoch 1920, training loss: 2.843655586242676, val loss: 3.0585386753082275, val_acc: 0.08484848484848485\n",
      "Epoch 1930, training loss: 2.8532662391662598, val loss: 3.0390264987945557, val_acc: 0.08484848484848485\n",
      "Epoch 1940, training loss: 2.844930648803711, val loss: 3.04365611076355, val_acc: 0.08969696969696969\n",
      "Epoch 1950, training loss: 2.8433690071105957, val loss: 3.0522334575653076, val_acc: 0.08363636363636363\n",
      "Epoch 1960, training loss: 2.825528144836426, val loss: 3.0467028617858887, val_acc: 0.08484848484848485\n",
      "Epoch 1970, training loss: 2.8164806365966797, val loss: 3.0633933544158936, val_acc: 0.08484848484848485\n",
      "Epoch 1980, training loss: 2.8137500286102295, val loss: 3.04937744140625, val_acc: 0.08969696969696969\n",
      "Epoch 1990, training loss: 2.82468318939209, val loss: 3.0548713207244873, val_acc: 0.08848484848484849\n",
      "Epoch 2000, training loss: 2.834935188293457, val loss: 3.0412044525146484, val_acc: 0.09090909090909091\n",
      "Epoch 2010, training loss: 2.818260431289673, val loss: 3.048440456390381, val_acc: 0.08848484848484849\n",
      "Epoch 2020, training loss: 2.8308305740356445, val loss: 3.0512454509735107, val_acc: 0.08848484848484849\n",
      "Epoch 2030, training loss: 2.820505142211914, val loss: 3.0620007514953613, val_acc: 0.08848484848484849\n",
      "Epoch 2040, training loss: 2.817005157470703, val loss: 3.055715322494507, val_acc: 0.08969696969696969\n",
      "Epoch 2050, training loss: 2.8251755237579346, val loss: 3.06809401512146, val_acc: 0.09090909090909091\n",
      "Epoch 2060, training loss: 2.835740327835083, val loss: 3.0457305908203125, val_acc: 0.09454545454545454\n",
      "Epoch 2070, training loss: 2.8164820671081543, val loss: 3.0575499534606934, val_acc: 0.08727272727272728\n",
      "Epoch 2080, training loss: 2.8218493461608887, val loss: 3.0782222747802734, val_acc: 0.09454545454545454\n",
      "Epoch 2090, training loss: 2.812410831451416, val loss: 3.064345598220825, val_acc: 0.08969696969696969\n",
      "Epoch 2100, training loss: 2.8311963081359863, val loss: 3.0634827613830566, val_acc: 0.08969696969696969\n",
      "Epoch 2110, training loss: 2.8319602012634277, val loss: 3.055797815322876, val_acc: 0.08969696969696969\n",
      "Epoch 2120, training loss: 2.8241872787475586, val loss: 3.065187692642212, val_acc: 0.08848484848484849\n",
      "Epoch 2130, training loss: 2.8277578353881836, val loss: 3.0471949577331543, val_acc: 0.08606060606060606\n",
      "Epoch 2140, training loss: 2.8199515342712402, val loss: 3.057701826095581, val_acc: 0.09333333333333334\n",
      "Epoch 2150, training loss: 2.8370132446289062, val loss: 3.0589184761047363, val_acc: 0.08969696969696969\n",
      "Epoch 2160, training loss: 2.812495470046997, val loss: 3.0690550804138184, val_acc: 0.08848484848484849\n",
      "Epoch 2170, training loss: 2.832732915878296, val loss: 3.06174898147583, val_acc: 0.09212121212121212\n",
      "Epoch 2180, training loss: 2.82610821723938, val loss: 3.044731378555298, val_acc: 0.08969696969696969\n",
      "Epoch 2190, training loss: 2.857832908630371, val loss: 2.984511375427246, val_acc: 0.08484848484848485\n",
      "Epoch 2200, training loss: 2.8140921592712402, val loss: 2.998232126235962, val_acc: 0.09090909090909091\n",
      "Epoch 2210, training loss: 2.805732250213623, val loss: 3.0229737758636475, val_acc: 0.08606060606060606\n",
      "Epoch 2220, training loss: 2.824124813079834, val loss: 3.031209707260132, val_acc: 0.08363636363636363\n",
      "Epoch 2230, training loss: 2.8239822387695312, val loss: 3.0179126262664795, val_acc: 0.08969696969696969\n",
      "Epoch 2240, training loss: 2.8077056407928467, val loss: 3.0950684547424316, val_acc: 0.09212121212121212\n",
      "Epoch 2250, training loss: 2.824453115463257, val loss: 3.100505828857422, val_acc: 0.08969696969696969\n",
      "Epoch 2260, training loss: 2.827232599258423, val loss: 3.0606608390808105, val_acc: 0.09212121212121212\n",
      "Epoch 2270, training loss: 2.83134388923645, val loss: 3.0956826210021973, val_acc: 0.09212121212121212\n",
      "Epoch 2280, training loss: 2.8430724143981934, val loss: 3.0903561115264893, val_acc: 0.08969696969696969\n",
      "Epoch 2290, training loss: 2.8229620456695557, val loss: 3.06658935546875, val_acc: 0.09212121212121212\n",
      "Epoch 2300, training loss: 2.814206123352051, val loss: 3.0597198009490967, val_acc: 0.08969696969696969\n",
      "Epoch 2310, training loss: 2.8214974403381348, val loss: 3.0620243549346924, val_acc: 0.08727272727272728\n",
      "Epoch 2320, training loss: 2.8220627307891846, val loss: 3.084705352783203, val_acc: 0.09090909090909091\n",
      "Epoch 2330, training loss: 2.8337788581848145, val loss: 3.0636074542999268, val_acc: 0.08969696969696969\n",
      "Epoch 2340, training loss: 2.793506622314453, val loss: 3.0696330070495605, val_acc: 0.08969696969696969\n",
      "Epoch 2350, training loss: 2.835759401321411, val loss: 3.0767807960510254, val_acc: 0.09575757575757576\n",
      "Epoch 2360, training loss: 2.8078575134277344, val loss: 3.0718133449554443, val_acc: 0.09090909090909091\n",
      "Epoch 2370, training loss: 2.8119049072265625, val loss: 3.0719265937805176, val_acc: 0.08969696969696969\n",
      "Epoch 2380, training loss: 2.810516595840454, val loss: 3.0773603916168213, val_acc: 0.09090909090909091\n",
      "Epoch 2390, training loss: 2.799600839614868, val loss: 3.0560078620910645, val_acc: 0.08727272727272728\n",
      "Epoch 2400, training loss: 2.8171682357788086, val loss: 3.0512757301330566, val_acc: 0.08727272727272728\n",
      "Epoch 2410, training loss: 2.8378570079803467, val loss: 3.0980639457702637, val_acc: 0.08848484848484849\n",
      "Epoch 2420, training loss: 2.8118858337402344, val loss: 3.07552170753479, val_acc: 0.08969696969696969\n",
      "Epoch 2430, training loss: 2.8439409732818604, val loss: 3.0648086071014404, val_acc: 0.09090909090909091\n",
      "Epoch 2440, training loss: 2.818377733230591, val loss: 3.0834720134735107, val_acc: 0.08848484848484849\n",
      "Epoch 2450, training loss: 2.8168046474456787, val loss: 3.0849287509918213, val_acc: 0.08606060606060606\n",
      "Epoch 2460, training loss: 2.827545166015625, val loss: 3.1175858974456787, val_acc: 0.08606060606060606\n",
      "Epoch 2470, training loss: 2.803006172180176, val loss: 3.0639493465423584, val_acc: 0.08727272727272728\n",
      "Epoch 2480, training loss: 2.806365489959717, val loss: 3.0928754806518555, val_acc: 0.08848484848484849\n",
      "Epoch 2490, training loss: 2.81129789352417, val loss: 3.0648906230926514, val_acc: 0.08484848484848485\n",
      "Epoch 2500, training loss: 2.8027937412261963, val loss: 3.060175895690918, val_acc: 0.08727272727272728\n",
      "Epoch 2510, training loss: 2.7998411655426025, val loss: 3.082563638687134, val_acc: 0.08606060606060606\n",
      "Epoch 2520, training loss: 2.8406879901885986, val loss: 3.1277353763580322, val_acc: 0.08484848484848485\n",
      "Epoch 2530, training loss: 2.8159537315368652, val loss: 3.160682439804077, val_acc: 0.08848484848484849\n",
      "Epoch 2540, training loss: 2.818652391433716, val loss: 3.119295358657837, val_acc: 0.08484848484848485\n",
      "Epoch 2550, training loss: 2.805149793624878, val loss: 3.083988904953003, val_acc: 0.08484848484848485\n",
      "Epoch 2560, training loss: 2.8298842906951904, val loss: 3.0649755001068115, val_acc: 0.08121212121212121\n",
      "Epoch 2570, training loss: 2.8039867877960205, val loss: 3.07277512550354, val_acc: 0.08969696969696969\n",
      "Epoch 2580, training loss: 2.8191046714782715, val loss: 3.0644690990448, val_acc: 0.08484848484848485\n",
      "Epoch 2590, training loss: 2.8107352256774902, val loss: 3.0887372493743896, val_acc: 0.09090909090909091\n",
      "Epoch 2600, training loss: 2.837890386581421, val loss: 3.0985851287841797, val_acc: 0.08727272727272728\n",
      "Epoch 2610, training loss: 2.8284733295440674, val loss: 3.086841344833374, val_acc: 0.08606060606060606\n",
      "Epoch 2620, training loss: 2.7915947437286377, val loss: 3.0755152702331543, val_acc: 0.08121212121212121\n",
      "Epoch 2630, training loss: 2.8115038871765137, val loss: 3.0977895259857178, val_acc: 0.08484848484848485\n",
      "Epoch 2640, training loss: 2.782510280609131, val loss: 3.098599433898926, val_acc: 0.09212121212121212\n",
      "Epoch 2650, training loss: 2.821052074432373, val loss: 3.080352783203125, val_acc: 0.08727272727272728\n",
      "Epoch 2660, training loss: 2.813889265060425, val loss: 3.085766077041626, val_acc: 0.08848484848484849\n",
      "Epoch 2670, training loss: 2.807608127593994, val loss: 3.0777456760406494, val_acc: 0.08848484848484849\n",
      "Epoch 2680, training loss: 2.789914846420288, val loss: 3.142010450363159, val_acc: 0.08484848484848485\n",
      "Epoch 2690, training loss: 2.8000237941741943, val loss: 3.072789192199707, val_acc: 0.08969696969696969\n",
      "Epoch 2700, training loss: 2.8057544231414795, val loss: 3.04849910736084, val_acc: 0.08363636363636363\n",
      "Epoch 2710, training loss: 2.81435489654541, val loss: 3.053417444229126, val_acc: 0.08848484848484849\n",
      "Epoch 2720, training loss: 2.8069100379943848, val loss: 3.0888547897338867, val_acc: 0.09090909090909091\n",
      "Epoch 2730, training loss: 2.785586357116699, val loss: 3.099506378173828, val_acc: 0.09090909090909091\n",
      "Epoch 2740, training loss: 2.803187608718872, val loss: 3.1064510345458984, val_acc: 0.08969696969696969\n",
      "Epoch 2750, training loss: 2.8172898292541504, val loss: 3.0826504230499268, val_acc: 0.08606060606060606\n",
      "Epoch 2760, training loss: 2.8056528568267822, val loss: 3.094892978668213, val_acc: 0.09090909090909091\n",
      "Epoch 2770, training loss: 2.800875186920166, val loss: 3.1040713787078857, val_acc: 0.08848484848484849\n",
      "Epoch 2780, training loss: 2.8131487369537354, val loss: 3.105379343032837, val_acc: 0.08606060606060606\n",
      "Epoch 2790, training loss: 2.808931827545166, val loss: 3.1081578731536865, val_acc: 0.09090909090909091\n",
      "Epoch 2800, training loss: 2.8133602142333984, val loss: 3.0453941822052, val_acc: 0.08121212121212121\n",
      "Epoch 2810, training loss: 2.801191806793213, val loss: 3.1178853511810303, val_acc: 0.08727272727272728\n",
      "Epoch 2820, training loss: 2.8075315952301025, val loss: 3.104762554168701, val_acc: 0.08848484848484849\n",
      "Epoch 2830, training loss: 2.810699224472046, val loss: 3.081616163253784, val_acc: 0.08606060606060606\n",
      "Epoch 2840, training loss: 2.822726249694824, val loss: 3.1208302974700928, val_acc: 0.08727272727272728\n",
      "Epoch 2850, training loss: 2.8072621822357178, val loss: 3.1363766193389893, val_acc: 0.08848484848484849\n",
      "Epoch 2860, training loss: 2.799184560775757, val loss: 3.118496894836426, val_acc: 0.09575757575757576\n",
      "Epoch 2870, training loss: 2.818660259246826, val loss: 3.0748186111450195, val_acc: 0.08727272727272728\n",
      "Epoch 2880, training loss: 2.7867565155029297, val loss: 3.0867979526519775, val_acc: 0.08606060606060606\n",
      "Epoch 2890, training loss: 2.797638416290283, val loss: 3.1672778129577637, val_acc: 0.09454545454545454\n",
      "Epoch 2900, training loss: 2.8207218647003174, val loss: 3.087347984313965, val_acc: 0.09090909090909091\n",
      "Epoch 2910, training loss: 2.787734031677246, val loss: 3.088998556137085, val_acc: 0.08727272727272728\n",
      "Epoch 2920, training loss: 2.8040316104888916, val loss: 3.1144168376922607, val_acc: 0.08969696969696969\n",
      "Epoch 2930, training loss: 2.788112163543701, val loss: 3.1207456588745117, val_acc: 0.09454545454545454\n",
      "Early stopping at epoch 1934 with validation accuracy 0.100606\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gcn_models = {}\n",
    "for name, data in dataset.items():\n",
    "    print(f\"Training GCN with {name}\")\n",
    "\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj = to_scipy_sparse_matrix(data.edge_index, num_nodes=num_nodes)\n",
    "    adj_norm = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    features = data.x\n",
    "    labels = data.y\n",
    "\n",
    "    nclass = labels.max().item()+1\n",
    "    gcn = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "    gcn = gcn.to(device)\n",
    "    data = data.to(device)\n",
    "    adj_norm = adj_norm.to(device)\n",
    "    train(gcn, data, adj_norm, lr=0.001, epochs=5000, patience=1000)\n",
    "\n",
    "    gcn_models[name] = gcn\n",
    "\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GCN with data_clip\n",
      "Test set results: loss= 2.9625 accuracy= 0.0267\n",
      "====================\n",
      "Testing GCN with data_cnn\n",
      "Test set results: loss= 3.7026 accuracy= 0.0462\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# test models\n",
    "for name, model in gcn_models.items():\n",
    "    print(f\"Testing GCN with {name}\")\n",
    "    data = dataset[name]\n",
    "\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj = to_scipy_sparse_matrix(data.edge_index, num_nodes=num_nodes)\n",
    "    adj_norm = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    test(model, data, adj_norm)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_norm = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "# nclass = labels.max().item()+1\n",
    "# gcn = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "# # move the model and data from CPU to GPU\n",
    "# # device = 'cpu' # device ='cuda'\n",
    "# gcn = gcn.to(device)\n",
    "# data = data.to(device)\n",
    "# adj_norm = adj_norm.to(device)\n",
    "# train(gcn, data, adj_norm, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Implementing GAT for Product Classification (20 points)\n",
    "\n",
    "## Code implementation (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each GAT layer is upadted as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n",
    "\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention score between node i and j is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_{i,j} =\n",
    "\\frac{\n",
    "\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n",
    "\\right)\\right)}\n",
    "{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
    "\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n",
    "\\right)\\right)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "import torch.optim as optim\n",
    "\n",
    "class GATConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ------   \n",
    "    in_channels: input neural channels\n",
    "    out_channels: output neural channels\n",
    "    heads: the number of multi-head\n",
    "    concat: How to combine multiple heads? Concat or pooling? \n",
    "    negative_slope: parameter for LeakyReLU activation\n",
    "    dropout: dropout ratio\n",
    "    bias: bias term\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels,\n",
    "                 out_channels, heads=1, concat=True,\n",
    "                 negative_slope=0.2, dropout=0.0,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super(GATConv, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin = Linear(in_channels, heads * out_channels,\n",
    "                              bias=False, weight_initializer='glorot')\n",
    "\n",
    "        # attention parameters\n",
    "        self.att = Parameter(torch.Tensor(1, heads, 2*out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"param initialization\"\"\"\n",
    "        self.lin.reset_parameters()\n",
    "        glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "    \n",
    "    def forward(self, x, edge_index, size=None):\n",
    "        \"\"\"forward pass\"\"\"\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        output = self.propagate(edge_index, size=size, x=x) # \n",
    "        \n",
    "        if self.concat is True:\n",
    "            #################\n",
    "            #### write your code here  ####\n",
    "            output = output.view(-1, self.heads * self.out_channels)\n",
    "            ################\n",
    "        else:\n",
    "            #################\n",
    "            #### write your code here  ####\n",
    "            output = output.mean(dim=1) # average heads\n",
    "            ################\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "    def message(self, edge_index_i, x_i, x_j, size_i):\n",
    "        \"\"\"compute attentin score\n",
    "    \n",
    "        parameters\n",
    "        ----\n",
    "        edge_index_i: the first dimension of edge index, the neighbors of x_i \n",
    "        x_i: source nodes' features\n",
    "        x_j: target nodes' features\n",
    "        size_i: source node size\n",
    "        \"\"\"\n",
    "        \n",
    "        ###############################################\n",
    "        #### write your code here, compute alpha score before softmax####\n",
    "\n",
    "        \n",
    "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
    "        alpha = alpha.view(-1, self.heads, 2*self.out_channels) # reshape to (num_edges, heads, 2*out_channels)\n",
    "\n",
    "\n",
    "        # print(f\"x_i shape: {x_i.shape}\")   # Expected: (num_edges, heads, out_channels)\n",
    "        # print(f\"x_j shape: {x_j.shape}\")   # Expected: (num_edges, heads, out_channels)\n",
    "        # print(f\"alpha shape: {alpha.shape}\")  # Should be (num_edges, heads, 2*out_channels)\n",
    "        # print(f\"self.att shape: {self.att.shape}\")\n",
    "\n",
    "        alpha = (alpha * self.att).sum(dim=-1)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        ###############################################\n",
    "        \n",
    "        alpha = softmax(src=alpha, index=edge_index_i, num_nodes=size_i)\n",
    "        \n",
    "        # apply dropout to alpha \n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        ##################################\n",
    "        #### write your code here, obtaining the return output ####\n",
    "        #################################\n",
    "        return alpha.unsqueeze(-1) * x_j.view(-1, self.heads, self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    \"\"\" 2 GAT layers.\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    nfeat : input dimension\n",
    "    nhid : hidden dimensions\n",
    "    nclass : number of classes\n",
    "    heads: attention heads\n",
    "    output_heads: the heads in output layer\n",
    "    dropout : dropout ratio\n",
    "    with_bias: bias term\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, heads=8, output_heads=1, dropout=0.5, with_bias=True):\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            nfeat,\n",
    "            nhid,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            nhid * heads,\n",
    "            nclass,\n",
    "            heads=output_heads,\n",
    "            concat=False,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.conv1(x, edge_index)) # \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"parameter initialization.\n",
    "        \"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, lr=0.01, weight_decay=5e-4, epochs=200, patience=5):\n",
    "    \"\"\"train the model\"\"\"\n",
    "    #################\n",
    "    #### write your code here ####\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    labels = data.y\n",
    "    train_mask = data.train_mask \n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_val_epoch = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = F.nll_loss(output[train_mask], labels[train_mask]) # compute training loss\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validate each epoch\n",
    "        _, _, acc_val, loss_val = validation(model, data, verbose=False)\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {i}, training loss: {loss.item()}, val loss: {loss_val}, val_acc: {acc_val}\")\n",
    "            # print('Epoch {}, training loss: {}, '.format(i, loss.item()))\n",
    "\n",
    "        # early stopping\n",
    "        if acc_val > best_val_acc:\n",
    "            best_val_acc = acc_val\n",
    "            best_val_epoch = i\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {best_val_epoch} with validation accuracy {best_val_acc:4f}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None: \n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    ################\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    \"\"\"test the performance\"\"\"\n",
    "    #################\n",
    "    #### write your code here ####\n",
    "    model.eval() # eval() \n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model(data) #  \n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) #  \n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() #  \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()\n",
    "    ################\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(model, data, verbose=True):\n",
    "    \"\"\"add a validation for early stopping\"\"\"\n",
    "    model.eval() # eval() \n",
    "    val_mask = data.val_mask\n",
    "    labels = data.y \n",
    "    output = model(data) #  \n",
    "    loss_val = F.nll_loss(output[val_mask], labels[val_mask])\n",
    "    preds = output[val_mask].argmax(1) #  \n",
    "    acc_val= preds.eq(labels[val_mask]).cpu().numpy().mean() #  \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Val set results:\",\n",
    "            \"loss= {:.4f}\".format(loss_val.item()),\n",
    "            \"accuracy= {:.4f}\".format(acc_val))\n",
    "    return preds, output, acc_val.item(), loss_val.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train GAT model and test your model and report your results (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAT with data_clip\n",
      "Epoch 0, training loss: 3.7320942878723145, val loss: 3.166841745376587, val_acc: 0.03151515151515152\n",
      "Epoch 10, training loss: 3.060434103012085, val loss: 2.984902858734131, val_acc: 0.06060606060606061\n",
      "Epoch 20, training loss: 3.0288753509521484, val loss: 2.994938373565674, val_acc: 0.02909090909090909\n",
      "Epoch 30, training loss: 3.0032763481140137, val loss: 2.9747159481048584, val_acc: 0.08\n",
      "Epoch 40, training loss: 2.9837636947631836, val loss: 2.9673240184783936, val_acc: 0.10787878787878788\n",
      "Epoch 50, training loss: 2.9738552570343018, val loss: 2.9596495628356934, val_acc: 0.11393939393939394\n",
      "Epoch 60, training loss: 2.9568281173706055, val loss: 2.951387882232666, val_acc: 0.08727272727272728\n",
      "Epoch 70, training loss: 2.966040849685669, val loss: 2.949796676635742, val_acc: 0.0993939393939394\n",
      "Epoch 80, training loss: 2.9473276138305664, val loss: 2.943307876586914, val_acc: 0.13090909090909092\n",
      "Epoch 90, training loss: 2.9319992065429688, val loss: 2.9431724548339844, val_acc: 0.11878787878787879\n",
      "Epoch 100, training loss: 2.934929132461548, val loss: 2.930814027786255, val_acc: 0.1103030303030303\n",
      "Epoch 110, training loss: 2.9320080280303955, val loss: 2.9257655143737793, val_acc: 0.12242424242424242\n",
      "Epoch 120, training loss: 2.9239656925201416, val loss: 2.9301092624664307, val_acc: 0.10787878787878788\n",
      "Epoch 130, training loss: 2.9212424755096436, val loss: 2.9129812717437744, val_acc: 0.1406060606060606\n",
      "Epoch 140, training loss: 2.9052536487579346, val loss: 2.9081146717071533, val_acc: 0.12606060606060607\n",
      "Epoch 150, training loss: 2.909126043319702, val loss: 2.909435749053955, val_acc: 0.1296969696969697\n",
      "Epoch 160, training loss: 2.893563747406006, val loss: 2.8979063034057617, val_acc: 0.11878787878787879\n",
      "Epoch 170, training loss: 2.8692924976348877, val loss: 2.894631862640381, val_acc: 0.15151515151515152\n",
      "Epoch 180, training loss: 2.888033151626587, val loss: 2.88592267036438, val_acc: 0.14666666666666667\n",
      "Epoch 190, training loss: 2.8900516033172607, val loss: 2.892693519592285, val_acc: 0.14424242424242426\n",
      "Epoch 200, training loss: 2.879814386367798, val loss: 2.8770318031311035, val_acc: 0.1296969696969697\n",
      "Epoch 210, training loss: 2.864692449569702, val loss: 2.8866422176361084, val_acc: 0.13818181818181818\n",
      "Epoch 220, training loss: 2.862072229385376, val loss: 2.8718690872192383, val_acc: 0.16363636363636364\n",
      "Epoch 230, training loss: 2.8562304973602295, val loss: 2.8617167472839355, val_acc: 0.14909090909090908\n",
      "Epoch 240, training loss: 2.8594143390655518, val loss: 2.8662173748016357, val_acc: 0.16\n",
      "Epoch 250, training loss: 2.8502893447875977, val loss: 2.858792543411255, val_acc: 0.16606060606060605\n",
      "Epoch 260, training loss: 2.84763503074646, val loss: 2.8494157791137695, val_acc: 0.15515151515151515\n",
      "Epoch 270, training loss: 2.8629627227783203, val loss: 2.8509955406188965, val_acc: 0.15272727272727274\n",
      "Epoch 280, training loss: 2.8403091430664062, val loss: 2.848400831222534, val_acc: 0.15272727272727274\n",
      "Epoch 290, training loss: 2.8420886993408203, val loss: 2.8459548950195312, val_acc: 0.16242424242424242\n",
      "Epoch 300, training loss: 2.847583293914795, val loss: 2.8424344062805176, val_acc: 0.16\n",
      "Epoch 310, training loss: 2.831753969192505, val loss: 2.837998390197754, val_acc: 0.15757575757575756\n",
      "Epoch 320, training loss: 2.849571466445923, val loss: 2.8344521522521973, val_acc: 0.15272727272727274\n",
      "Epoch 330, training loss: 2.835909843444824, val loss: 2.8393101692199707, val_acc: 0.16242424242424242\n",
      "Epoch 340, training loss: 2.8128750324249268, val loss: 2.816828489303589, val_acc: 0.17212121212121212\n",
      "Epoch 350, training loss: 2.814382314682007, val loss: 2.8324966430664062, val_acc: 0.16484848484848486\n",
      "Epoch 360, training loss: 2.82985782623291, val loss: 2.8176403045654297, val_acc: 0.16727272727272727\n",
      "Epoch 370, training loss: 2.7909293174743652, val loss: 2.8147940635681152, val_acc: 0.18303030303030304\n",
      "Epoch 380, training loss: 2.811143159866333, val loss: 2.8251218795776367, val_acc: 0.16363636363636364\n",
      "Epoch 390, training loss: 2.811516046524048, val loss: 2.813270092010498, val_acc: 0.16484848484848486\n",
      "Epoch 400, training loss: 2.8198161125183105, val loss: 2.8190202713012695, val_acc: 0.15393939393939393\n",
      "Epoch 410, training loss: 2.821516752243042, val loss: 2.811169147491455, val_acc: 0.16484848484848486\n",
      "Epoch 420, training loss: 2.797762870788574, val loss: 2.803163528442383, val_acc: 0.18666666666666668\n",
      "Epoch 430, training loss: 2.7943968772888184, val loss: 2.8023014068603516, val_acc: 0.1696969696969697\n",
      "Epoch 440, training loss: 2.816861629486084, val loss: 2.8065388202667236, val_acc: 0.17939393939393938\n",
      "Epoch 450, training loss: 2.8051767349243164, val loss: 2.798715114593506, val_acc: 0.1781818181818182\n",
      "Epoch 460, training loss: 2.805344343185425, val loss: 2.7942888736724854, val_acc: 0.18303030303030304\n",
      "Epoch 470, training loss: 2.783872604370117, val loss: 2.7874345779418945, val_acc: 0.1890909090909091\n",
      "Epoch 480, training loss: 2.754481554031372, val loss: 2.779768466949463, val_acc: 0.18424242424242424\n",
      "Epoch 490, training loss: 2.776221513748169, val loss: 2.78680682182312, val_acc: 0.1890909090909091\n",
      "Epoch 500, training loss: 2.798142194747925, val loss: 2.7770113945007324, val_acc: 0.18545454545454546\n",
      "Epoch 510, training loss: 2.736008405685425, val loss: 2.7753334045410156, val_acc: 0.1903030303030303\n",
      "Epoch 520, training loss: 2.7322614192962646, val loss: 2.7735402584075928, val_acc: 0.1709090909090909\n",
      "Epoch 530, training loss: 2.78604793548584, val loss: 2.773437261581421, val_acc: 0.17939393939393938\n",
      "Epoch 540, training loss: 2.743865489959717, val loss: 2.774646043777466, val_acc: 0.18545454545454546\n",
      "Epoch 550, training loss: 2.761040210723877, val loss: 2.7526957988739014, val_acc: 0.20606060606060606\n",
      "Epoch 560, training loss: 2.748298406600952, val loss: 2.763502597808838, val_acc: 0.19393939393939394\n",
      "Epoch 570, training loss: 2.7693443298339844, val loss: 2.7721471786499023, val_acc: 0.18303030303030304\n",
      "Epoch 580, training loss: 2.759413957595825, val loss: 2.7499194145202637, val_acc: 0.1987878787878788\n",
      "Epoch 590, training loss: 2.7485759258270264, val loss: 2.7550249099731445, val_acc: 0.18787878787878787\n",
      "Epoch 600, training loss: 2.7525298595428467, val loss: 2.7622528076171875, val_acc: 0.18787878787878787\n",
      "Epoch 610, training loss: 2.719346523284912, val loss: 2.7596678733825684, val_acc: 0.18424242424242424\n",
      "Epoch 620, training loss: 2.7404398918151855, val loss: 2.747807264328003, val_acc: 0.19515151515151516\n",
      "Epoch 630, training loss: 2.751493215560913, val loss: 2.742060661315918, val_acc: 0.20363636363636364\n",
      "Epoch 640, training loss: 2.743570566177368, val loss: 2.73868989944458, val_acc: 0.19151515151515153\n",
      "Epoch 650, training loss: 2.7675979137420654, val loss: 2.7560834884643555, val_acc: 0.19151515151515153\n",
      "Epoch 660, training loss: 2.718256950378418, val loss: 2.7362592220306396, val_acc: 0.2\n",
      "Epoch 670, training loss: 2.754041910171509, val loss: 2.7424020767211914, val_acc: 0.19757575757575757\n",
      "Epoch 680, training loss: 2.705542802810669, val loss: 2.741353988647461, val_acc: 0.19393939393939394\n",
      "Epoch 690, training loss: 2.709610939025879, val loss: 2.7270491123199463, val_acc: 0.2109090909090909\n",
      "Epoch 700, training loss: 2.7400691509246826, val loss: 2.739614963531494, val_acc: 0.19757575757575757\n",
      "Epoch 710, training loss: 2.725109338760376, val loss: 2.7359514236450195, val_acc: 0.21454545454545454\n",
      "Epoch 720, training loss: 2.719529390335083, val loss: 2.7222609519958496, val_acc: 0.21454545454545454\n",
      "Epoch 730, training loss: 2.738088846206665, val loss: 2.732832908630371, val_acc: 0.2\n",
      "Epoch 740, training loss: 2.7382726669311523, val loss: 2.7396860122680664, val_acc: 0.20484848484848484\n",
      "Epoch 750, training loss: 2.712111711502075, val loss: 2.738668441772461, val_acc: 0.20242424242424242\n",
      "Epoch 760, training loss: 2.716517210006714, val loss: 2.7281618118286133, val_acc: 0.19757575757575757\n",
      "Epoch 770, training loss: 2.7658023834228516, val loss: 2.741133213043213, val_acc: 0.19151515151515153\n",
      "Epoch 780, training loss: 2.706892251968384, val loss: 2.736341714859009, val_acc: 0.1903030303030303\n",
      "Epoch 790, training loss: 2.7391767501831055, val loss: 2.7334609031677246, val_acc: 0.1987878787878788\n",
      "Epoch 800, training loss: 2.6827175617218018, val loss: 2.721445083618164, val_acc: 0.2084848484848485\n",
      "Epoch 810, training loss: 2.7139503955841064, val loss: 2.710083484649658, val_acc: 0.21575757575757576\n",
      "Epoch 820, training loss: 2.7240993976593018, val loss: 2.7171571254730225, val_acc: 0.1903030303030303\n",
      "Epoch 830, training loss: 2.696915626525879, val loss: 2.714092969894409, val_acc: 0.2096969696969697\n",
      "Epoch 840, training loss: 2.713205099105835, val loss: 2.709929943084717, val_acc: 0.20606060606060606\n",
      "Epoch 850, training loss: 2.7110238075256348, val loss: 2.714028835296631, val_acc: 0.19393939393939394\n",
      "Epoch 860, training loss: 2.690295696258545, val loss: 2.704676628112793, val_acc: 0.21454545454545454\n",
      "Epoch 870, training loss: 2.695944309234619, val loss: 2.71610951423645, val_acc: 0.21333333333333335\n",
      "Epoch 880, training loss: 2.710587501525879, val loss: 2.704052686691284, val_acc: 0.2096969696969697\n",
      "Epoch 890, training loss: 2.6765573024749756, val loss: 2.7130074501037598, val_acc: 0.2084848484848485\n",
      "Epoch 900, training loss: 2.674736499786377, val loss: 2.7057464122772217, val_acc: 0.22666666666666666\n",
      "Epoch 910, training loss: 2.6957077980041504, val loss: 2.709973096847534, val_acc: 0.2096969696969697\n",
      "Epoch 920, training loss: 2.688154458999634, val loss: 2.6978495121002197, val_acc: 0.2206060606060606\n",
      "Epoch 930, training loss: 2.6832339763641357, val loss: 2.6966464519500732, val_acc: 0.21333333333333335\n",
      "Epoch 940, training loss: 2.6880853176116943, val loss: 2.703495740890503, val_acc: 0.2109090909090909\n",
      "Epoch 950, training loss: 2.7103588581085205, val loss: 2.7026329040527344, val_acc: 0.20242424242424242\n",
      "Epoch 960, training loss: 2.656162977218628, val loss: 2.709812641143799, val_acc: 0.21333333333333335\n",
      "Epoch 970, training loss: 2.707740545272827, val loss: 2.708592653274536, val_acc: 0.2084848484848485\n",
      "Epoch 980, training loss: 2.6968092918395996, val loss: 2.7054755687713623, val_acc: 0.21575757575757576\n",
      "Epoch 990, training loss: 2.670152425765991, val loss: 2.700526714324951, val_acc: 0.2109090909090909\n",
      "Epoch 1000, training loss: 2.6462771892547607, val loss: 2.6913816928863525, val_acc: 0.21575757575757576\n",
      "Epoch 1010, training loss: 2.69461989402771, val loss: 2.692091703414917, val_acc: 0.21818181818181817\n",
      "Epoch 1020, training loss: 2.681281805038452, val loss: 2.6903915405273438, val_acc: 0.21454545454545454\n",
      "Epoch 1030, training loss: 2.6499829292297363, val loss: 2.6855154037475586, val_acc: 0.22787878787878788\n",
      "Epoch 1040, training loss: 2.68618106842041, val loss: 2.6782639026641846, val_acc: 0.23272727272727273\n",
      "Epoch 1050, training loss: 2.668687105178833, val loss: 2.6848342418670654, val_acc: 0.22424242424242424\n",
      "Epoch 1060, training loss: 2.6940081119537354, val loss: 2.6882822513580322, val_acc: 0.22666666666666666\n",
      "Epoch 1070, training loss: 2.687894821166992, val loss: 2.690430164337158, val_acc: 0.21696969696969698\n",
      "Epoch 1080, training loss: 2.706057548522949, val loss: 2.68862247467041, val_acc: 0.22545454545454546\n",
      "Epoch 1090, training loss: 2.64489483833313, val loss: 2.678548812866211, val_acc: 0.22545454545454546\n",
      "Epoch 1100, training loss: 2.6686160564422607, val loss: 2.6756672859191895, val_acc: 0.23272727272727273\n",
      "Epoch 1110, training loss: 2.678396701812744, val loss: 2.684922695159912, val_acc: 0.20606060606060606\n",
      "Epoch 1120, training loss: 2.662367820739746, val loss: 2.6832640171051025, val_acc: 0.22424242424242424\n",
      "Epoch 1130, training loss: 2.6406123638153076, val loss: 2.6758902072906494, val_acc: 0.20484848484848484\n",
      "Epoch 1140, training loss: 2.6578190326690674, val loss: 2.674978017807007, val_acc: 0.2290909090909091\n",
      "Epoch 1150, training loss: 2.6413755416870117, val loss: 2.677316904067993, val_acc: 0.2096969696969697\n",
      "Epoch 1160, training loss: 2.6665494441986084, val loss: 2.690032482147217, val_acc: 0.2096969696969697\n",
      "Epoch 1170, training loss: 2.623225688934326, val loss: 2.6714043617248535, val_acc: 0.21212121212121213\n",
      "Epoch 1180, training loss: 2.6749088764190674, val loss: 2.6761934757232666, val_acc: 0.21818181818181817\n",
      "Epoch 1190, training loss: 2.702871084213257, val loss: 2.6750214099884033, val_acc: 0.21575757575757576\n",
      "Epoch 1200, training loss: 2.664607048034668, val loss: 2.6679627895355225, val_acc: 0.22787878787878788\n",
      "Epoch 1210, training loss: 2.633995771408081, val loss: 2.6590335369110107, val_acc: 0.23030303030303031\n",
      "Epoch 1220, training loss: 2.669384479522705, val loss: 2.6888349056243896, val_acc: 0.2193939393939394\n",
      "Epoch 1230, training loss: 2.6635000705718994, val loss: 2.6589179039001465, val_acc: 0.24242424242424243\n",
      "Epoch 1240, training loss: 2.6642796993255615, val loss: 2.664480447769165, val_acc: 0.23272727272727273\n",
      "Epoch 1250, training loss: 2.63810396194458, val loss: 2.6673009395599365, val_acc: 0.2290909090909091\n",
      "Epoch 1260, training loss: 2.648475170135498, val loss: 2.663227081298828, val_acc: 0.2315151515151515\n",
      "Epoch 1270, training loss: 2.6656546592712402, val loss: 2.664296865463257, val_acc: 0.23757575757575758\n",
      "Epoch 1280, training loss: 2.6201488971710205, val loss: 2.654142379760742, val_acc: 0.22787878787878788\n",
      "Epoch 1290, training loss: 2.6457366943359375, val loss: 2.675476551055908, val_acc: 0.20727272727272728\n",
      "Epoch 1300, training loss: 2.671269655227661, val loss: 2.6580090522766113, val_acc: 0.22545454545454546\n",
      "Epoch 1310, training loss: 2.6378443241119385, val loss: 2.6542530059814453, val_acc: 0.2509090909090909\n",
      "Epoch 1320, training loss: 2.662043333053589, val loss: 2.6748201847076416, val_acc: 0.21818181818181817\n",
      "Epoch 1330, training loss: 2.6195857524871826, val loss: 2.651543378829956, val_acc: 0.23272727272727273\n",
      "Epoch 1340, training loss: 2.6518828868865967, val loss: 2.6594769954681396, val_acc: 0.23636363636363636\n",
      "Epoch 1350, training loss: 2.677816390991211, val loss: 2.660055160522461, val_acc: 0.21575757575757576\n",
      "Epoch 1360, training loss: 2.6344192028045654, val loss: 2.648994207382202, val_acc: 0.22181818181818183\n",
      "Epoch 1370, training loss: 2.637444257736206, val loss: 2.6619741916656494, val_acc: 0.21818181818181817\n",
      "Epoch 1380, training loss: 2.6514532566070557, val loss: 2.6551332473754883, val_acc: 0.22787878787878788\n",
      "Epoch 1390, training loss: 2.6319823265075684, val loss: 2.647451162338257, val_acc: 0.24848484848484848\n",
      "Epoch 1400, training loss: 2.652129888534546, val loss: 2.6391875743865967, val_acc: 0.2412121212121212\n",
      "Epoch 1410, training loss: 2.629971742630005, val loss: 2.6467761993408203, val_acc: 0.23757575757575758\n",
      "Epoch 1420, training loss: 2.604656934738159, val loss: 2.6572213172912598, val_acc: 0.23272727272727273\n",
      "Epoch 1430, training loss: 2.6493237018585205, val loss: 2.631232738494873, val_acc: 0.24\n",
      "Epoch 1440, training loss: 2.616217613220215, val loss: 2.655121326446533, val_acc: 0.22787878787878788\n",
      "Epoch 1450, training loss: 2.6433191299438477, val loss: 2.656543016433716, val_acc: 0.22787878787878788\n",
      "Epoch 1460, training loss: 2.59668231010437, val loss: 2.6423826217651367, val_acc: 0.23636363636363636\n",
      "Epoch 1470, training loss: 2.625425338745117, val loss: 2.6529181003570557, val_acc: 0.23272727272727273\n",
      "Epoch 1480, training loss: 2.5921201705932617, val loss: 2.645864486694336, val_acc: 0.22666666666666666\n",
      "Epoch 1490, training loss: 2.599055767059326, val loss: 2.64323353767395, val_acc: 0.24484848484848484\n",
      "Epoch 1500, training loss: 2.6214475631713867, val loss: 2.6516127586364746, val_acc: 0.2387878787878788\n",
      "Epoch 1510, training loss: 2.650283098220825, val loss: 2.6461989879608154, val_acc: 0.24606060606060606\n",
      "Epoch 1520, training loss: 2.6070473194122314, val loss: 2.6331794261932373, val_acc: 0.24\n",
      "Epoch 1530, training loss: 2.643404245376587, val loss: 2.6475746631622314, val_acc: 0.22545454545454546\n",
      "Epoch 1540, training loss: 2.6271252632141113, val loss: 2.630234718322754, val_acc: 0.24484848484848484\n",
      "Epoch 1550, training loss: 2.629369020462036, val loss: 2.625922918319702, val_acc: 0.2412121212121212\n",
      "Epoch 1560, training loss: 2.6132354736328125, val loss: 2.6507036685943604, val_acc: 0.2206060606060606\n",
      "Epoch 1570, training loss: 2.617100477218628, val loss: 2.6382827758789062, val_acc: 0.21818181818181817\n",
      "Epoch 1580, training loss: 2.6390209197998047, val loss: 2.631728172302246, val_acc: 0.25212121212121213\n",
      "Epoch 1590, training loss: 2.613995313644409, val loss: 2.633253335952759, val_acc: 0.24\n",
      "Epoch 1600, training loss: 2.6012587547302246, val loss: 2.646723508834839, val_acc: 0.23515151515151514\n",
      "Epoch 1610, training loss: 2.647325038909912, val loss: 2.63192081451416, val_acc: 0.23393939393939395\n",
      "Epoch 1620, training loss: 2.588724136352539, val loss: 2.6169610023498535, val_acc: 0.2581818181818182\n",
      "Epoch 1630, training loss: 2.610891819000244, val loss: 2.6461849212646484, val_acc: 0.23272727272727273\n",
      "Epoch 1640, training loss: 2.602631092071533, val loss: 2.6304750442504883, val_acc: 0.2387878787878788\n",
      "Epoch 1650, training loss: 2.5978176593780518, val loss: 2.629354953765869, val_acc: 0.23515151515151514\n",
      "Epoch 1660, training loss: 2.630389928817749, val loss: 2.629476308822632, val_acc: 0.2290909090909091\n",
      "Epoch 1670, training loss: 2.6158714294433594, val loss: 2.6172091960906982, val_acc: 0.24848484848484848\n",
      "Epoch 1680, training loss: 2.608029842376709, val loss: 2.628981113433838, val_acc: 0.24\n",
      "Epoch 1690, training loss: 2.6093595027923584, val loss: 2.633216142654419, val_acc: 0.24\n",
      "Epoch 1700, training loss: 2.6089675426483154, val loss: 2.619880199432373, val_acc: 0.23515151515151514\n",
      "Epoch 1710, training loss: 2.5976309776306152, val loss: 2.621568441390991, val_acc: 0.2387878787878788\n",
      "Epoch 1720, training loss: 2.6418328285217285, val loss: 2.6263718605041504, val_acc: 0.24363636363636362\n",
      "Epoch 1730, training loss: 2.5798425674438477, val loss: 2.6247904300689697, val_acc: 0.23030303030303031\n",
      "Epoch 1740, training loss: 2.6384074687957764, val loss: 2.6217167377471924, val_acc: 0.2315151515151515\n",
      "Epoch 1750, training loss: 2.5832600593566895, val loss: 2.636469841003418, val_acc: 0.2206060606060606\n",
      "Epoch 1760, training loss: 2.597557544708252, val loss: 2.631565809249878, val_acc: 0.23393939393939395\n",
      "Epoch 1770, training loss: 2.5568714141845703, val loss: 2.623905658721924, val_acc: 0.2315151515151515\n",
      "Epoch 1780, training loss: 2.5767006874084473, val loss: 2.625986337661743, val_acc: 0.24848484848484848\n",
      "Epoch 1790, training loss: 2.599698543548584, val loss: 2.638843297958374, val_acc: 0.23393939393939395\n",
      "Epoch 1800, training loss: 2.6041853427886963, val loss: 2.6196975708007812, val_acc: 0.23515151515151514\n",
      "Epoch 1810, training loss: 2.568385124206543, val loss: 2.6320371627807617, val_acc: 0.22787878787878788\n",
      "Epoch 1820, training loss: 2.624692678451538, val loss: 2.6185665130615234, val_acc: 0.23636363636363636\n",
      "Epoch 1830, training loss: 2.5592949390411377, val loss: 2.6151578426361084, val_acc: 0.2387878787878788\n",
      "Epoch 1840, training loss: 2.595140218734741, val loss: 2.623075246810913, val_acc: 0.23757575757575758\n",
      "Epoch 1850, training loss: 2.6039600372314453, val loss: 2.617465019226074, val_acc: 0.24363636363636362\n",
      "Epoch 1860, training loss: 2.594330310821533, val loss: 2.6153862476348877, val_acc: 0.2387878787878788\n",
      "Epoch 1870, training loss: 2.592669725418091, val loss: 2.620680093765259, val_acc: 0.23636363636363636\n",
      "Epoch 1880, training loss: 2.611112117767334, val loss: 2.6065921783447266, val_acc: 0.24606060606060606\n",
      "Epoch 1890, training loss: 2.5829639434814453, val loss: 2.611194133758545, val_acc: 0.2496969696969697\n",
      "Epoch 1900, training loss: 2.6025683879852295, val loss: 2.6031978130340576, val_acc: 0.24\n",
      "Epoch 1910, training loss: 2.5972118377685547, val loss: 2.6168956756591797, val_acc: 0.25212121212121213\n",
      "Epoch 1920, training loss: 2.5876855850219727, val loss: 2.6240272521972656, val_acc: 0.23636363636363636\n",
      "Epoch 1930, training loss: 2.6125259399414062, val loss: 2.6083035469055176, val_acc: 0.24484848484848484\n",
      "Epoch 1940, training loss: 2.5442047119140625, val loss: 2.6066980361938477, val_acc: 0.24606060606060606\n",
      "Epoch 1950, training loss: 2.5684986114501953, val loss: 2.6095004081726074, val_acc: 0.24\n",
      "Epoch 1960, training loss: 2.552316427230835, val loss: 2.60831618309021, val_acc: 0.24\n",
      "Epoch 1970, training loss: 2.5856668949127197, val loss: 2.611056089401245, val_acc: 0.23515151515151514\n",
      "Epoch 1980, training loss: 2.5458762645721436, val loss: 2.6107943058013916, val_acc: 0.2412121212121212\n",
      "Epoch 1990, training loss: 2.5596370697021484, val loss: 2.61458158493042, val_acc: 0.2496969696969697\n",
      "Epoch 2000, training loss: 2.576341390609741, val loss: 2.6163554191589355, val_acc: 0.24\n",
      "Epoch 2010, training loss: 2.564723253250122, val loss: 2.604348659515381, val_acc: 0.24242424242424243\n",
      "Epoch 2020, training loss: 2.58911395072937, val loss: 2.601996660232544, val_acc: 0.23030303030303031\n",
      "Epoch 2030, training loss: 2.5830941200256348, val loss: 2.619067907333374, val_acc: 0.23636363636363636\n",
      "Epoch 2040, training loss: 2.624190330505371, val loss: 2.6013989448547363, val_acc: 0.22666666666666666\n",
      "Epoch 2050, training loss: 2.584839105606079, val loss: 2.5923268795013428, val_acc: 0.22666666666666666\n",
      "Epoch 2060, training loss: 2.577122688293457, val loss: 2.6067581176757812, val_acc: 0.25212121212121213\n",
      "Epoch 2070, training loss: 2.5638718605041504, val loss: 2.6103365421295166, val_acc: 0.2387878787878788\n",
      "Epoch 2080, training loss: 2.574265956878662, val loss: 2.596505641937256, val_acc: 0.26181818181818184\n",
      "Epoch 2090, training loss: 2.537733554840088, val loss: 2.600583076477051, val_acc: 0.23272727272727273\n",
      "Epoch 2100, training loss: 2.565124034881592, val loss: 2.5960943698883057, val_acc: 0.23272727272727273\n",
      "Epoch 2110, training loss: 2.5692553520202637, val loss: 2.598604202270508, val_acc: 0.2496969696969697\n",
      "Epoch 2120, training loss: 2.5433530807495117, val loss: 2.6190543174743652, val_acc: 0.24242424242424243\n",
      "Epoch 2130, training loss: 2.5271201133728027, val loss: 2.5912740230560303, val_acc: 0.27515151515151515\n",
      "Epoch 2140, training loss: 2.5812318325042725, val loss: 2.5992331504821777, val_acc: 0.24484848484848484\n",
      "Epoch 2150, training loss: 2.5840954780578613, val loss: 2.6118462085723877, val_acc: 0.24363636363636362\n",
      "Epoch 2160, training loss: 2.5568017959594727, val loss: 2.596672296524048, val_acc: 0.24606060606060606\n",
      "Epoch 2170, training loss: 2.588663101196289, val loss: 2.595432758331299, val_acc: 0.2509090909090909\n",
      "Epoch 2180, training loss: 2.576986074447632, val loss: 2.6055757999420166, val_acc: 0.24848484848484848\n",
      "Epoch 2190, training loss: 2.5727486610412598, val loss: 2.5985171794891357, val_acc: 0.24606060606060606\n",
      "Epoch 2200, training loss: 2.5888662338256836, val loss: 2.5966780185699463, val_acc: 0.25212121212121213\n",
      "Epoch 2210, training loss: 2.606328248977661, val loss: 2.596517562866211, val_acc: 0.23636363636363636\n",
      "Epoch 2220, training loss: 2.574368953704834, val loss: 2.6079161167144775, val_acc: 0.22666666666666666\n",
      "Epoch 2230, training loss: 2.56435489654541, val loss: 2.599555253982544, val_acc: 0.2387878787878788\n",
      "Epoch 2240, training loss: 2.580348253250122, val loss: 2.5910656452178955, val_acc: 0.22787878787878788\n",
      "Epoch 2250, training loss: 2.5589075088500977, val loss: 2.5913357734680176, val_acc: 0.24484848484848484\n",
      "Epoch 2260, training loss: 2.531442403793335, val loss: 2.5964205265045166, val_acc: 0.2496969696969697\n",
      "Epoch 2270, training loss: 2.579511880874634, val loss: 2.598686933517456, val_acc: 0.23636363636363636\n",
      "Epoch 2280, training loss: 2.5693154335021973, val loss: 2.596824884414673, val_acc: 0.24727272727272728\n",
      "Epoch 2290, training loss: 2.572873115539551, val loss: 2.58064603805542, val_acc: 0.26181818181818184\n",
      "Epoch 2300, training loss: 2.5020811557769775, val loss: 2.5930068492889404, val_acc: 0.24606060606060606\n",
      "Epoch 2310, training loss: 2.575584650039673, val loss: 2.5904643535614014, val_acc: 0.25212121212121213\n",
      "Epoch 2320, training loss: 2.531085729598999, val loss: 2.5939784049987793, val_acc: 0.263030303030303\n",
      "Epoch 2330, training loss: 2.548570156097412, val loss: 2.5891923904418945, val_acc: 0.2509090909090909\n",
      "Epoch 2340, training loss: 2.5757336616516113, val loss: 2.597522258758545, val_acc: 0.24848484848484848\n",
      "Epoch 2350, training loss: 2.570223569869995, val loss: 2.5873019695281982, val_acc: 0.2412121212121212\n",
      "Epoch 2360, training loss: 2.5605905055999756, val loss: 2.597170352935791, val_acc: 0.24727272727272728\n",
      "Epoch 2370, training loss: 2.586803674697876, val loss: 2.587449073791504, val_acc: 0.25212121212121213\n",
      "Epoch 2380, training loss: 2.5582172870635986, val loss: 2.584350109100342, val_acc: 0.2581818181818182\n",
      "Epoch 2390, training loss: 2.552537679672241, val loss: 2.5927722454071045, val_acc: 0.23030303030303031\n",
      "Epoch 2400, training loss: 2.5410938262939453, val loss: 2.581792116165161, val_acc: 0.24242424242424243\n",
      "Epoch 2410, training loss: 2.56563401222229, val loss: 2.5753800868988037, val_acc: 0.24363636363636362\n",
      "Epoch 2420, training loss: 2.5646469593048096, val loss: 2.59291410446167, val_acc: 0.24848484848484848\n",
      "Epoch 2430, training loss: 2.5287022590637207, val loss: 2.58390474319458, val_acc: 0.2581818181818182\n",
      "Epoch 2440, training loss: 2.5545175075531006, val loss: 2.5719001293182373, val_acc: 0.2642424242424242\n",
      "Epoch 2450, training loss: 2.5390188694000244, val loss: 2.58748197555542, val_acc: 0.263030303030303\n",
      "Epoch 2460, training loss: 2.5339601039886475, val loss: 2.591251850128174, val_acc: 0.2606060606060606\n",
      "Epoch 2470, training loss: 2.5839529037475586, val loss: 2.5860331058502197, val_acc: 0.2545454545454545\n",
      "Epoch 2480, training loss: 2.5295984745025635, val loss: 2.605221748352051, val_acc: 0.24727272727272728\n",
      "Epoch 2490, training loss: 2.588547468185425, val loss: 2.5837295055389404, val_acc: 0.25333333333333335\n",
      "Epoch 2500, training loss: 2.530001163482666, val loss: 2.5846312046051025, val_acc: 0.25212121212121213\n",
      "Epoch 2510, training loss: 2.529395341873169, val loss: 2.581859827041626, val_acc: 0.26545454545454544\n",
      "Epoch 2520, training loss: 2.5425310134887695, val loss: 2.5807058811187744, val_acc: 0.2387878787878788\n",
      "Epoch 2530, training loss: 2.5409348011016846, val loss: 2.5774576663970947, val_acc: 0.24727272727272728\n",
      "Epoch 2540, training loss: 2.5080089569091797, val loss: 2.581557512283325, val_acc: 0.24484848484848484\n",
      "Epoch 2550, training loss: 2.560051202774048, val loss: 2.5886006355285645, val_acc: 0.24484848484848484\n",
      "Epoch 2560, training loss: 2.535905599594116, val loss: 2.572129487991333, val_acc: 0.26666666666666666\n",
      "Epoch 2570, training loss: 2.5521204471588135, val loss: 2.5653867721557617, val_acc: 0.25575757575757574\n",
      "Epoch 2580, training loss: 2.5231926441192627, val loss: 2.581002712249756, val_acc: 0.24727272727272728\n",
      "Epoch 2590, training loss: 2.514181613922119, val loss: 2.584731340408325, val_acc: 0.24606060606060606\n",
      "Epoch 2600, training loss: 2.550753355026245, val loss: 2.5805490016937256, val_acc: 0.24242424242424243\n",
      "Epoch 2610, training loss: 2.5840513706207275, val loss: 2.5801284313201904, val_acc: 0.2581818181818182\n",
      "Epoch 2620, training loss: 2.489996910095215, val loss: 2.5747733116149902, val_acc: 0.25696969696969696\n",
      "Epoch 2630, training loss: 2.510467290878296, val loss: 2.575261354446411, val_acc: 0.24727272727272728\n",
      "Epoch 2640, training loss: 2.5900497436523438, val loss: 2.566669225692749, val_acc: 0.25696969696969696\n",
      "Epoch 2650, training loss: 2.524132490158081, val loss: 2.5695791244506836, val_acc: 0.2496969696969697\n",
      "Epoch 2660, training loss: 2.520000696182251, val loss: 2.578979730606079, val_acc: 0.24606060606060606\n",
      "Epoch 2670, training loss: 2.5429482460021973, val loss: 2.576986789703369, val_acc: 0.24848484848484848\n",
      "Epoch 2680, training loss: 2.5047683715820312, val loss: 2.574554920196533, val_acc: 0.26181818181818184\n",
      "Epoch 2690, training loss: 2.5329253673553467, val loss: 2.5771682262420654, val_acc: 0.2545454545454545\n",
      "Epoch 2700, training loss: 2.5262458324432373, val loss: 2.572370767593384, val_acc: 0.2581818181818182\n",
      "Epoch 2710, training loss: 2.546435832977295, val loss: 2.570481777191162, val_acc: 0.2509090909090909\n",
      "Epoch 2720, training loss: 2.505929708480835, val loss: 2.5679116249084473, val_acc: 0.2593939393939394\n",
      "Epoch 2730, training loss: 2.5405731201171875, val loss: 2.574021339416504, val_acc: 0.2581818181818182\n",
      "Epoch 2740, training loss: 2.551394462585449, val loss: 2.564296007156372, val_acc: 0.2593939393939394\n",
      "Epoch 2750, training loss: 2.5295498371124268, val loss: 2.574946641921997, val_acc: 0.25212121212121213\n",
      "Epoch 2760, training loss: 2.4980785846710205, val loss: 2.568563461303711, val_acc: 0.25575757575757574\n",
      "Epoch 2770, training loss: 2.4957215785980225, val loss: 2.554647922515869, val_acc: 0.2593939393939394\n",
      "Epoch 2780, training loss: 2.532287359237671, val loss: 2.5640687942504883, val_acc: 0.26545454545454544\n",
      "Epoch 2790, training loss: 2.538926124572754, val loss: 2.5649447441101074, val_acc: 0.25575757575757574\n",
      "Epoch 2800, training loss: 2.574634552001953, val loss: 2.561394453048706, val_acc: 0.2739393939393939\n",
      "Epoch 2810, training loss: 2.510472059249878, val loss: 2.570347785949707, val_acc: 0.2606060606060606\n",
      "Epoch 2820, training loss: 2.5029568672180176, val loss: 2.5833182334899902, val_acc: 0.24727272727272728\n",
      "Epoch 2830, training loss: 2.5104868412017822, val loss: 2.5746209621429443, val_acc: 0.24727272727272728\n",
      "Epoch 2840, training loss: 2.529132843017578, val loss: 2.573090076446533, val_acc: 0.2593939393939394\n",
      "Epoch 2850, training loss: 2.4954583644866943, val loss: 2.555363655090332, val_acc: 0.2496969696969697\n",
      "Epoch 2860, training loss: 2.5544180870056152, val loss: 2.556654453277588, val_acc: 0.2703030303030303\n",
      "Epoch 2870, training loss: 2.478414297103882, val loss: 2.5757360458374023, val_acc: 0.24848484848484848\n",
      "Epoch 2880, training loss: 2.498680830001831, val loss: 2.586805820465088, val_acc: 0.24727272727272728\n",
      "Epoch 2890, training loss: 2.540482759475708, val loss: 2.5650503635406494, val_acc: 0.2545454545454545\n",
      "Epoch 2900, training loss: 2.523024320602417, val loss: 2.556511878967285, val_acc: 0.26666666666666666\n",
      "Epoch 2910, training loss: 2.4827308654785156, val loss: 2.5755679607391357, val_acc: 0.24727272727272728\n",
      "Epoch 2920, training loss: 2.5234007835388184, val loss: 2.5600807666778564, val_acc: 0.25333333333333335\n",
      "Epoch 2930, training loss: 2.5425024032592773, val loss: 2.5511724948883057, val_acc: 0.2642424242424242\n",
      "Epoch 2940, training loss: 2.5376462936401367, val loss: 2.5656864643096924, val_acc: 0.2509090909090909\n",
      "Epoch 2950, training loss: 2.543260097503662, val loss: 2.5779213905334473, val_acc: 0.2509090909090909\n",
      "Epoch 2960, training loss: 2.5109434127807617, val loss: 2.5745208263397217, val_acc: 0.24727272727272728\n",
      "Epoch 2970, training loss: 2.4744205474853516, val loss: 2.5710036754608154, val_acc: 0.25333333333333335\n",
      "Epoch 2980, training loss: 2.5029025077819824, val loss: 2.564807415008545, val_acc: 0.2496969696969697\n",
      "Epoch 2990, training loss: 2.560802698135376, val loss: 2.568286657333374, val_acc: 0.2606060606060606\n",
      "Epoch 3000, training loss: 2.5306787490844727, val loss: 2.5697710514068604, val_acc: 0.2496969696969697\n",
      "Epoch 3010, training loss: 2.5054030418395996, val loss: 2.5595269203186035, val_acc: 0.25575757575757574\n",
      "Epoch 3020, training loss: 2.5072622299194336, val loss: 2.5686302185058594, val_acc: 0.25212121212121213\n",
      "Epoch 3030, training loss: 2.5434727668762207, val loss: 2.5601606369018555, val_acc: 0.26181818181818184\n",
      "Epoch 3040, training loss: 2.5061421394348145, val loss: 2.5655550956726074, val_acc: 0.2545454545454545\n",
      "Epoch 3050, training loss: 2.486910581588745, val loss: 2.5506105422973633, val_acc: 0.2606060606060606\n",
      "Epoch 3060, training loss: 2.5232367515563965, val loss: 2.560607433319092, val_acc: 0.26181818181818184\n",
      "Epoch 3070, training loss: 2.5291011333465576, val loss: 2.569301128387451, val_acc: 0.24\n",
      "Epoch 3080, training loss: 2.485354423522949, val loss: 2.5508551597595215, val_acc: 0.2703030303030303\n",
      "Epoch 3090, training loss: 2.505801200866699, val loss: 2.5586628913879395, val_acc: 0.2545454545454545\n",
      "Epoch 3100, training loss: 2.4887027740478516, val loss: 2.552034616470337, val_acc: 0.2581818181818182\n",
      "Epoch 3110, training loss: 2.5663506984710693, val loss: 2.5531885623931885, val_acc: 0.25696969696969696\n",
      "Epoch 3120, training loss: 2.502171039581299, val loss: 2.5569851398468018, val_acc: 0.24848484848484848\n",
      "Epoch 3130, training loss: 2.494852304458618, val loss: 2.549024820327759, val_acc: 0.26666666666666666\n",
      "Epoch 3140, training loss: 2.5029590129852295, val loss: 2.55625581741333, val_acc: 0.25333333333333335\n",
      "Epoch 3150, training loss: 2.543699026107788, val loss: 2.560518503189087, val_acc: 0.2606060606060606\n",
      "Epoch 3160, training loss: 2.504227876663208, val loss: 2.5511739253997803, val_acc: 0.2545454545454545\n",
      "Epoch 3170, training loss: 2.5113210678100586, val loss: 2.553715467453003, val_acc: 0.25212121212121213\n",
      "Epoch 3180, training loss: 2.501669406890869, val loss: 2.5522265434265137, val_acc: 0.25696969696969696\n",
      "Epoch 3190, training loss: 2.538745164871216, val loss: 2.5699925422668457, val_acc: 0.24\n",
      "Epoch 3200, training loss: 2.5282211303710938, val loss: 2.5703516006469727, val_acc: 0.24242424242424243\n",
      "Epoch 3210, training loss: 2.5207557678222656, val loss: 2.5562245845794678, val_acc: 0.2593939393939394\n",
      "Epoch 3220, training loss: 2.5329227447509766, val loss: 2.5710203647613525, val_acc: 0.24606060606060606\n",
      "Epoch 3230, training loss: 2.5289831161499023, val loss: 2.5611023902893066, val_acc: 0.25212121212121213\n",
      "Epoch 3240, training loss: 2.4992380142211914, val loss: 2.554986000061035, val_acc: 0.26181818181818184\n",
      "Epoch 3250, training loss: 2.4901487827301025, val loss: 2.542266845703125, val_acc: 0.2642424242424242\n",
      "Epoch 3260, training loss: 2.5184552669525146, val loss: 2.5526492595672607, val_acc: 0.2593939393939394\n",
      "Epoch 3270, training loss: 2.5029077529907227, val loss: 2.5521466732025146, val_acc: 0.2727272727272727\n",
      "Epoch 3280, training loss: 2.519595146179199, val loss: 2.550949811935425, val_acc: 0.2642424242424242\n",
      "Epoch 3290, training loss: 2.500307321548462, val loss: 2.539771556854248, val_acc: 0.263030303030303\n",
      "Epoch 3300, training loss: 2.509502649307251, val loss: 2.554368495941162, val_acc: 0.26545454545454544\n",
      "Epoch 3310, training loss: 2.507326126098633, val loss: 2.543039083480835, val_acc: 0.2606060606060606\n",
      "Epoch 3320, training loss: 2.521963596343994, val loss: 2.5480074882507324, val_acc: 0.2787878787878788\n",
      "Epoch 3330, training loss: 2.498213052749634, val loss: 2.544218063354492, val_acc: 0.2545454545454545\n",
      "Epoch 3340, training loss: 2.493364095687866, val loss: 2.54693865776062, val_acc: 0.263030303030303\n",
      "Epoch 3350, training loss: 2.4950199127197266, val loss: 2.5624618530273438, val_acc: 0.2509090909090909\n",
      "Epoch 3360, training loss: 2.515397310256958, val loss: 2.547691583633423, val_acc: 0.26545454545454544\n",
      "Epoch 3370, training loss: 2.4858546257019043, val loss: 2.5418343544006348, val_acc: 0.25212121212121213\n",
      "Epoch 3380, training loss: 2.466991901397705, val loss: 2.5394370555877686, val_acc: 0.2593939393939394\n",
      "Epoch 3390, training loss: 2.490969181060791, val loss: 2.549874782562256, val_acc: 0.263030303030303\n",
      "Epoch 3400, training loss: 2.477069616317749, val loss: 2.5457630157470703, val_acc: 0.25575757575757574\n",
      "Epoch 3410, training loss: 2.5057132244110107, val loss: 2.5382187366485596, val_acc: 0.2581818181818182\n",
      "Epoch 3420, training loss: 2.5320751667022705, val loss: 2.552553653717041, val_acc: 0.24727272727272728\n",
      "Epoch 3430, training loss: 2.506883382797241, val loss: 2.5553483963012695, val_acc: 0.2678787878787879\n",
      "Epoch 3440, training loss: 2.4818923473358154, val loss: 2.540334939956665, val_acc: 0.25696969696969696\n",
      "Epoch 3450, training loss: 2.486175060272217, val loss: 2.5452077388763428, val_acc: 0.2545454545454545\n",
      "Epoch 3460, training loss: 2.49123477935791, val loss: 2.5444953441619873, val_acc: 0.26666666666666666\n",
      "Epoch 3470, training loss: 2.4340977668762207, val loss: 2.546104669570923, val_acc: 0.27151515151515154\n",
      "Epoch 3480, training loss: 2.497624397277832, val loss: 2.5631518363952637, val_acc: 0.24727272727272728\n",
      "Epoch 3490, training loss: 2.5091350078582764, val loss: 2.5439329147338867, val_acc: 0.2642424242424242\n",
      "Epoch 3500, training loss: 2.5024116039276123, val loss: 2.539763927459717, val_acc: 0.26181818181818184\n",
      "Epoch 3510, training loss: 2.507366418838501, val loss: 2.5547451972961426, val_acc: 0.25333333333333335\n",
      "Epoch 3520, training loss: 2.518157482147217, val loss: 2.532949924468994, val_acc: 0.2545454545454545\n",
      "Epoch 3530, training loss: 2.502826690673828, val loss: 2.5358519554138184, val_acc: 0.27151515151515154\n",
      "Epoch 3540, training loss: 2.495438575744629, val loss: 2.5357513427734375, val_acc: 0.2642424242424242\n",
      "Epoch 3550, training loss: 2.474332094192505, val loss: 2.535240650177002, val_acc: 0.27515151515151515\n",
      "Epoch 3560, training loss: 2.473684787750244, val loss: 2.53615140914917, val_acc: 0.2606060606060606\n",
      "Epoch 3570, training loss: 2.501974582672119, val loss: 2.541677713394165, val_acc: 0.2690909090909091\n",
      "Epoch 3580, training loss: 2.4764161109924316, val loss: 2.55560564994812, val_acc: 0.2545454545454545\n",
      "Epoch 3590, training loss: 2.503911256790161, val loss: 2.5364186763763428, val_acc: 0.27636363636363637\n",
      "Epoch 3600, training loss: 2.4774019718170166, val loss: 2.5535881519317627, val_acc: 0.25575757575757574\n",
      "Epoch 3610, training loss: 2.468592882156372, val loss: 2.553770065307617, val_acc: 0.2581818181818182\n",
      "Epoch 3620, training loss: 2.4553678035736084, val loss: 2.545992851257324, val_acc: 0.2703030303030303\n",
      "Epoch 3630, training loss: 2.496183156967163, val loss: 2.5374526977539062, val_acc: 0.2703030303030303\n",
      "Epoch 3640, training loss: 2.52294659614563, val loss: 2.5498921871185303, val_acc: 0.26666666666666666\n",
      "Epoch 3650, training loss: 2.492541790008545, val loss: 2.5559558868408203, val_acc: 0.25575757575757574\n",
      "Epoch 3660, training loss: 2.4970574378967285, val loss: 2.535027027130127, val_acc: 0.25696969696969696\n",
      "Epoch 3670, training loss: 2.4981722831726074, val loss: 2.541239023208618, val_acc: 0.25696969696969696\n",
      "Epoch 3680, training loss: 2.472139835357666, val loss: 2.5567145347595215, val_acc: 0.25575757575757574\n",
      "Epoch 3690, training loss: 2.473233222961426, val loss: 2.5333428382873535, val_acc: 0.2642424242424242\n",
      "Epoch 3700, training loss: 2.489759922027588, val loss: 2.545828342437744, val_acc: 0.24606060606060606\n",
      "Epoch 3710, training loss: 2.4310662746429443, val loss: 2.5366547107696533, val_acc: 0.2678787878787879\n",
      "Epoch 3720, training loss: 2.463773727416992, val loss: 2.5621588230133057, val_acc: 0.25333333333333335\n",
      "Epoch 3730, training loss: 2.479501962661743, val loss: 2.5619161128997803, val_acc: 0.26545454545454544\n",
      "Epoch 3740, training loss: 2.4958457946777344, val loss: 2.540149688720703, val_acc: 0.2593939393939394\n",
      "Epoch 3750, training loss: 2.445770740509033, val loss: 2.5363752841949463, val_acc: 0.2690909090909091\n",
      "Epoch 3760, training loss: 2.442582845687866, val loss: 2.528796434402466, val_acc: 0.27151515151515154\n",
      "Epoch 3770, training loss: 2.4971933364868164, val loss: 2.531331777572632, val_acc: 0.26666666666666666\n",
      "Epoch 3780, training loss: 2.4872584342956543, val loss: 2.5223312377929688, val_acc: 0.2642424242424242\n",
      "Epoch 3790, training loss: 2.5001752376556396, val loss: 2.535618543624878, val_acc: 0.2678787878787879\n",
      "Epoch 3800, training loss: 2.4436826705932617, val loss: 2.5370516777038574, val_acc: 0.2606060606060606\n",
      "Epoch 3810, training loss: 2.530653953552246, val loss: 2.5391807556152344, val_acc: 0.2690909090909091\n",
      "Epoch 3820, training loss: 2.4398536682128906, val loss: 2.5218262672424316, val_acc: 0.25212121212121213\n",
      "Epoch 3830, training loss: 2.4657950401306152, val loss: 2.5430781841278076, val_acc: 0.2739393939393939\n",
      "Epoch 3840, training loss: 2.470088005065918, val loss: 2.54310941696167, val_acc: 0.2642424242424242\n",
      "Epoch 3850, training loss: 2.463283061981201, val loss: 2.542207956314087, val_acc: 0.2545454545454545\n",
      "Epoch 3860, training loss: 2.4820058345794678, val loss: 2.5342180728912354, val_acc: 0.2678787878787879\n",
      "Epoch 3870, training loss: 2.473834991455078, val loss: 2.5453603267669678, val_acc: 0.25575757575757574\n",
      "Epoch 3880, training loss: 2.4317405223846436, val loss: 2.5279550552368164, val_acc: 0.26181818181818184\n",
      "Epoch 3890, training loss: 2.4993231296539307, val loss: 2.518810272216797, val_acc: 0.263030303030303\n",
      "Epoch 3900, training loss: 2.4674630165100098, val loss: 2.5296194553375244, val_acc: 0.26545454545454544\n",
      "Epoch 3910, training loss: 2.4806160926818848, val loss: 2.538942575454712, val_acc: 0.2727272727272727\n",
      "Epoch 3920, training loss: 2.471761465072632, val loss: 2.5427770614624023, val_acc: 0.2678787878787879\n",
      "Epoch 3930, training loss: 2.478177070617676, val loss: 2.5308244228363037, val_acc: 0.25575757575757574\n",
      "Epoch 3940, training loss: 2.4920592308044434, val loss: 2.5347917079925537, val_acc: 0.2606060606060606\n",
      "Epoch 3950, training loss: 2.4495296478271484, val loss: 2.5287113189697266, val_acc: 0.2690909090909091\n",
      "Epoch 3960, training loss: 2.4376513957977295, val loss: 2.527085065841675, val_acc: 0.27151515151515154\n",
      "Epoch 3970, training loss: 2.485452651977539, val loss: 2.5252020359039307, val_acc: 0.2775757575757576\n",
      "Epoch 3980, training loss: 2.4907138347625732, val loss: 2.5277254581451416, val_acc: 0.27151515151515154\n",
      "Epoch 3990, training loss: 2.4524810314178467, val loss: 2.521998405456543, val_acc: 0.25575757575757574\n",
      "Epoch 4000, training loss: 2.465021848678589, val loss: 2.518397331237793, val_acc: 0.2581818181818182\n",
      "Epoch 4010, training loss: 2.4586541652679443, val loss: 2.521496534347534, val_acc: 0.26666666666666666\n",
      "Epoch 4020, training loss: 2.4811370372772217, val loss: 2.53582763671875, val_acc: 0.263030303030303\n",
      "Epoch 4030, training loss: 2.463552474975586, val loss: 2.5327539443969727, val_acc: 0.2642424242424242\n",
      "Epoch 4040, training loss: 2.47059965133667, val loss: 2.519325017929077, val_acc: 0.2703030303030303\n",
      "Epoch 4050, training loss: 2.4801223278045654, val loss: 2.5224857330322266, val_acc: 0.2703030303030303\n",
      "Epoch 4060, training loss: 2.4429614543914795, val loss: 2.521324634552002, val_acc: 0.263030303030303\n",
      "Epoch 4070, training loss: 2.499549388885498, val loss: 2.528928518295288, val_acc: 0.26181818181818184\n",
      "Epoch 4080, training loss: 2.474501132965088, val loss: 2.534126043319702, val_acc: 0.263030303030303\n",
      "Epoch 4090, training loss: 2.4288523197174072, val loss: 2.5270578861236572, val_acc: 0.2606060606060606\n",
      "Epoch 4100, training loss: 2.5004971027374268, val loss: 2.533864736557007, val_acc: 0.25575757575757574\n",
      "Epoch 4110, training loss: 2.437579393386841, val loss: 2.5373969078063965, val_acc: 0.2606060606060606\n",
      "Epoch 4120, training loss: 2.4274210929870605, val loss: 2.541212320327759, val_acc: 0.25696969696969696\n",
      "Epoch 4130, training loss: 2.456122636795044, val loss: 2.528223752975464, val_acc: 0.25696969696969696\n",
      "Epoch 4140, training loss: 2.4726052284240723, val loss: 2.5462148189544678, val_acc: 0.2703030303030303\n",
      "Epoch 4150, training loss: 2.471468448638916, val loss: 2.5137405395507812, val_acc: 0.28363636363636363\n",
      "Epoch 4160, training loss: 2.4551637172698975, val loss: 2.523259401321411, val_acc: 0.2703030303030303\n",
      "Epoch 4170, training loss: 2.4601070880889893, val loss: 2.53587007522583, val_acc: 0.25696969696969696\n",
      "Epoch 4180, training loss: 2.480348825454712, val loss: 2.52130126953125, val_acc: 0.2739393939393939\n",
      "Epoch 4190, training loss: 2.4205360412597656, val loss: 2.528754234313965, val_acc: 0.26666666666666666\n",
      "Epoch 4200, training loss: 2.453868865966797, val loss: 2.531081438064575, val_acc: 0.2787878787878788\n",
      "Epoch 4210, training loss: 2.4834868907928467, val loss: 2.532405376434326, val_acc: 0.27151515151515154\n",
      "Epoch 4220, training loss: 2.418809652328491, val loss: 2.5248794555664062, val_acc: 0.2703030303030303\n",
      "Epoch 4230, training loss: 2.472365140914917, val loss: 2.5107226371765137, val_acc: 0.25696969696969696\n",
      "Epoch 4240, training loss: 2.467379331588745, val loss: 2.521807909011841, val_acc: 0.2606060606060606\n",
      "Epoch 4250, training loss: 2.482661247253418, val loss: 2.5279150009155273, val_acc: 0.2642424242424242\n",
      "Epoch 4260, training loss: 2.4770314693450928, val loss: 2.523846387863159, val_acc: 0.26545454545454544\n",
      "Epoch 4270, training loss: 2.4275283813476562, val loss: 2.5271286964416504, val_acc: 0.26545454545454544\n",
      "Epoch 4280, training loss: 2.5018813610076904, val loss: 2.524082899093628, val_acc: 0.26181818181818184\n",
      "Epoch 4290, training loss: 2.443836212158203, val loss: 2.531022787094116, val_acc: 0.26666666666666666\n",
      "Epoch 4300, training loss: 2.467747926712036, val loss: 2.5257415771484375, val_acc: 0.2642424242424242\n",
      "Epoch 4310, training loss: 2.473738193511963, val loss: 2.522404670715332, val_acc: 0.2678787878787879\n",
      "Epoch 4320, training loss: 2.42636775970459, val loss: 2.545522689819336, val_acc: 0.263030303030303\n",
      "Epoch 4330, training loss: 2.44578218460083, val loss: 2.5275118350982666, val_acc: 0.2739393939393939\n",
      "Epoch 4340, training loss: 2.4731321334838867, val loss: 2.5181806087493896, val_acc: 0.2703030303030303\n",
      "Epoch 4350, training loss: 2.4522366523742676, val loss: 2.5139033794403076, val_acc: 0.27515151515151515\n",
      "Epoch 4360, training loss: 2.458470106124878, val loss: 2.5304489135742188, val_acc: 0.25575757575757574\n",
      "Epoch 4370, training loss: 2.4549665451049805, val loss: 2.5279054641723633, val_acc: 0.28484848484848485\n",
      "Epoch 4380, training loss: 2.482245683670044, val loss: 2.5323476791381836, val_acc: 0.26666666666666666\n",
      "Epoch 4390, training loss: 2.454550266265869, val loss: 2.518315315246582, val_acc: 0.2678787878787879\n",
      "Epoch 4400, training loss: 2.485865592956543, val loss: 2.5232582092285156, val_acc: 0.2703030303030303\n",
      "Epoch 4410, training loss: 2.456085681915283, val loss: 2.5342776775360107, val_acc: 0.26181818181818184\n",
      "Epoch 4420, training loss: 2.4599554538726807, val loss: 2.519252300262451, val_acc: 0.2775757575757576\n",
      "Epoch 4430, training loss: 2.528325319290161, val loss: 2.5142853260040283, val_acc: 0.2787878787878788\n",
      "Epoch 4440, training loss: 2.4723119735717773, val loss: 2.5247035026550293, val_acc: 0.26181818181818184\n",
      "Epoch 4450, training loss: 2.462007761001587, val loss: 2.5237698554992676, val_acc: 0.26666666666666666\n",
      "Epoch 4460, training loss: 2.4428937435150146, val loss: 2.5226082801818848, val_acc: 0.263030303030303\n",
      "Epoch 4470, training loss: 2.426584005355835, val loss: 2.5306055545806885, val_acc: 0.2593939393939394\n",
      "Epoch 4480, training loss: 2.472083330154419, val loss: 2.51507306098938, val_acc: 0.263030303030303\n",
      "Epoch 4490, training loss: 2.4523260593414307, val loss: 2.5241568088531494, val_acc: 0.2727272727272727\n",
      "Epoch 4500, training loss: 2.4668290615081787, val loss: 2.5229737758636475, val_acc: 0.2739393939393939\n",
      "Epoch 4510, training loss: 2.432169198989868, val loss: 2.509781837463379, val_acc: 0.2775757575757576\n",
      "Epoch 4520, training loss: 2.456221342086792, val loss: 2.5161612033843994, val_acc: 0.27151515151515154\n",
      "Epoch 4530, training loss: 2.4629790782928467, val loss: 2.517138719558716, val_acc: 0.2775757575757576\n",
      "Epoch 4540, training loss: 2.4805920124053955, val loss: 2.5165913105010986, val_acc: 0.2739393939393939\n",
      "Epoch 4550, training loss: 2.4524953365325928, val loss: 2.5293445587158203, val_acc: 0.27515151515151515\n",
      "Epoch 4560, training loss: 2.4104318618774414, val loss: 2.5160768032073975, val_acc: 0.2787878787878788\n",
      "Epoch 4570, training loss: 2.418999433517456, val loss: 2.520641565322876, val_acc: 0.26545454545454544\n",
      "Epoch 4580, training loss: 2.4603142738342285, val loss: 2.5316526889801025, val_acc: 0.26666666666666666\n",
      "Epoch 4590, training loss: 2.4373841285705566, val loss: 2.532615900039673, val_acc: 0.26666666666666666\n",
      "Epoch 4600, training loss: 2.45097279548645, val loss: 2.5367684364318848, val_acc: 0.25333333333333335\n",
      "Epoch 4610, training loss: 2.4125819206237793, val loss: 2.5281965732574463, val_acc: 0.2642424242424242\n",
      "Epoch 4620, training loss: 2.428109884262085, val loss: 2.5173494815826416, val_acc: 0.2678787878787879\n",
      "Epoch 4630, training loss: 2.482084274291992, val loss: 2.5167365074157715, val_acc: 0.2690909090909091\n",
      "Epoch 4640, training loss: 2.4714479446411133, val loss: 2.524113416671753, val_acc: 0.2593939393939394\n",
      "Epoch 4650, training loss: 2.461566209793091, val loss: 2.5129380226135254, val_acc: 0.2593939393939394\n",
      "Epoch 4660, training loss: 2.4273781776428223, val loss: 2.509046792984009, val_acc: 0.2606060606060606\n",
      "Epoch 4670, training loss: 2.4500837326049805, val loss: 2.5263888835906982, val_acc: 0.27515151515151515\n",
      "Epoch 4680, training loss: 2.3835301399230957, val loss: 2.5112051963806152, val_acc: 0.26181818181818184\n",
      "Epoch 4690, training loss: 2.4319095611572266, val loss: 2.5069282054901123, val_acc: 0.26181818181818184\n",
      "Epoch 4700, training loss: 2.458291530609131, val loss: 2.501616954803467, val_acc: 0.2812121212121212\n",
      "Epoch 4710, training loss: 2.4655113220214844, val loss: 2.514320135116577, val_acc: 0.2727272727272727\n",
      "Epoch 4720, training loss: 2.400254726409912, val loss: 2.517935276031494, val_acc: 0.2727272727272727\n",
      "Epoch 4730, training loss: 2.497499465942383, val loss: 2.5164072513580322, val_acc: 0.2581818181818182\n",
      "Epoch 4740, training loss: 2.456745147705078, val loss: 2.5282273292541504, val_acc: 0.27636363636363637\n",
      "Epoch 4750, training loss: 2.4245071411132812, val loss: 2.512733221054077, val_acc: 0.27151515151515154\n",
      "Epoch 4760, training loss: 2.427349090576172, val loss: 2.527499198913574, val_acc: 0.2727272727272727\n",
      "Epoch 4770, training loss: 2.4218788146972656, val loss: 2.514737844467163, val_acc: 0.26666666666666666\n",
      "Epoch 4780, training loss: 2.4198408126831055, val loss: 2.5081968307495117, val_acc: 0.2727272727272727\n",
      "Epoch 4790, training loss: 2.424407958984375, val loss: 2.5156350135803223, val_acc: 0.26666666666666666\n",
      "Epoch 4800, training loss: 2.4269936084747314, val loss: 2.515395402908325, val_acc: 0.26545454545454544\n",
      "Epoch 4810, training loss: 2.4328908920288086, val loss: 2.5256433486938477, val_acc: 0.2642424242424242\n",
      "Epoch 4820, training loss: 2.450327157974243, val loss: 2.5062320232391357, val_acc: 0.27636363636363637\n",
      "Epoch 4830, training loss: 2.476154088973999, val loss: 2.5020785331726074, val_acc: 0.2775757575757576\n",
      "Epoch 4840, training loss: 2.4523868560791016, val loss: 2.511765241622925, val_acc: 0.2727272727272727\n",
      "Epoch 4850, training loss: 2.4232566356658936, val loss: 2.519160509109497, val_acc: 0.2703030303030303\n",
      "Epoch 4860, training loss: 2.4383456707000732, val loss: 2.506629467010498, val_acc: 0.2787878787878788\n",
      "Epoch 4870, training loss: 2.4385201930999756, val loss: 2.5059356689453125, val_acc: 0.2775757575757576\n",
      "Epoch 4880, training loss: 2.440965414047241, val loss: 2.4921982288360596, val_acc: 0.2872727272727273\n",
      "Epoch 4890, training loss: 2.4436705112457275, val loss: 2.5130434036254883, val_acc: 0.2703030303030303\n",
      "Epoch 4900, training loss: 2.420320510864258, val loss: 2.5165560245513916, val_acc: 0.27151515151515154\n",
      "Epoch 4910, training loss: 2.402780294418335, val loss: 2.520185947418213, val_acc: 0.26181818181818184\n",
      "Epoch 4920, training loss: 2.436565637588501, val loss: 2.5134801864624023, val_acc: 0.2787878787878788\n",
      "Epoch 4930, training loss: 2.4362454414367676, val loss: 2.509504556655884, val_acc: 0.2703030303030303\n",
      "Epoch 4940, training loss: 2.423219919204712, val loss: 2.495896339416504, val_acc: 0.2824242424242424\n",
      "Epoch 4950, training loss: 2.4231319427490234, val loss: 2.5166122913360596, val_acc: 0.2642424242424242\n",
      "Epoch 4960, training loss: 2.4328689575195312, val loss: 2.5139403343200684, val_acc: 0.2824242424242424\n",
      "Epoch 4970, training loss: 2.3897523880004883, val loss: 2.508918046951294, val_acc: 0.2727272727272727\n",
      "Epoch 4980, training loss: 2.465343475341797, val loss: 2.511949300765991, val_acc: 0.2581818181818182\n",
      "Epoch 4990, training loss: 2.4375672340393066, val loss: 2.5275230407714844, val_acc: 0.27151515151515154\n",
      "====================\n",
      "Training GAT with data_cnn\n",
      "Epoch 0, training loss: 36.01515579223633, val loss: 22.383129119873047, val_acc: 0.055757575757575756\n",
      "Epoch 10, training loss: 16.425050735473633, val loss: 9.778404235839844, val_acc: 0.020606060606060607\n",
      "Epoch 20, training loss: 5.33627462387085, val loss: 3.47007155418396, val_acc: 0.06909090909090909\n",
      "Epoch 30, training loss: 3.718737840652466, val loss: 3.1683037281036377, val_acc: 0.06545454545454546\n",
      "Epoch 40, training loss: 3.602536678314209, val loss: 3.114348888397217, val_acc: 0.03151515151515152\n",
      "Epoch 50, training loss: 3.4562113285064697, val loss: 3.053872585296631, val_acc: 0.04363636363636364\n",
      "Epoch 60, training loss: 3.4164323806762695, val loss: 3.0271551609039307, val_acc: 0.05696969696969697\n",
      "Epoch 70, training loss: 3.386305332183838, val loss: 3.02142333984375, val_acc: 0.06545454545454546\n",
      "Epoch 80, training loss: 3.3420233726501465, val loss: 3.004427433013916, val_acc: 0.06666666666666667\n",
      "Epoch 90, training loss: 3.331815719604492, val loss: 2.997541904449463, val_acc: 0.08606060606060606\n",
      "Epoch 100, training loss: 3.3070476055145264, val loss: 2.9861061573028564, val_acc: 0.08363636363636363\n",
      "Epoch 110, training loss: 3.197841167449951, val loss: 2.9665489196777344, val_acc: 0.0993939393939394\n",
      "Epoch 120, training loss: 3.313803195953369, val loss: 2.9526710510253906, val_acc: 0.11393939393939394\n",
      "Epoch 130, training loss: 3.206184148788452, val loss: 2.961212158203125, val_acc: 0.1103030303030303\n",
      "Epoch 140, training loss: 3.2386837005615234, val loss: 2.9250645637512207, val_acc: 0.12606060606060607\n",
      "Epoch 150, training loss: 3.235727071762085, val loss: 2.930208683013916, val_acc: 0.12606060606060607\n",
      "Epoch 160, training loss: 3.142242431640625, val loss: 2.895582914352417, val_acc: 0.14303030303030304\n",
      "Epoch 170, training loss: 3.0401389598846436, val loss: 2.882335662841797, val_acc: 0.16727272727272727\n",
      "Epoch 180, training loss: 3.0297789573669434, val loss: 2.8760392665863037, val_acc: 0.15151515151515152\n",
      "Epoch 190, training loss: 3.0190160274505615, val loss: 2.8759396076202393, val_acc: 0.15272727272727274\n",
      "Epoch 200, training loss: 2.9301249980926514, val loss: 2.8218672275543213, val_acc: 0.1890909090909091\n",
      "Epoch 210, training loss: 2.950953960418701, val loss: 2.8145618438720703, val_acc: 0.19393939393939394\n",
      "Epoch 220, training loss: 2.834299087524414, val loss: 2.807293653488159, val_acc: 0.18666666666666668\n",
      "Epoch 230, training loss: 2.8689277172088623, val loss: 2.7663416862487793, val_acc: 0.2084848484848485\n",
      "Epoch 240, training loss: 2.858936071395874, val loss: 2.7449443340301514, val_acc: 0.22666666666666666\n",
      "Epoch 250, training loss: 2.7802112102508545, val loss: 2.7199511528015137, val_acc: 0.2412121212121212\n",
      "Epoch 260, training loss: 2.774397373199463, val loss: 2.7345123291015625, val_acc: 0.22303030303030302\n",
      "Epoch 270, training loss: 2.764206647872925, val loss: 2.700502395629883, val_acc: 0.22787878787878788\n",
      "Epoch 280, training loss: 2.7561869621276855, val loss: 2.6958255767822266, val_acc: 0.24484848484848484\n",
      "Epoch 290, training loss: 2.606743574142456, val loss: 2.6682300567626953, val_acc: 0.26181818181818184\n",
      "Epoch 300, training loss: 2.6783032417297363, val loss: 2.637557029724121, val_acc: 0.28484848484848485\n",
      "Epoch 310, training loss: 2.6437413692474365, val loss: 2.633120536804199, val_acc: 0.2775757575757576\n",
      "Epoch 320, training loss: 2.6326828002929688, val loss: 2.6107099056243896, val_acc: 0.2775757575757576\n",
      "Epoch 330, training loss: 2.544121026992798, val loss: 2.6226625442504883, val_acc: 0.2593939393939394\n",
      "Epoch 340, training loss: 2.481558084487915, val loss: 2.599266290664673, val_acc: 0.2678787878787879\n",
      "Epoch 350, training loss: 2.5202980041503906, val loss: 2.568593978881836, val_acc: 0.2872727272727273\n",
      "Epoch 360, training loss: 2.4583466053009033, val loss: 2.570904016494751, val_acc: 0.27636363636363637\n",
      "Epoch 370, training loss: 2.4137401580810547, val loss: 2.5561447143554688, val_acc: 0.2921212121212121\n",
      "Epoch 380, training loss: 2.4486782550811768, val loss: 2.5523171424865723, val_acc: 0.28606060606060607\n",
      "Epoch 390, training loss: 2.4179790019989014, val loss: 2.554725170135498, val_acc: 0.27636363636363637\n",
      "Epoch 400, training loss: 2.3476874828338623, val loss: 2.533621311187744, val_acc: 0.28363636363636363\n",
      "Epoch 410, training loss: 2.433595895767212, val loss: 2.540304183959961, val_acc: 0.2824242424242424\n",
      "Epoch 420, training loss: 2.2973642349243164, val loss: 2.522150993347168, val_acc: 0.2824242424242424\n",
      "Epoch 430, training loss: 2.325486660003662, val loss: 2.5157928466796875, val_acc: 0.28606060606060607\n",
      "Epoch 440, training loss: 2.285963773727417, val loss: 2.5151615142822266, val_acc: 0.29454545454545455\n",
      "Epoch 450, training loss: 2.3193249702453613, val loss: 2.5093889236450195, val_acc: 0.2896969696969697\n",
      "Epoch 460, training loss: 2.279496431350708, val loss: 2.5157902240753174, val_acc: 0.2787878787878788\n",
      "Epoch 470, training loss: 2.2136385440826416, val loss: 2.5044093132019043, val_acc: 0.28606060606060607\n",
      "Epoch 480, training loss: 2.192800998687744, val loss: 2.481557607650757, val_acc: 0.296969696969697\n",
      "Epoch 490, training loss: 2.232438564300537, val loss: 2.4715189933776855, val_acc: 0.2921212121212121\n",
      "Epoch 500, training loss: 2.263070583343506, val loss: 2.5038347244262695, val_acc: 0.2824242424242424\n",
      "Epoch 510, training loss: 2.1732969284057617, val loss: 2.4947047233581543, val_acc: 0.2909090909090909\n",
      "Epoch 520, training loss: 2.16702938079834, val loss: 2.504486083984375, val_acc: 0.27636363636363637\n",
      "Epoch 530, training loss: 2.174558162689209, val loss: 2.4898059368133545, val_acc: 0.3006060606060606\n",
      "Epoch 540, training loss: 2.197741985321045, val loss: 2.464834213256836, val_acc: 0.3006060606060606\n",
      "Epoch 550, training loss: 2.175978660583496, val loss: 2.4765162467956543, val_acc: 0.2884848484848485\n",
      "Epoch 560, training loss: 2.089698076248169, val loss: 2.4708051681518555, val_acc: 0.29333333333333333\n",
      "Epoch 570, training loss: 2.1500868797302246, val loss: 2.4538590908050537, val_acc: 0.29818181818181816\n",
      "Epoch 580, training loss: 2.15628719329834, val loss: 2.4656708240509033, val_acc: 0.29818181818181816\n",
      "Epoch 590, training loss: 2.062422275543213, val loss: 2.466585397720337, val_acc: 0.30666666666666664\n",
      "Epoch 600, training loss: 2.1743874549865723, val loss: 2.4542300701141357, val_acc: 0.3090909090909091\n",
      "Epoch 610, training loss: 2.04604434967041, val loss: 2.446394205093384, val_acc: 0.3090909090909091\n",
      "Epoch 620, training loss: 2.0230512619018555, val loss: 2.45123028755188, val_acc: 0.3006060606060606\n",
      "Epoch 630, training loss: 2.0775041580200195, val loss: 2.4607808589935303, val_acc: 0.296969696969697\n",
      "Epoch 640, training loss: 2.085313558578491, val loss: 2.454777956008911, val_acc: 0.29818181818181816\n",
      "Epoch 650, training loss: 2.0574452877044678, val loss: 2.4557902812957764, val_acc: 0.2957575757575758\n",
      "Epoch 660, training loss: 2.1163461208343506, val loss: 2.4821462631225586, val_acc: 0.29454545454545455\n",
      "Epoch 670, training loss: 1.9867911338806152, val loss: 2.4623429775238037, val_acc: 0.3006060606060606\n",
      "Epoch 680, training loss: 2.083364725112915, val loss: 2.4673800468444824, val_acc: 0.3054545454545455\n",
      "Epoch 690, training loss: 2.0415475368499756, val loss: 2.463505744934082, val_acc: 0.30303030303030304\n",
      "Epoch 700, training loss: 2.0066487789154053, val loss: 2.4766833782196045, val_acc: 0.3090909090909091\n",
      "Epoch 710, training loss: 2.0211024284362793, val loss: 2.45302152633667, val_acc: 0.3018181818181818\n",
      "Epoch 720, training loss: 1.995998740196228, val loss: 2.4616661071777344, val_acc: 0.30303030303030304\n",
      "Epoch 730, training loss: 2.0980019569396973, val loss: 2.458470106124878, val_acc: 0.30303030303030304\n",
      "Epoch 740, training loss: 1.9828341007232666, val loss: 2.4682376384735107, val_acc: 0.30424242424242426\n",
      "Epoch 750, training loss: 1.944790244102478, val loss: 2.482001304626465, val_acc: 0.29454545454545455\n",
      "Epoch 760, training loss: 2.024505376815796, val loss: 2.4669907093048096, val_acc: 0.3006060606060606\n",
      "Epoch 770, training loss: 1.9224878549575806, val loss: 2.467716693878174, val_acc: 0.30303030303030304\n",
      "Epoch 780, training loss: 2.015367269515991, val loss: 2.4562346935272217, val_acc: 0.3006060606060606\n",
      "Epoch 790, training loss: 1.9860103130340576, val loss: 2.461862802505493, val_acc: 0.29818181818181816\n",
      "Epoch 800, training loss: 2.0040080547332764, val loss: 2.462017774581909, val_acc: 0.29818181818181816\n",
      "Epoch 810, training loss: 1.9602633714675903, val loss: 2.45322585105896, val_acc: 0.30424242424242426\n",
      "Epoch 820, training loss: 1.9542632102966309, val loss: 2.4746594429016113, val_acc: 0.3115151515151515\n",
      "Epoch 830, training loss: 1.9754364490509033, val loss: 2.4663729667663574, val_acc: 0.3103030303030303\n",
      "Epoch 840, training loss: 1.9622976779937744, val loss: 2.491124153137207, val_acc: 0.29333333333333333\n",
      "Epoch 850, training loss: 2.0213897228240967, val loss: 2.4816694259643555, val_acc: 0.2909090909090909\n",
      "Epoch 860, training loss: 1.9629758596420288, val loss: 2.4949588775634766, val_acc: 0.30666666666666664\n",
      "Epoch 870, training loss: 1.8990846872329712, val loss: 2.49548077583313, val_acc: 0.3006060606060606\n",
      "Epoch 880, training loss: 1.891804575920105, val loss: 2.4690804481506348, val_acc: 0.29333333333333333\n",
      "Epoch 890, training loss: 1.8947283029556274, val loss: 2.480201482772827, val_acc: 0.30303030303030304\n",
      "Epoch 900, training loss: 1.9668079614639282, val loss: 2.4814701080322266, val_acc: 0.31272727272727274\n",
      "Epoch 910, training loss: 1.925073266029358, val loss: 2.462143898010254, val_acc: 0.3103030303030303\n",
      "Epoch 920, training loss: 1.924397349357605, val loss: 2.4774069786071777, val_acc: 0.30424242424242426\n",
      "Epoch 930, training loss: 1.8981573581695557, val loss: 2.48260235786438, val_acc: 0.30303030303030304\n",
      "Epoch 940, training loss: 1.9009840488433838, val loss: 2.4834225177764893, val_acc: 0.30424242424242426\n",
      "Epoch 950, training loss: 1.8604791164398193, val loss: 2.4823646545410156, val_acc: 0.3103030303030303\n",
      "Epoch 960, training loss: 1.8059552907943726, val loss: 2.4908463954925537, val_acc: 0.3103030303030303\n",
      "Epoch 970, training loss: 1.8329920768737793, val loss: 2.4777848720550537, val_acc: 0.30787878787878786\n",
      "Epoch 980, training loss: 1.8925414085388184, val loss: 2.4851126670837402, val_acc: 0.32242424242424245\n",
      "Epoch 990, training loss: 1.8460693359375, val loss: 2.5100979804992676, val_acc: 0.3090909090909091\n",
      "Epoch 1000, training loss: 1.856406807899475, val loss: 2.5006206035614014, val_acc: 0.3018181818181818\n",
      "Epoch 1010, training loss: 1.843972086906433, val loss: 2.509277105331421, val_acc: 0.30303030303030304\n",
      "Epoch 1020, training loss: 1.8296555280685425, val loss: 2.508002519607544, val_acc: 0.3090909090909091\n",
      "Epoch 1030, training loss: 1.8443372249603271, val loss: 2.4997949600219727, val_acc: 0.2993939393939394\n",
      "Epoch 1040, training loss: 1.790941596031189, val loss: 2.49495792388916, val_acc: 0.30666666666666664\n",
      "Epoch 1050, training loss: 1.7345253229141235, val loss: 2.504978656768799, val_acc: 0.3090909090909091\n",
      "Epoch 1060, training loss: 1.753896951675415, val loss: 2.5126636028289795, val_acc: 0.3090909090909091\n",
      "Epoch 1070, training loss: 1.813557744026184, val loss: 2.4960787296295166, val_acc: 0.30424242424242426\n",
      "Epoch 1080, training loss: 1.8273098468780518, val loss: 2.5030195713043213, val_acc: 0.31272727272727274\n",
      "Epoch 1090, training loss: 1.7058050632476807, val loss: 2.501295328140259, val_acc: 0.3103030303030303\n",
      "Epoch 1100, training loss: 1.8327972888946533, val loss: 2.496371030807495, val_acc: 0.3006060606060606\n",
      "Epoch 1110, training loss: 1.8170669078826904, val loss: 2.4953551292419434, val_acc: 0.3115151515151515\n",
      "Epoch 1120, training loss: 1.72355318069458, val loss: 2.480531692504883, val_acc: 0.3115151515151515\n",
      "Epoch 1130, training loss: 1.8427027463912964, val loss: 2.509040117263794, val_acc: 0.3115151515151515\n",
      "Epoch 1140, training loss: 1.8389381170272827, val loss: 2.5151069164276123, val_acc: 0.31636363636363635\n",
      "Epoch 1150, training loss: 1.7826615571975708, val loss: 2.5172042846679688, val_acc: 0.3090909090909091\n",
      "Epoch 1160, training loss: 1.8122787475585938, val loss: 2.517660617828369, val_acc: 0.31393939393939396\n",
      "Epoch 1170, training loss: 1.718196153640747, val loss: 2.514688730239868, val_acc: 0.31393939393939396\n",
      "Epoch 1180, training loss: 1.7645763158798218, val loss: 2.5298209190368652, val_acc: 0.30303030303030304\n",
      "Epoch 1190, training loss: 1.829663872718811, val loss: 2.5305356979370117, val_acc: 0.31393939393939396\n",
      "Epoch 1200, training loss: 1.7218338251113892, val loss: 2.5277597904205322, val_acc: 0.32484848484848483\n",
      "Epoch 1210, training loss: 1.7846033573150635, val loss: 2.5038094520568848, val_acc: 0.32484848484848483\n",
      "Epoch 1220, training loss: 1.7674471139907837, val loss: 2.5377156734466553, val_acc: 0.3151515151515151\n",
      "Epoch 1230, training loss: 1.7651435136795044, val loss: 2.5259904861450195, val_acc: 0.3115151515151515\n",
      "Epoch 1240, training loss: 1.6591757535934448, val loss: 2.522305488586426, val_acc: 0.32\n",
      "Epoch 1250, training loss: 1.7183070182800293, val loss: 2.5233371257781982, val_acc: 0.31272727272727274\n",
      "Epoch 1260, training loss: 1.7604345083236694, val loss: 2.5483689308166504, val_acc: 0.32242424242424245\n",
      "Epoch 1270, training loss: 1.7465983629226685, val loss: 2.536058187484741, val_acc: 0.31393939393939396\n",
      "Epoch 1280, training loss: 1.7288469076156616, val loss: 2.5281033515930176, val_acc: 0.31272727272727274\n",
      "Epoch 1290, training loss: 1.6639906167984009, val loss: 2.5359597206115723, val_acc: 0.31757575757575757\n",
      "Epoch 1300, training loss: 1.6963177919387817, val loss: 2.557600498199463, val_acc: 0.32606060606060605\n",
      "Epoch 1310, training loss: 1.655564308166504, val loss: 2.5703186988830566, val_acc: 0.32242424242424245\n",
      "Epoch 1320, training loss: 1.6788461208343506, val loss: 2.547079563140869, val_acc: 0.31757575757575757\n",
      "Epoch 1330, training loss: 1.6407599449157715, val loss: 2.5309817790985107, val_acc: 0.32\n",
      "Epoch 1340, training loss: 1.7260711193084717, val loss: 2.5474135875701904, val_acc: 0.3212121212121212\n",
      "Epoch 1350, training loss: 1.7766051292419434, val loss: 2.541686773300171, val_acc: 0.3212121212121212\n",
      "Epoch 1360, training loss: 1.7863765954971313, val loss: 2.5466036796569824, val_acc: 0.3151515151515151\n",
      "Epoch 1370, training loss: 1.6787326335906982, val loss: 2.5755057334899902, val_acc: 0.3212121212121212\n",
      "Epoch 1380, training loss: 1.6091970205307007, val loss: 2.5437538623809814, val_acc: 0.32606060606060605\n",
      "Epoch 1390, training loss: 1.6983963251113892, val loss: 2.552842617034912, val_acc: 0.3236363636363636\n",
      "Epoch 1400, training loss: 1.6719993352890015, val loss: 2.553027868270874, val_acc: 0.3284848484848485\n",
      "Epoch 1410, training loss: 1.7396714687347412, val loss: 2.543606996536255, val_acc: 0.3284848484848485\n",
      "Epoch 1420, training loss: 1.6305631399154663, val loss: 2.5775556564331055, val_acc: 0.32484848484848483\n",
      "Epoch 1430, training loss: 1.7168303728103638, val loss: 2.547319173812866, val_acc: 0.31757575757575757\n",
      "Epoch 1440, training loss: 1.5828266143798828, val loss: 2.551011562347412, val_acc: 0.33090909090909093\n",
      "Epoch 1450, training loss: 1.6736698150634766, val loss: 2.568222999572754, val_acc: 0.32242424242424245\n",
      "Epoch 1460, training loss: 1.6966979503631592, val loss: 2.5712411403656006, val_acc: 0.32\n",
      "Epoch 1470, training loss: 1.7477147579193115, val loss: 2.583441972732544, val_acc: 0.32\n",
      "Epoch 1480, training loss: 1.645910382270813, val loss: 2.5595152378082275, val_acc: 0.3284848484848485\n",
      "Epoch 1490, training loss: 1.6481329202651978, val loss: 2.605620861053467, val_acc: 0.31636363636363635\n",
      "Epoch 1500, training loss: 1.6416622400283813, val loss: 2.5556447505950928, val_acc: 0.32606060606060605\n",
      "Epoch 1510, training loss: 1.6454185247421265, val loss: 2.571533679962158, val_acc: 0.3284848484848485\n",
      "Epoch 1520, training loss: 1.610425591468811, val loss: 2.5853469371795654, val_acc: 0.3236363636363636\n",
      "Epoch 1530, training loss: 1.6520469188690186, val loss: 2.581597328186035, val_acc: 0.3321212121212121\n",
      "Epoch 1540, training loss: 1.6936986446380615, val loss: 2.567410707473755, val_acc: 0.336969696969697\n",
      "Epoch 1550, training loss: 1.5944429636001587, val loss: 2.5852839946746826, val_acc: 0.32727272727272727\n",
      "Epoch 1560, training loss: 1.7038758993148804, val loss: 2.6228790283203125, val_acc: 0.33090909090909093\n",
      "Epoch 1570, training loss: 1.6048020124435425, val loss: 2.6180624961853027, val_acc: 0.32606060606060605\n",
      "Epoch 1580, training loss: 1.59267258644104, val loss: 2.622586488723755, val_acc: 0.32606060606060605\n",
      "Epoch 1590, training loss: 1.5595802068710327, val loss: 2.626894235610962, val_acc: 0.3296969696969697\n",
      "Epoch 1600, training loss: 1.6440180540084839, val loss: 2.612597703933716, val_acc: 0.33454545454545453\n",
      "Epoch 1610, training loss: 1.5775625705718994, val loss: 2.6187210083007812, val_acc: 0.33575757575757575\n",
      "Epoch 1620, training loss: 1.5544774532318115, val loss: 2.6217401027679443, val_acc: 0.3333333333333333\n",
      "Epoch 1630, training loss: 1.6894441843032837, val loss: 2.6091790199279785, val_acc: 0.3296969696969697\n",
      "Epoch 1640, training loss: 1.6662063598632812, val loss: 2.6255249977111816, val_acc: 0.32727272727272727\n",
      "Epoch 1650, training loss: 1.5830607414245605, val loss: 2.635516405105591, val_acc: 0.3406060606060606\n",
      "Epoch 1660, training loss: 1.6045500040054321, val loss: 2.6174681186676025, val_acc: 0.32242424242424245\n",
      "Epoch 1670, training loss: 1.6157174110412598, val loss: 2.6071908473968506, val_acc: 0.3333333333333333\n",
      "Epoch 1680, training loss: 1.581315040588379, val loss: 2.6253573894500732, val_acc: 0.336969696969697\n",
      "Epoch 1690, training loss: 1.5616337060928345, val loss: 2.619171619415283, val_acc: 0.3418181818181818\n",
      "Epoch 1700, training loss: 1.5568925142288208, val loss: 2.6423733234405518, val_acc: 0.33090909090909093\n",
      "Epoch 1710, training loss: 1.561777949333191, val loss: 2.6061654090881348, val_acc: 0.33090909090909093\n",
      "Epoch 1720, training loss: 1.4822735786437988, val loss: 2.6361029148101807, val_acc: 0.3393939393939394\n",
      "Epoch 1730, training loss: 1.568346619606018, val loss: 2.6201419830322266, val_acc: 0.3333333333333333\n",
      "Epoch 1740, training loss: 1.616984248161316, val loss: 2.6220247745513916, val_acc: 0.3381818181818182\n",
      "Epoch 1750, training loss: 1.5587764978408813, val loss: 2.6322262287139893, val_acc: 0.336969696969697\n",
      "Epoch 1760, training loss: 1.6182419061660767, val loss: 2.650527238845825, val_acc: 0.343030303030303\n",
      "Epoch 1770, training loss: 1.601983666419983, val loss: 2.6590516567230225, val_acc: 0.33454545454545453\n",
      "Epoch 1780, training loss: 1.5070070028305054, val loss: 2.663358449935913, val_acc: 0.3381818181818182\n",
      "Epoch 1790, training loss: 1.6498948335647583, val loss: 2.643057346343994, val_acc: 0.3418181818181818\n",
      "Epoch 1800, training loss: 1.5972263813018799, val loss: 2.6784768104553223, val_acc: 0.3284848484848485\n",
      "Epoch 1810, training loss: 1.6353650093078613, val loss: 2.6396191120147705, val_acc: 0.33090909090909093\n",
      "Epoch 1820, training loss: 1.5002760887145996, val loss: 2.677199602127075, val_acc: 0.32727272727272727\n",
      "Epoch 1830, training loss: 1.5614293813705444, val loss: 2.6360318660736084, val_acc: 0.343030303030303\n",
      "Epoch 1840, training loss: 1.5588352680206299, val loss: 2.6702282428741455, val_acc: 0.3296969696969697\n",
      "Epoch 1850, training loss: 1.5722991228103638, val loss: 2.647010564804077, val_acc: 0.33454545454545453\n",
      "Epoch 1860, training loss: 1.579893946647644, val loss: 2.6731131076812744, val_acc: 0.3296969696969697\n",
      "Epoch 1870, training loss: 1.5700372457504272, val loss: 2.6660499572753906, val_acc: 0.3296969696969697\n",
      "Epoch 1880, training loss: 1.5793486833572388, val loss: 2.655322790145874, val_acc: 0.33575757575757575\n",
      "Epoch 1890, training loss: 1.4707834720611572, val loss: 2.681499719619751, val_acc: 0.3333333333333333\n",
      "Epoch 1900, training loss: 1.4816877841949463, val loss: 2.6789588928222656, val_acc: 0.32606060606060605\n",
      "Epoch 1910, training loss: 1.6048173904418945, val loss: 2.697535753250122, val_acc: 0.3381818181818182\n",
      "Epoch 1920, training loss: 1.5459157228469849, val loss: 2.657975196838379, val_acc: 0.33090909090909093\n",
      "Epoch 1930, training loss: 1.5950385332107544, val loss: 2.7168641090393066, val_acc: 0.33454545454545453\n",
      "Epoch 1940, training loss: 1.5212565660476685, val loss: 2.6782655715942383, val_acc: 0.3393939393939394\n",
      "Epoch 1950, training loss: 1.480943202972412, val loss: 2.7128560543060303, val_acc: 0.3381818181818182\n",
      "Epoch 1960, training loss: 1.5542477369308472, val loss: 2.7271854877471924, val_acc: 0.3321212121212121\n",
      "Epoch 1970, training loss: 1.654395341873169, val loss: 2.690159797668457, val_acc: 0.3393939393939394\n",
      "Epoch 1980, training loss: 1.5415394306182861, val loss: 2.7059478759765625, val_acc: 0.3321212121212121\n",
      "Epoch 1990, training loss: 1.5513709783554077, val loss: 2.7039670944213867, val_acc: 0.3381818181818182\n",
      "Epoch 2000, training loss: 1.4796817302703857, val loss: 2.7081363201141357, val_acc: 0.33090909090909093\n",
      "Epoch 2010, training loss: 1.526052474975586, val loss: 2.7056527137756348, val_acc: 0.33575757575757575\n",
      "Epoch 2020, training loss: 1.4658492803573608, val loss: 2.66753888130188, val_acc: 0.3527272727272727\n",
      "Epoch 2030, training loss: 1.5539029836654663, val loss: 2.6893091201782227, val_acc: 0.33454545454545453\n",
      "Epoch 2040, training loss: 1.5434083938598633, val loss: 2.694507360458374, val_acc: 0.33454545454545453\n",
      "Epoch 2050, training loss: 1.5276035070419312, val loss: 2.7126290798187256, val_acc: 0.33454545454545453\n",
      "Epoch 2060, training loss: 1.450864553451538, val loss: 2.694694995880127, val_acc: 0.3321212121212121\n",
      "Epoch 2070, training loss: 1.524568796157837, val loss: 2.6843087673187256, val_acc: 0.33575757575757575\n",
      "Epoch 2080, training loss: 1.4260846376419067, val loss: 2.7110393047332764, val_acc: 0.343030303030303\n",
      "Epoch 2090, training loss: 1.5773051977157593, val loss: 2.685572385787964, val_acc: 0.33575757575757575\n",
      "Epoch 2100, training loss: 1.587195634841919, val loss: 2.7154598236083984, val_acc: 0.3381818181818182\n",
      "Epoch 2110, training loss: 1.5731899738311768, val loss: 2.6772348880767822, val_acc: 0.343030303030303\n",
      "Epoch 2120, training loss: 1.4967200756072998, val loss: 2.6682395935058594, val_acc: 0.3466666666666667\n",
      "Epoch 2130, training loss: 1.5208317041397095, val loss: 2.7072603702545166, val_acc: 0.336969696969697\n",
      "Epoch 2140, training loss: 1.4907833337783813, val loss: 2.743269443511963, val_acc: 0.3284848484848485\n",
      "Epoch 2150, training loss: 1.5120596885681152, val loss: 2.739530086517334, val_acc: 0.336969696969697\n",
      "Epoch 2160, training loss: 1.5799658298492432, val loss: 2.6985528469085693, val_acc: 0.3321212121212121\n",
      "Epoch 2170, training loss: 1.4861589670181274, val loss: 2.697499990463257, val_acc: 0.33454545454545453\n",
      "Epoch 2180, training loss: 1.456045389175415, val loss: 2.7560713291168213, val_acc: 0.33454545454545453\n",
      "Epoch 2190, training loss: 1.4945027828216553, val loss: 2.6919808387756348, val_acc: 0.343030303030303\n",
      "Epoch 2200, training loss: 1.4444162845611572, val loss: 2.7591278553009033, val_acc: 0.33454545454545453\n",
      "Epoch 2210, training loss: 1.5082200765609741, val loss: 2.69675612449646, val_acc: 0.343030303030303\n",
      "Epoch 2220, training loss: 1.6008282899856567, val loss: 2.754331111907959, val_acc: 0.3393939393939394\n",
      "Epoch 2230, training loss: 1.4955132007598877, val loss: 2.6935739517211914, val_acc: 0.34545454545454546\n",
      "Epoch 2240, training loss: 1.4558295011520386, val loss: 2.73079776763916, val_acc: 0.3333333333333333\n",
      "Epoch 2250, training loss: 1.5315368175506592, val loss: 2.6897013187408447, val_acc: 0.3381818181818182\n",
      "Epoch 2260, training loss: 1.4316998720169067, val loss: 2.7454140186309814, val_acc: 0.33454545454545453\n",
      "Epoch 2270, training loss: 1.4530210494995117, val loss: 2.7472262382507324, val_acc: 0.3321212121212121\n",
      "Epoch 2280, training loss: 1.4991209506988525, val loss: 2.744941473007202, val_acc: 0.343030303030303\n",
      "Epoch 2290, training loss: 1.5012381076812744, val loss: 2.7371366024017334, val_acc: 0.3418181818181818\n",
      "Epoch 2300, training loss: 1.487509846687317, val loss: 2.745863437652588, val_acc: 0.3418181818181818\n",
      "Epoch 2310, training loss: 1.487025260925293, val loss: 2.7546072006225586, val_acc: 0.34424242424242424\n",
      "Epoch 2320, training loss: 1.4990036487579346, val loss: 2.7308831214904785, val_acc: 0.3333333333333333\n",
      "Epoch 2330, training loss: 1.464573621749878, val loss: 2.7577672004699707, val_acc: 0.3490909090909091\n",
      "Epoch 2340, training loss: 1.516352891921997, val loss: 2.734501600265503, val_acc: 0.3393939393939394\n",
      "Epoch 2350, training loss: 1.451562523841858, val loss: 2.75707745552063, val_acc: 0.3418181818181818\n",
      "Epoch 2360, training loss: 1.4308116436004639, val loss: 2.7439815998077393, val_acc: 0.343030303030303\n",
      "Epoch 2370, training loss: 1.421282172203064, val loss: 2.756610155105591, val_acc: 0.3406060606060606\n",
      "Epoch 2380, training loss: 1.465158462524414, val loss: 2.739582061767578, val_acc: 0.3321212121212121\n",
      "Epoch 2390, training loss: 1.5167721509933472, val loss: 2.7446560859680176, val_acc: 0.33575757575757575\n",
      "Epoch 2400, training loss: 1.4593651294708252, val loss: 2.727713108062744, val_acc: 0.3406060606060606\n",
      "Epoch 2410, training loss: 1.5244451761245728, val loss: 2.7913379669189453, val_acc: 0.33090909090909093\n",
      "Epoch 2420, training loss: 1.5493292808532715, val loss: 2.7556209564208984, val_acc: 0.3333333333333333\n",
      "Epoch 2430, training loss: 1.4881354570388794, val loss: 2.750696897506714, val_acc: 0.35393939393939394\n",
      "Epoch 2440, training loss: 1.4458016157150269, val loss: 2.73791241645813, val_acc: 0.34545454545454546\n",
      "Epoch 2450, training loss: 1.5007535219192505, val loss: 2.722228527069092, val_acc: 0.3381818181818182\n",
      "Epoch 2460, training loss: 1.4609242677688599, val loss: 2.739732265472412, val_acc: 0.3490909090909091\n",
      "Epoch 2470, training loss: 1.5208652019500732, val loss: 2.753643274307251, val_acc: 0.3381818181818182\n",
      "Epoch 2480, training loss: 1.4614142179489136, val loss: 2.7665207386016846, val_acc: 0.3418181818181818\n",
      "Epoch 2490, training loss: 1.4608696699142456, val loss: 2.7763025760650635, val_acc: 0.3466666666666667\n",
      "Epoch 2500, training loss: 1.4379520416259766, val loss: 2.755905866622925, val_acc: 0.34424242424242424\n",
      "Epoch 2510, training loss: 1.4714562892913818, val loss: 2.7621803283691406, val_acc: 0.33575757575757575\n",
      "Epoch 2520, training loss: 1.4455418586730957, val loss: 2.7581920623779297, val_acc: 0.3418181818181818\n",
      "Epoch 2530, training loss: 1.4679203033447266, val loss: 2.761173963546753, val_acc: 0.34424242424242424\n",
      "Epoch 2540, training loss: 1.496964693069458, val loss: 2.741441249847412, val_acc: 0.336969696969697\n",
      "Epoch 2550, training loss: 1.4790297746658325, val loss: 2.7376534938812256, val_acc: 0.3381818181818182\n",
      "Epoch 2560, training loss: 1.5194154977798462, val loss: 2.854518413543701, val_acc: 0.336969696969697\n",
      "Epoch 2570, training loss: 1.4510743618011475, val loss: 2.764106035232544, val_acc: 0.3381818181818182\n",
      "Epoch 2580, training loss: 1.4530469179153442, val loss: 2.782789468765259, val_acc: 0.3296969696969697\n",
      "Epoch 2590, training loss: 1.495599627494812, val loss: 2.752136707305908, val_acc: 0.343030303030303\n",
      "Epoch 2600, training loss: 1.4524506330490112, val loss: 2.8048551082611084, val_acc: 0.32606060606060605\n",
      "Epoch 2610, training loss: 1.4543510675430298, val loss: 2.778242826461792, val_acc: 0.34545454545454546\n",
      "Epoch 2620, training loss: 1.4786144495010376, val loss: 2.797236680984497, val_acc: 0.3503030303030303\n",
      "Epoch 2630, training loss: 1.4938794374465942, val loss: 2.7308542728424072, val_acc: 0.3478787878787879\n",
      "Epoch 2640, training loss: 1.5393710136413574, val loss: 2.826788902282715, val_acc: 0.336969696969697\n",
      "Epoch 2650, training loss: 1.4821029901504517, val loss: 2.8661253452301025, val_acc: 0.34424242424242424\n",
      "Epoch 2660, training loss: 1.4193044900894165, val loss: 2.8029212951660156, val_acc: 0.34545454545454546\n",
      "Epoch 2670, training loss: 1.4669188261032104, val loss: 2.7856056690216064, val_acc: 0.336969696969697\n",
      "Epoch 2680, training loss: 1.5111531019210815, val loss: 2.7660956382751465, val_acc: 0.3490909090909091\n",
      "Epoch 2690, training loss: 1.487654209136963, val loss: 2.7536778450012207, val_acc: 0.336969696969697\n",
      "Epoch 2700, training loss: 1.4445534944534302, val loss: 2.791705846786499, val_acc: 0.3381818181818182\n",
      "Epoch 2710, training loss: 1.4919406175613403, val loss: 2.7709848880767822, val_acc: 0.3393939393939394\n",
      "Epoch 2720, training loss: 1.4791548252105713, val loss: 2.7943403720855713, val_acc: 0.34424242424242424\n",
      "Epoch 2730, training loss: 1.3557651042938232, val loss: 2.7991044521331787, val_acc: 0.34545454545454546\n",
      "Epoch 2740, training loss: 1.447839617729187, val loss: 2.7758708000183105, val_acc: 0.3381818181818182\n",
      "Epoch 2750, training loss: 1.379971981048584, val loss: 2.774873971939087, val_acc: 0.34424242424242424\n",
      "Epoch 2760, training loss: 1.4292597770690918, val loss: 2.7905170917510986, val_acc: 0.3406060606060606\n",
      "Epoch 2770, training loss: 1.4149365425109863, val loss: 2.815307855606079, val_acc: 0.3406060606060606\n",
      "Epoch 2780, training loss: 1.4576823711395264, val loss: 2.7827751636505127, val_acc: 0.34424242424242424\n",
      "Epoch 2790, training loss: 1.4623197317123413, val loss: 2.8092124462127686, val_acc: 0.34545454545454546\n",
      "Epoch 2800, training loss: 1.4857457876205444, val loss: 2.7628326416015625, val_acc: 0.3466666666666667\n",
      "Epoch 2810, training loss: 1.4116078615188599, val loss: 2.761306047439575, val_acc: 0.3478787878787879\n",
      "Epoch 2820, training loss: 1.4088401794433594, val loss: 2.7821407318115234, val_acc: 0.336969696969697\n",
      "Epoch 2830, training loss: 1.4448659420013428, val loss: 2.7854394912719727, val_acc: 0.35515151515151516\n",
      "Epoch 2840, training loss: 1.4624474048614502, val loss: 2.8030619621276855, val_acc: 0.3321212121212121\n",
      "Epoch 2850, training loss: 1.4625805616378784, val loss: 2.836515188217163, val_acc: 0.3393939393939394\n",
      "Epoch 2860, training loss: 1.4249796867370605, val loss: 2.8038554191589355, val_acc: 0.3418181818181818\n",
      "Epoch 2870, training loss: 1.451546311378479, val loss: 2.8071484565734863, val_acc: 0.33575757575757575\n",
      "Epoch 2880, training loss: 1.4288159608840942, val loss: 2.8086960315704346, val_acc: 0.3478787878787879\n",
      "Epoch 2890, training loss: 1.4597116708755493, val loss: 2.8177664279937744, val_acc: 0.3418181818181818\n",
      "Epoch 2900, training loss: 1.525200605392456, val loss: 2.842602014541626, val_acc: 0.33575757575757575\n",
      "Epoch 2910, training loss: 1.3919310569763184, val loss: 2.8130862712860107, val_acc: 0.3406060606060606\n",
      "Epoch 2920, training loss: 1.4575269222259521, val loss: 2.7850899696350098, val_acc: 0.3418181818181818\n",
      "Epoch 2930, training loss: 1.4182348251342773, val loss: 2.8416571617126465, val_acc: 0.343030303030303\n",
      "Epoch 2940, training loss: 1.443610668182373, val loss: 2.8050456047058105, val_acc: 0.343030303030303\n",
      "Epoch 2950, training loss: 1.3948098421096802, val loss: 2.8149302005767822, val_acc: 0.3527272727272727\n",
      "Epoch 2960, training loss: 1.4319682121276855, val loss: 2.8006362915039062, val_acc: 0.336969696969697\n",
      "Epoch 2970, training loss: 1.3870967626571655, val loss: 2.8183765411376953, val_acc: 0.3393939393939394\n",
      "Epoch 2980, training loss: 1.4608314037322998, val loss: 2.829801321029663, val_acc: 0.3466666666666667\n",
      "Epoch 2990, training loss: 1.3997621536254883, val loss: 2.835451602935791, val_acc: 0.3418181818181818\n",
      "Epoch 3000, training loss: 1.3365614414215088, val loss: 2.851597785949707, val_acc: 0.343030303030303\n",
      "Epoch 3010, training loss: 1.4404836893081665, val loss: 2.8533122539520264, val_acc: 0.3466666666666667\n",
      "Epoch 3020, training loss: 1.4683465957641602, val loss: 2.8359556198120117, val_acc: 0.3418181818181818\n",
      "Epoch 3030, training loss: 1.4544334411621094, val loss: 2.81881046295166, val_acc: 0.3393939393939394\n",
      "Epoch 3040, training loss: 1.4317508935928345, val loss: 2.831637144088745, val_acc: 0.343030303030303\n",
      "Epoch 3050, training loss: 1.4610906839370728, val loss: 2.8011746406555176, val_acc: 0.3478787878787879\n",
      "Epoch 3060, training loss: 1.4367998838424683, val loss: 2.8041749000549316, val_acc: 0.3393939393939394\n",
      "Epoch 3070, training loss: 1.4560058116912842, val loss: 2.889150381088257, val_acc: 0.3418181818181818\n",
      "Epoch 3080, training loss: 1.3993775844573975, val loss: 2.827225923538208, val_acc: 0.343030303030303\n",
      "Epoch 3090, training loss: 1.3725873231887817, val loss: 2.8528215885162354, val_acc: 0.3381818181818182\n",
      "Epoch 3100, training loss: 1.3959434032440186, val loss: 2.832811117172241, val_acc: 0.3393939393939394\n",
      "Epoch 3110, training loss: 1.3608943223953247, val loss: 2.8288655281066895, val_acc: 0.343030303030303\n",
      "Epoch 3120, training loss: 1.4924180507659912, val loss: 2.82981276512146, val_acc: 0.33575757575757575\n",
      "Epoch 3130, training loss: 1.395696997642517, val loss: 2.8583085536956787, val_acc: 0.33575757575757575\n",
      "Epoch 3140, training loss: 1.4342129230499268, val loss: 2.831498384475708, val_acc: 0.3418181818181818\n",
      "Epoch 3150, training loss: 1.4369739294052124, val loss: 2.813985824584961, val_acc: 0.3321212121212121\n",
      "Epoch 3160, training loss: 1.3449251651763916, val loss: 2.82607102394104, val_acc: 0.3321212121212121\n",
      "Epoch 3170, training loss: 1.4475200176239014, val loss: 2.841041088104248, val_acc: 0.3333333333333333\n",
      "Epoch 3180, training loss: 1.3846439123153687, val loss: 2.8566792011260986, val_acc: 0.3418181818181818\n",
      "Epoch 3190, training loss: 1.4237881898880005, val loss: 2.805816650390625, val_acc: 0.336969696969697\n",
      "Epoch 3200, training loss: 1.4417237043380737, val loss: 2.8524065017700195, val_acc: 0.3478787878787879\n",
      "Epoch 3210, training loss: 1.4637190103530884, val loss: 2.8281326293945312, val_acc: 0.3406060606060606\n",
      "Epoch 3220, training loss: 1.3951576948165894, val loss: 2.8507907390594482, val_acc: 0.3393939393939394\n",
      "Epoch 3230, training loss: 1.364820957183838, val loss: 2.8778340816497803, val_acc: 0.343030303030303\n",
      "Epoch 3240, training loss: 1.3833833932876587, val loss: 2.880180835723877, val_acc: 0.343030303030303\n",
      "Epoch 3250, training loss: 1.3411123752593994, val loss: 2.8646562099456787, val_acc: 0.3466666666666667\n",
      "Epoch 3260, training loss: 1.3945000171661377, val loss: 2.8601772785186768, val_acc: 0.3418181818181818\n",
      "Epoch 3270, training loss: 1.4543559551239014, val loss: 2.9051156044006348, val_acc: 0.3418181818181818\n",
      "Epoch 3280, training loss: 1.3650288581848145, val loss: 2.8633105754852295, val_acc: 0.3466666666666667\n",
      "Epoch 3290, training loss: 1.3287763595581055, val loss: 2.8568522930145264, val_acc: 0.33454545454545453\n",
      "Epoch 3300, training loss: 1.4381730556488037, val loss: 2.8675637245178223, val_acc: 0.34545454545454546\n",
      "Epoch 3310, training loss: 1.3599685430526733, val loss: 2.8602731227874756, val_acc: 0.3478787878787879\n",
      "Epoch 3320, training loss: 1.3775075674057007, val loss: 2.8753480911254883, val_acc: 0.3284848484848485\n",
      "Epoch 3330, training loss: 1.4318870306015015, val loss: 2.861302137374878, val_acc: 0.3478787878787879\n",
      "Epoch 3340, training loss: 1.3701976537704468, val loss: 2.862438440322876, val_acc: 0.343030303030303\n",
      "Epoch 3350, training loss: 1.3916003704071045, val loss: 2.8610310554504395, val_acc: 0.336969696969697\n",
      "Epoch 3360, training loss: 1.423511028289795, val loss: 2.8937861919403076, val_acc: 0.3418181818181818\n",
      "Epoch 3370, training loss: 1.4592971801757812, val loss: 2.8453986644744873, val_acc: 0.343030303030303\n",
      "Epoch 3380, training loss: 1.4466099739074707, val loss: 2.860384702682495, val_acc: 0.3503030303030303\n",
      "Epoch 3390, training loss: 1.4402505159378052, val loss: 2.87388277053833, val_acc: 0.34545454545454546\n",
      "Epoch 3400, training loss: 1.3993817567825317, val loss: 2.8718676567077637, val_acc: 0.3466666666666667\n",
      "Epoch 3410, training loss: 1.3872770071029663, val loss: 2.9053854942321777, val_acc: 0.3406060606060606\n",
      "Epoch 3420, training loss: 1.4185218811035156, val loss: 2.8374714851379395, val_acc: 0.343030303030303\n",
      "Epoch 3430, training loss: 1.3821110725402832, val loss: 2.9356181621551514, val_acc: 0.3466666666666667\n",
      "Epoch 3440, training loss: 1.3945492506027222, val loss: 2.9311952590942383, val_acc: 0.336969696969697\n",
      "Epoch 3450, training loss: 1.3875732421875, val loss: 2.9024441242218018, val_acc: 0.3515151515151515\n",
      "Epoch 3460, training loss: 1.4295670986175537, val loss: 2.8690991401672363, val_acc: 0.3466666666666667\n",
      "Epoch 3470, training loss: 1.394302248954773, val loss: 2.903677463531494, val_acc: 0.3466666666666667\n",
      "Epoch 3480, training loss: 1.381800889968872, val loss: 2.9279592037200928, val_acc: 0.34545454545454546\n",
      "Epoch 3490, training loss: 1.3059749603271484, val loss: 2.9102654457092285, val_acc: 0.3503030303030303\n",
      "Epoch 3500, training loss: 1.3893624544143677, val loss: 2.913339614868164, val_acc: 0.343030303030303\n",
      "Epoch 3510, training loss: 1.2912023067474365, val loss: 2.883167028427124, val_acc: 0.3406060606060606\n",
      "Epoch 3520, training loss: 1.3759558200836182, val loss: 2.8883392810821533, val_acc: 0.3418181818181818\n",
      "Epoch 3530, training loss: 1.3878947496414185, val loss: 2.8924009799957275, val_acc: 0.3490909090909091\n",
      "Epoch 3540, training loss: 1.3870853185653687, val loss: 2.9501495361328125, val_acc: 0.34545454545454546\n",
      "Epoch 3550, training loss: 1.3809808492660522, val loss: 2.9024298191070557, val_acc: 0.336969696969697\n",
      "Epoch 3560, training loss: 1.2825474739074707, val loss: 2.917074203491211, val_acc: 0.343030303030303\n",
      "Epoch 3570, training loss: 1.3685495853424072, val loss: 2.912839412689209, val_acc: 0.343030303030303\n",
      "Epoch 3580, training loss: 1.4573395252227783, val loss: 2.924315929412842, val_acc: 0.34424242424242424\n",
      "Epoch 3590, training loss: 1.3923202753067017, val loss: 2.8689053058624268, val_acc: 0.3418181818181818\n",
      "Epoch 3600, training loss: 1.3767796754837036, val loss: 2.9286253452301025, val_acc: 0.34545454545454546\n",
      "Epoch 3610, training loss: 1.3785502910614014, val loss: 2.912872791290283, val_acc: 0.3478787878787879\n",
      "Epoch 3620, training loss: 1.3879998922348022, val loss: 2.9492177963256836, val_acc: 0.343030303030303\n",
      "Epoch 3630, training loss: 1.3471276760101318, val loss: 2.8937902450561523, val_acc: 0.3406060606060606\n",
      "Epoch 3640, training loss: 1.3539950847625732, val loss: 2.982464551925659, val_acc: 0.34545454545454546\n",
      "Epoch 3650, training loss: 1.3330715894699097, val loss: 2.8921871185302734, val_acc: 0.3418181818181818\n",
      "Epoch 3660, training loss: 1.3172028064727783, val loss: 2.892583131790161, val_acc: 0.34424242424242424\n",
      "Epoch 3670, training loss: 1.3080534934997559, val loss: 2.9657418727874756, val_acc: 0.3466666666666667\n",
      "Epoch 3680, training loss: 1.3885090351104736, val loss: 2.8850767612457275, val_acc: 0.3381818181818182\n",
      "Epoch 3690, training loss: 1.403527021408081, val loss: 2.934406280517578, val_acc: 0.3515151515151515\n",
      "Early stopping at epoch 2695 with validation accuracy 0.358788\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# from torch_geometric.datasets import Planetoid\n",
    "# dataset = Planetoid(root='./data', name='Cora') # \n",
    "# data = dataset[0]\n",
    "# nclass = data.y.max().item()+1\n",
    "\n",
    "gat_models = {}\n",
    "for name, data in dataset.items():\n",
    "    print(f\"Training GAT with {name}\")\n",
    "\n",
    "    features = data.x\n",
    "    labels = data.y\n",
    "\n",
    "    nclass = labels.max().item()+1\n",
    "\n",
    "    gat = GAT(nfeat=data.x.shape[1],\n",
    "        nhid=8, heads=8, nclass=nclass)\n",
    "    gat = gat.to(device)\n",
    "    data = data.to(device)\n",
    "    train(gat, data, lr=0.001, epochs=5000, patience=1000)\n",
    "\n",
    "    gat_models[name] = gat\n",
    "\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GAT with data_clip\n",
      "Test set results: loss= 2.6825 accuracy= 0.1649\n",
      "====================\n",
      "Testing GAT with data_cnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 3.5012 accuracy= 0.2126\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# test models\n",
    "for name, model in gat_models.items():\n",
    "    print(f\"Testing GAT with {name}\")\n",
    "    data = dataset[name]\n",
    "\n",
    "    test(model, data)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Implementing GraphSAGE for Product Classification (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation (15 points)\n",
    "The key component of GraphSAGE is sampling. We will use the NeighborSampler in PyG to implement this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that there are two weight matrices in GraphSAGE \n",
    "\n",
    "$$ \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "# dataset = Planetoid(root='./data', name='Cora') # \n",
    "# data = dataset[0]\n",
    "# nclass = data.y.max().item()+1\n",
    "\n",
    "# sizes=[10,5] # \n",
    "# train_idx = torch.arange(data.num_nodes)[data.train_mask]\n",
    "# train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n",
    "#                                sizes=sizes, batch_size=128,\n",
    "#                                shuffle=True, num_workers=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = dataset[\"data_clip\"] # the dataset i wrote, not the Planetoid\n",
    "nclass = dummy_data.y.max().item()+1\n",
    "\n",
    "sizes=[10,5]\n",
    "train_idx = torch.arange(dummy_data.num_nodes)[dummy_data.train_mask]\n",
    "train_loader = NeighborSampler(dummy_data.edge_index, node_idx=train_idx,\n",
    "                               sizes=sizes, batch_size=128,\n",
    "                               shuffle=True, num_workers=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\" 2 GraphSAGE layers\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    nfeat : input dimension\n",
    "    nhid : hidden dimension\n",
    "    nclass : number of classes\n",
    "    dropout : dropout ratio\n",
    "    with_bias: bias term\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(nfeat, nhid, bias=with_bias))\n",
    "        self.convs.append(SAGEConv(nhid, nclass, bias=with_bias))\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"parameter init\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        \"\"\" neighborsampler forward pass\"\"\"\n",
    "        num_layers = len(adjs)\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            #################\n",
    "            #### Write your code here ####\n",
    "            x_target = x[:size[1]]\n",
    "        \n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != num_layers-1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            \n",
    "        output = F.log_softmax(x, dim=1)  # no activation for final layer\n",
    "            ################\n",
    "        \n",
    "        return output   \n",
    "            \n",
    "    def inference(self, data):\n",
    "        \"\"\"Inference process: please note that we don't need sampling here, instread we use the all adj\"\"\"\n",
    "        #################\n",
    "        #### Write your code here ####\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            # x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        output = F.log_softmax(x, dim=1) \n",
    "        return output\n",
    "        ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_loader, epochs, device='cpu', lr=0.01, weight_decay=5e-4):\n",
    "    \"\"\"\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    x = data.x.to(device)\n",
    "    y = data.y.squeeze().to(device)\n",
    "    \n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        for batch_size, n_id, adjs in train_loader:\n",
    "            # `n_id`\n",
    "            # `adjs``(edge_index, e_id, size)`\n",
    "            adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x[n_id], adjs)\n",
    "            loss = F.nll_loss(out, y[n_id[:batch_size]]) # n_id[:batch_size]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        loss = total_loss / len(train_loader)\n",
    "        if it % 10 ==0:\n",
    "            print('Epoch:', it, 'training loss:', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    \"\"\"test the performance\"\"\"\n",
    "    model.eval() \n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model.inference(data) # \n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) # \n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() # \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GraphSAGE model and report your results (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GraphSAGE with data_clip\n",
      "Epoch: 0 training loss: 24.068870782852173\n",
      "Epoch: 10 training loss: 23.894299507141113\n",
      "Epoch: 20 training loss: 23.744982957839966\n",
      "Epoch: 30 training loss: 23.599663734436035\n",
      "Epoch: 40 training loss: 23.3934326171875\n",
      "Epoch: 50 training loss: 23.38793134689331\n",
      "Epoch: 60 training loss: 23.321603298187256\n",
      "Epoch: 70 training loss: 23.19320821762085\n",
      "Epoch: 80 training loss: 23.242236614227295\n",
      "Epoch: 90 training loss: 23.02843999862671\n",
      "Epoch: 100 training loss: 23.11826467514038\n",
      "Epoch: 110 training loss: 22.960644960403442\n",
      "Epoch: 120 training loss: 23.121707916259766\n",
      "Epoch: 130 training loss: 23.076107025146484\n",
      "Epoch: 140 training loss: 22.953493356704712\n",
      "Epoch: 150 training loss: 22.981765270233154\n",
      "Epoch: 160 training loss: 22.904357194900513\n",
      "Epoch: 170 training loss: 22.930277109146118\n",
      "Epoch: 180 training loss: 22.958219051361084\n",
      "Epoch: 190 training loss: 22.954202890396118\n",
      "Epoch: 200 training loss: 22.885342597961426\n",
      "Epoch: 210 training loss: 22.827638864517212\n",
      "Epoch: 220 training loss: 22.93965172767639\n",
      "Epoch: 230 training loss: 22.865912675857544\n",
      "Epoch: 240 training loss: 22.79129648208618\n",
      "Epoch: 250 training loss: 22.69395112991333\n",
      "Epoch: 260 training loss: 22.957608699798584\n",
      "Epoch: 270 training loss: 22.73660445213318\n",
      "Epoch: 280 training loss: 22.871428728103638\n",
      "Epoch: 290 training loss: 22.871416091918945\n",
      "Epoch: 300 training loss: 22.557020902633667\n",
      "Epoch: 310 training loss: 22.72205662727356\n",
      "Epoch: 320 training loss: 22.568334579467773\n",
      "Epoch: 330 training loss: 22.549059867858887\n",
      "Epoch: 340 training loss: 22.509270668029785\n",
      "Epoch: 350 training loss: 22.428152799606323\n",
      "Epoch: 360 training loss: 22.513808965682983\n",
      "Epoch: 370 training loss: 22.38943099975586\n",
      "Epoch: 380 training loss: 22.56614065170288\n",
      "Epoch: 390 training loss: 22.446707725524902\n",
      "Epoch: 400 training loss: 22.204303741455078\n",
      "Epoch: 410 training loss: 22.663647174835205\n",
      "Epoch: 420 training loss: 22.590035915374756\n",
      "Epoch: 430 training loss: 22.4520480632782\n",
      "Epoch: 440 training loss: 22.20398712158203\n",
      "Epoch: 450 training loss: 22.452394008636475\n",
      "Epoch: 460 training loss: 22.25460457801819\n",
      "Epoch: 470 training loss: 22.38203740119934\n",
      "Epoch: 480 training loss: 22.273760080337524\n",
      "Epoch: 490 training loss: 22.42371940612793\n",
      "Epoch: 500 training loss: 22.26651167869568\n",
      "Epoch: 510 training loss: 22.28831148147583\n",
      "Epoch: 520 training loss: 22.256201028823853\n",
      "Epoch: 530 training loss: 22.45770287513733\n",
      "Epoch: 540 training loss: 22.259225606918335\n",
      "Epoch: 550 training loss: 22.201950788497925\n",
      "Epoch: 560 training loss: 22.269408464431763\n",
      "Epoch: 570 training loss: 22.252638578414917\n",
      "Epoch: 580 training loss: 22.226417303085327\n",
      "Epoch: 590 training loss: 22.304574489593506\n",
      "Epoch: 600 training loss: 22.307331562042236\n",
      "Epoch: 610 training loss: 22.270581483840942\n",
      "Epoch: 620 training loss: 22.056806564331055\n",
      "Epoch: 630 training loss: 22.28463649749756\n",
      "Epoch: 640 training loss: 22.25271511077881\n",
      "Epoch: 650 training loss: 22.161534547805786\n",
      "Epoch: 660 training loss: 22.317052364349365\n",
      "Epoch: 670 training loss: 22.149507522583008\n",
      "Epoch: 680 training loss: 21.853102922439575\n",
      "Epoch: 690 training loss: 22.116942405700684\n",
      "Epoch: 700 training loss: 22.237402200698853\n",
      "Epoch: 710 training loss: 22.168949127197266\n",
      "Epoch: 720 training loss: 22.20052409172058\n",
      "Epoch: 730 training loss: 22.107798099517822\n",
      "Epoch: 740 training loss: 22.040230751037598\n",
      "Epoch: 750 training loss: 22.01890730857849\n",
      "Epoch: 760 training loss: 22.12467646598816\n",
      "Epoch: 770 training loss: 22.01958441734314\n",
      "Epoch: 780 training loss: 21.866817712783813\n",
      "Epoch: 790 training loss: 21.891409873962402\n",
      "Epoch: 800 training loss: 22.08189868927002\n",
      "Epoch: 810 training loss: 21.74373173713684\n",
      "Epoch: 820 training loss: 21.831957578659058\n",
      "Epoch: 830 training loss: 21.986573457717896\n",
      "Epoch: 840 training loss: 21.697230339050293\n",
      "Epoch: 850 training loss: 22.073769092559814\n",
      "Epoch: 860 training loss: 21.8852961063385\n",
      "Epoch: 870 training loss: 21.944828510284424\n",
      "Epoch: 880 training loss: 21.889490365982056\n",
      "Epoch: 890 training loss: 21.94542145729065\n",
      "Epoch: 900 training loss: 21.981045484542847\n",
      "Epoch: 910 training loss: 21.913904666900635\n",
      "Epoch: 920 training loss: 21.780686378479004\n",
      "Epoch: 930 training loss: 21.91586923599243\n",
      "Epoch: 940 training loss: 21.798521041870117\n",
      "Epoch: 950 training loss: 21.907835960388184\n",
      "Epoch: 960 training loss: 21.861053228378296\n",
      "Epoch: 970 training loss: 21.957555532455444\n",
      "Epoch: 980 training loss: 21.72004461288452\n",
      "Epoch: 990 training loss: 21.674240112304688\n",
      "Epoch: 1000 training loss: 21.84249234199524\n",
      "Epoch: 1010 training loss: 21.7803475856781\n",
      "Epoch: 1020 training loss: 21.803478240966797\n",
      "Epoch: 1030 training loss: 21.709770441055298\n",
      "Epoch: 1040 training loss: 21.821356296539307\n",
      "Epoch: 1050 training loss: 21.759801387786865\n",
      "Epoch: 1060 training loss: 21.44957733154297\n",
      "Epoch: 1070 training loss: 21.647698163986206\n",
      "Epoch: 1080 training loss: 21.772278308868408\n",
      "Epoch: 1090 training loss: 21.70020604133606\n",
      "Epoch: 1100 training loss: 21.97098183631897\n",
      "Epoch: 1110 training loss: 21.747591733932495\n",
      "Epoch: 1120 training loss: 21.78986620903015\n",
      "Epoch: 1130 training loss: 21.87278389930725\n",
      "Epoch: 1140 training loss: 21.71664333343506\n",
      "Epoch: 1150 training loss: 21.678043365478516\n",
      "Epoch: 1160 training loss: 21.744916439056396\n",
      "Epoch: 1170 training loss: 21.62619924545288\n",
      "Epoch: 1180 training loss: 21.624936819076538\n",
      "Epoch: 1190 training loss: 21.897401094436646\n",
      "Epoch: 1200 training loss: 21.5937397480011\n",
      "Epoch: 1210 training loss: 21.731825828552246\n",
      "Epoch: 1220 training loss: 21.742652416229248\n",
      "Epoch: 1230 training loss: 21.714736461639404\n",
      "Epoch: 1240 training loss: 21.75555968284607\n",
      "Epoch: 1250 training loss: 21.56645107269287\n",
      "Epoch: 1260 training loss: 21.458712100982666\n",
      "Epoch: 1270 training loss: 21.531550884246826\n",
      "Epoch: 1280 training loss: 21.594306230545044\n",
      "Epoch: 1290 training loss: 21.519345998764038\n",
      "Epoch: 1300 training loss: 21.308322191238403\n",
      "Epoch: 1310 training loss: 21.46100616455078\n",
      "Epoch: 1320 training loss: 21.615352869033813\n",
      "Epoch: 1330 training loss: 21.557304620742798\n",
      "Epoch: 1340 training loss: 21.64379119873047\n",
      "Epoch: 1350 training loss: 21.45537781715393\n",
      "Epoch: 1360 training loss: 21.749414205551147\n",
      "Epoch: 1370 training loss: 21.522118091583252\n",
      "Epoch: 1380 training loss: 21.54719877243042\n",
      "Epoch: 1390 training loss: 21.767438888549805\n",
      "Epoch: 1400 training loss: 21.620361328125\n",
      "Epoch: 1410 training loss: 21.779385328292847\n",
      "Epoch: 1420 training loss: 21.550788402557373\n",
      "Epoch: 1430 training loss: 21.620028495788574\n",
      "Epoch: 1440 training loss: 21.602203845977783\n",
      "Epoch: 1450 training loss: 21.638280153274536\n",
      "Epoch: 1460 training loss: 21.530482530593872\n",
      "Epoch: 1470 training loss: 21.612212896347046\n",
      "Epoch: 1480 training loss: 21.421733617782593\n",
      "Epoch: 1490 training loss: 21.810588121414185\n",
      "Epoch: 1500 training loss: 21.664795398712158\n",
      "Epoch: 1510 training loss: 21.540045022964478\n",
      "Epoch: 1520 training loss: 21.80967926979065\n",
      "Epoch: 1530 training loss: 21.364583492279053\n",
      "Epoch: 1540 training loss: 21.58363175392151\n",
      "Epoch: 1550 training loss: 21.554078340530396\n",
      "Epoch: 1560 training loss: 21.629244327545166\n",
      "Epoch: 1570 training loss: 21.4885413646698\n",
      "Epoch: 1580 training loss: 21.489410638809204\n",
      "Epoch: 1590 training loss: 21.435195207595825\n",
      "Epoch: 1600 training loss: 21.391506910324097\n",
      "Epoch: 1610 training loss: 21.65820550918579\n",
      "Epoch: 1620 training loss: 21.433740377426147\n",
      "Epoch: 1630 training loss: 21.31794261932373\n",
      "Epoch: 1640 training loss: 21.528545141220093\n",
      "Epoch: 1650 training loss: 21.547670602798462\n",
      "Epoch: 1660 training loss: 21.511914014816284\n",
      "Epoch: 1670 training loss: 21.420947074890137\n",
      "Epoch: 1680 training loss: 21.39409589767456\n",
      "Epoch: 1690 training loss: 21.4478120803833\n",
      "Epoch: 1700 training loss: 21.72347092628479\n",
      "Epoch: 1710 training loss: 21.336414337158203\n",
      "Epoch: 1720 training loss: 21.040138483047485\n",
      "Epoch: 1730 training loss: 21.398427486419678\n",
      "Epoch: 1740 training loss: 21.63824486732483\n",
      "Epoch: 1750 training loss: 21.245529651641846\n",
      "Epoch: 1760 training loss: 21.504443407058716\n",
      "Epoch: 1770 training loss: 21.34009885787964\n",
      "Epoch: 1780 training loss: 21.70961093902588\n",
      "Epoch: 1790 training loss: 21.199778079986572\n",
      "Epoch: 1800 training loss: 21.557846784591675\n",
      "Epoch: 1810 training loss: 21.274413108825684\n",
      "Epoch: 1820 training loss: 21.351329803466797\n",
      "Epoch: 1830 training loss: 21.267492055892944\n",
      "Epoch: 1840 training loss: 21.50425887107849\n",
      "Epoch: 1850 training loss: 21.508503198623657\n",
      "Epoch: 1860 training loss: 21.342674732208252\n",
      "Epoch: 1870 training loss: 21.48563861846924\n",
      "Epoch: 1880 training loss: 21.228333711624146\n",
      "Epoch: 1890 training loss: 21.288984537124634\n",
      "Epoch: 1900 training loss: 21.472543001174927\n",
      "Epoch: 1910 training loss: 21.338499784469604\n",
      "Epoch: 1920 training loss: 21.319648265838623\n",
      "Epoch: 1930 training loss: 21.585105657577515\n",
      "Epoch: 1940 training loss: 21.366208791732788\n",
      "Epoch: 1950 training loss: 21.325390338897705\n",
      "Epoch: 1960 training loss: 21.294666051864624\n",
      "Epoch: 1970 training loss: 21.182486295700073\n",
      "Epoch: 1980 training loss: 21.221527576446533\n",
      "Epoch: 1990 training loss: 21.292128086090088\n",
      "Epoch: 2000 training loss: 21.271949768066406\n",
      "Epoch: 2010 training loss: 21.23737645149231\n",
      "Epoch: 2020 training loss: 21.4563946723938\n",
      "Epoch: 2030 training loss: 21.138948440551758\n",
      "Epoch: 2040 training loss: 21.33544921875\n",
      "Epoch: 2050 training loss: 21.16627860069275\n",
      "Epoch: 2060 training loss: 21.291606187820435\n",
      "Epoch: 2070 training loss: 21.571916103363037\n",
      "Epoch: 2080 training loss: 21.30858564376831\n",
      "Epoch: 2090 training loss: 21.246148824691772\n",
      "Epoch: 2100 training loss: 20.991434335708618\n",
      "Epoch: 2110 training loss: 21.4372820854187\n",
      "Epoch: 2120 training loss: 21.245343923568726\n",
      "Epoch: 2130 training loss: 21.41415762901306\n",
      "Epoch: 2140 training loss: 21.455416679382324\n",
      "Epoch: 2150 training loss: 21.257023096084595\n",
      "Epoch: 2160 training loss: 21.22974467277527\n",
      "Epoch: 2170 training loss: 21.21074867248535\n",
      "Epoch: 2180 training loss: 21.225164890289307\n",
      "Epoch: 2190 training loss: 21.19102382659912\n",
      "Epoch: 2200 training loss: 21.279972791671753\n",
      "Epoch: 2210 training loss: 21.222548484802246\n",
      "Epoch: 2220 training loss: 21.152271032333374\n",
      "Epoch: 2230 training loss: 21.21260643005371\n",
      "Epoch: 2240 training loss: 21.00924515724182\n",
      "Epoch: 2250 training loss: 21.44672966003418\n",
      "Epoch: 2260 training loss: 21.20271635055542\n",
      "Epoch: 2270 training loss: 21.302345991134644\n",
      "Epoch: 2280 training loss: 21.259129285812378\n",
      "Epoch: 2290 training loss: 21.380974292755127\n",
      "Epoch: 2300 training loss: 20.998687982559204\n",
      "Epoch: 2310 training loss: 21.2408344745636\n",
      "Epoch: 2320 training loss: 21.060523986816406\n",
      "Epoch: 2330 training loss: 21.353480339050293\n",
      "Epoch: 2340 training loss: 21.349982023239136\n",
      "Epoch: 2350 training loss: 21.115813732147217\n",
      "Epoch: 2360 training loss: 21.15744972229004\n",
      "Epoch: 2370 training loss: 21.139739751815796\n",
      "Epoch: 2380 training loss: 21.227771520614624\n",
      "Epoch: 2390 training loss: 21.176437377929688\n",
      "Epoch: 2400 training loss: 21.193655490875244\n",
      "Epoch: 2410 training loss: 21.33236312866211\n",
      "Epoch: 2420 training loss: 21.0651798248291\n",
      "Epoch: 2430 training loss: 21.046475410461426\n",
      "Epoch: 2440 training loss: 21.11616563796997\n",
      "Epoch: 2450 training loss: 21.303151845932007\n",
      "Epoch: 2460 training loss: 21.2795147895813\n",
      "Epoch: 2470 training loss: 20.99246835708618\n",
      "Epoch: 2480 training loss: 21.058759212493896\n",
      "Epoch: 2490 training loss: 21.187144994735718\n",
      "Epoch: 2500 training loss: 20.921005487442017\n",
      "Epoch: 2510 training loss: 20.857923984527588\n",
      "Epoch: 2520 training loss: 21.058681964874268\n",
      "Epoch: 2530 training loss: 21.29599642753601\n",
      "Epoch: 2540 training loss: 21.237024307250977\n",
      "Epoch: 2550 training loss: 21.260486841201782\n",
      "Epoch: 2560 training loss: 21.271221160888672\n",
      "Epoch: 2570 training loss: 21.037611484527588\n",
      "Epoch: 2580 training loss: 21.18996238708496\n",
      "Epoch: 2590 training loss: 21.4094979763031\n",
      "Epoch: 2600 training loss: 21.3306725025177\n",
      "Epoch: 2610 training loss: 21.38128924369812\n",
      "Epoch: 2620 training loss: 20.999632835388184\n",
      "Epoch: 2630 training loss: 20.995610237121582\n",
      "Epoch: 2640 training loss: 21.020893812179565\n",
      "Epoch: 2650 training loss: 21.352271556854248\n",
      "Epoch: 2660 training loss: 20.896322011947632\n",
      "Epoch: 2670 training loss: 21.0569851398468\n",
      "Epoch: 2680 training loss: 21.131966590881348\n",
      "Epoch: 2690 training loss: 21.318446397781372\n",
      "Epoch: 2700 training loss: 21.114517211914062\n",
      "Epoch: 2710 training loss: 21.12028670310974\n",
      "Epoch: 2720 training loss: 21.131388902664185\n",
      "Epoch: 2730 training loss: 21.08323621749878\n",
      "Epoch: 2740 training loss: 21.10119318962097\n",
      "Epoch: 2750 training loss: 21.266618490219116\n",
      "Epoch: 2760 training loss: 21.28834056854248\n",
      "Epoch: 2770 training loss: 21.099937915802002\n",
      "Epoch: 2780 training loss: 21.031699895858765\n",
      "Epoch: 2790 training loss: 20.773512840270996\n",
      "Epoch: 2800 training loss: 21.23247718811035\n",
      "Epoch: 2810 training loss: 21.07096219062805\n",
      "Epoch: 2820 training loss: 21.087881565093994\n",
      "Epoch: 2830 training loss: 20.961379289627075\n",
      "Epoch: 2840 training loss: 21.117515325546265\n",
      "Epoch: 2850 training loss: 21.061176300048828\n",
      "Epoch: 2860 training loss: 21.174716234207153\n",
      "Epoch: 2870 training loss: 20.91683053970337\n",
      "Epoch: 2880 training loss: 21.16202974319458\n",
      "Epoch: 2890 training loss: 20.938458681106567\n",
      "Epoch: 2900 training loss: 20.974823713302612\n",
      "Epoch: 2910 training loss: 21.008551597595215\n",
      "Epoch: 2920 training loss: 20.85791277885437\n",
      "Epoch: 2930 training loss: 21.083351135253906\n",
      "Epoch: 2940 training loss: 21.170573949813843\n",
      "Epoch: 2950 training loss: 21.27136206626892\n",
      "Epoch: 2960 training loss: 21.00683617591858\n",
      "Epoch: 2970 training loss: 21.2906494140625\n",
      "Epoch: 2980 training loss: 21.040504455566406\n",
      "Epoch: 2990 training loss: 21.10127878189087\n",
      "Epoch: 3000 training loss: 20.93261694908142\n",
      "Epoch: 3010 training loss: 21.10800266265869\n",
      "Epoch: 3020 training loss: 21.066874742507935\n",
      "Epoch: 3030 training loss: 20.848849534988403\n",
      "Epoch: 3040 training loss: 21.00985312461853\n",
      "Epoch: 3050 training loss: 21.210723400115967\n",
      "Epoch: 3060 training loss: 20.89651370048523\n",
      "Epoch: 3070 training loss: 21.040406703948975\n",
      "Epoch: 3080 training loss: 21.05258846282959\n",
      "Epoch: 3090 training loss: 20.81008267402649\n",
      "Epoch: 3100 training loss: 21.05246663093567\n",
      "Epoch: 3110 training loss: 20.951574325561523\n",
      "Epoch: 3120 training loss: 21.139394760131836\n",
      "Epoch: 3130 training loss: 20.861229181289673\n",
      "Epoch: 3140 training loss: 20.910818815231323\n",
      "Epoch: 3150 training loss: 21.113778829574585\n",
      "Epoch: 3160 training loss: 20.968433141708374\n",
      "Epoch: 3170 training loss: 20.934970140457153\n",
      "Epoch: 3180 training loss: 21.140902519226074\n",
      "Epoch: 3190 training loss: 21.13810682296753\n",
      "Epoch: 3200 training loss: 20.95570993423462\n",
      "Epoch: 3210 training loss: 20.903796911239624\n",
      "Epoch: 3220 training loss: 20.909499883651733\n",
      "Epoch: 3230 training loss: 21.05630111694336\n",
      "Epoch: 3240 training loss: 21.15161442756653\n",
      "Epoch: 3250 training loss: 20.910475254058838\n",
      "Epoch: 3260 training loss: 20.799411058425903\n",
      "Epoch: 3270 training loss: 20.998700857162476\n",
      "Epoch: 3280 training loss: 21.1002140045166\n",
      "Epoch: 3290 training loss: 21.101230144500732\n",
      "Epoch: 3300 training loss: 20.940616607666016\n",
      "Epoch: 3310 training loss: 21.071905612945557\n",
      "Epoch: 3320 training loss: 20.76304292678833\n",
      "Epoch: 3330 training loss: 20.971713066101074\n",
      "Epoch: 3340 training loss: 20.938071489334106\n",
      "Epoch: 3350 training loss: 21.305799961090088\n",
      "Epoch: 3360 training loss: 20.709152221679688\n",
      "Epoch: 3370 training loss: 21.13917374610901\n",
      "Epoch: 3380 training loss: 21.14397168159485\n",
      "Epoch: 3390 training loss: 20.99207830429077\n",
      "Epoch: 3400 training loss: 20.958686590194702\n",
      "Epoch: 3410 training loss: 21.153964281082153\n",
      "Epoch: 3420 training loss: 20.985613584518433\n",
      "Epoch: 3430 training loss: 20.766387462615967\n",
      "Epoch: 3440 training loss: 20.86338710784912\n",
      "Epoch: 3450 training loss: 20.878849744796753\n",
      "Epoch: 3460 training loss: 21.073802947998047\n",
      "Epoch: 3470 training loss: 21.18352770805359\n",
      "Epoch: 3480 training loss: 20.8745756149292\n",
      "Epoch: 3490 training loss: 20.748931646347046\n",
      "Epoch: 3500 training loss: 20.756246089935303\n",
      "Epoch: 3510 training loss: 20.93999981880188\n",
      "Epoch: 3520 training loss: 21.113707542419434\n",
      "Epoch: 3530 training loss: 20.661282777786255\n",
      "Epoch: 3540 training loss: 20.605531454086304\n",
      "Epoch: 3550 training loss: 21.00154685974121\n",
      "Epoch: 3560 training loss: 21.153051137924194\n",
      "Epoch: 3570 training loss: 20.941566944122314\n",
      "Epoch: 3580 training loss: 20.834107398986816\n",
      "Epoch: 3590 training loss: 20.81040644645691\n",
      "Epoch: 3600 training loss: 20.945956468582153\n",
      "Epoch: 3610 training loss: 20.967277765274048\n",
      "Epoch: 3620 training loss: 21.135701656341553\n",
      "Epoch: 3630 training loss: 20.997860431671143\n",
      "Epoch: 3640 training loss: 21.124836683273315\n",
      "Epoch: 3650 training loss: 20.968647003173828\n",
      "Epoch: 3660 training loss: 20.84907364845276\n",
      "Epoch: 3670 training loss: 20.925873517990112\n",
      "Epoch: 3680 training loss: 21.136154174804688\n",
      "Epoch: 3690 training loss: 21.156031847000122\n",
      "Epoch: 3700 training loss: 20.990200757980347\n",
      "Epoch: 3710 training loss: 21.103158473968506\n",
      "Epoch: 3720 training loss: 21.070545434951782\n",
      "Epoch: 3730 training loss: 21.078896522521973\n",
      "Epoch: 3740 training loss: 20.690251350402832\n",
      "Epoch: 3750 training loss: 21.09952998161316\n",
      "Epoch: 3760 training loss: 20.76324439048767\n",
      "Epoch: 3770 training loss: 20.820258140563965\n",
      "Epoch: 3780 training loss: 20.753222227096558\n",
      "Epoch: 3790 training loss: 20.612388372421265\n",
      "Epoch: 3800 training loss: 20.670547008514404\n",
      "Epoch: 3810 training loss: 21.055749893188477\n",
      "Epoch: 3820 training loss: 21.098806381225586\n",
      "Epoch: 3830 training loss: 21.106405019760132\n",
      "Epoch: 3840 training loss: 20.986149311065674\n",
      "Epoch: 3850 training loss: 20.82684636116028\n",
      "Epoch: 3860 training loss: 20.703411102294922\n",
      "Epoch: 3870 training loss: 21.123614072799683\n",
      "Epoch: 3880 training loss: 20.978153705596924\n",
      "Epoch: 3890 training loss: 20.947103023529053\n",
      "Epoch: 3900 training loss: 21.056073427200317\n",
      "Epoch: 3910 training loss: 20.668405294418335\n",
      "Epoch: 3920 training loss: 20.948721408843994\n",
      "Epoch: 3930 training loss: 20.76443076133728\n",
      "Epoch: 3940 training loss: 20.828517198562622\n",
      "Epoch: 3950 training loss: 20.950502157211304\n",
      "Epoch: 3960 training loss: 20.96013879776001\n",
      "Epoch: 3970 training loss: 20.966719388961792\n",
      "Epoch: 3980 training loss: 20.791587591171265\n",
      "Epoch: 3990 training loss: 20.737871885299683\n",
      "Epoch: 4000 training loss: 20.979694366455078\n",
      "Epoch: 4010 training loss: 20.86311435699463\n",
      "Epoch: 4020 training loss: 20.732025384902954\n",
      "Epoch: 4030 training loss: 20.963677883148193\n",
      "Epoch: 4040 training loss: 20.945950031280518\n",
      "Epoch: 4050 training loss: 20.784199476242065\n",
      "Epoch: 4060 training loss: 20.719879388809204\n",
      "Epoch: 4070 training loss: 21.032928228378296\n",
      "Epoch: 4080 training loss: 20.86824941635132\n",
      "Epoch: 4090 training loss: 20.899808645248413\n",
      "Epoch: 4100 training loss: 21.077207803726196\n",
      "Epoch: 4110 training loss: 20.969608306884766\n",
      "Epoch: 4120 training loss: 20.794126749038696\n",
      "Epoch: 4130 training loss: 20.88806700706482\n",
      "Epoch: 4140 training loss: 21.01831889152527\n",
      "Epoch: 4150 training loss: 21.00813937187195\n",
      "Epoch: 4160 training loss: 20.91653609275818\n",
      "Epoch: 4170 training loss: 20.921982049942017\n",
      "Epoch: 4180 training loss: 20.732460021972656\n",
      "Epoch: 4190 training loss: 21.0082004070282\n",
      "Epoch: 4200 training loss: 20.861929178237915\n",
      "Epoch: 4210 training loss: 20.772436141967773\n",
      "Epoch: 4220 training loss: 20.926319122314453\n",
      "Epoch: 4230 training loss: 21.12849497795105\n",
      "Epoch: 4240 training loss: 20.942128658294678\n",
      "Epoch: 4250 training loss: 20.82742667198181\n",
      "Epoch: 4260 training loss: 21.044087171554565\n",
      "Epoch: 4270 training loss: 20.5593204498291\n",
      "Epoch: 4280 training loss: 20.927748918533325\n",
      "Epoch: 4290 training loss: 20.80252194404602\n",
      "Epoch: 4300 training loss: 20.875982999801636\n",
      "Epoch: 4310 training loss: 20.862574338912964\n",
      "Epoch: 4320 training loss: 20.998682498931885\n",
      "Epoch: 4330 training loss: 20.904576301574707\n",
      "Epoch: 4340 training loss: 20.667171955108643\n",
      "Epoch: 4350 training loss: 20.9685959815979\n",
      "Epoch: 4360 training loss: 21.054682970046997\n",
      "Epoch: 4370 training loss: 20.668196439743042\n",
      "Epoch: 4380 training loss: 20.7629554271698\n",
      "Epoch: 4390 training loss: 20.605529069900513\n",
      "Epoch: 4400 training loss: 20.99466633796692\n",
      "Epoch: 4410 training loss: 20.719165802001953\n",
      "Epoch: 4420 training loss: 20.87914204597473\n",
      "Epoch: 4430 training loss: 20.868401765823364\n",
      "Epoch: 4440 training loss: 20.631500244140625\n",
      "Epoch: 4450 training loss: 20.791894674301147\n",
      "Epoch: 4460 training loss: 20.84813356399536\n",
      "Epoch: 4470 training loss: 20.992051124572754\n",
      "Epoch: 4480 training loss: 20.860742568969727\n",
      "Epoch: 4490 training loss: 20.8070285320282\n",
      "Epoch: 4500 training loss: 21.260606050491333\n",
      "Epoch: 4510 training loss: 20.74737048149109\n",
      "Epoch: 4520 training loss: 20.740410566329956\n",
      "Epoch: 4530 training loss: 20.97625970840454\n",
      "Epoch: 4540 training loss: 20.901174306869507\n",
      "Epoch: 4550 training loss: 20.68131422996521\n",
      "Epoch: 4560 training loss: 21.142472505569458\n",
      "Epoch: 4570 training loss: 20.831602096557617\n",
      "Epoch: 4580 training loss: 20.675260066986084\n",
      "Epoch: 4590 training loss: 20.959298610687256\n",
      "Epoch: 4600 training loss: 20.78547430038452\n",
      "Epoch: 4610 training loss: 20.788597345352173\n",
      "Epoch: 4620 training loss: 20.807236671447754\n",
      "Epoch: 4630 training loss: 20.823721408843994\n",
      "Epoch: 4640 training loss: 20.747936487197876\n",
      "Epoch: 4650 training loss: 20.797703504562378\n",
      "Epoch: 4660 training loss: 20.79473042488098\n",
      "Epoch: 4670 training loss: 20.719972610473633\n",
      "Epoch: 4680 training loss: 20.788029432296753\n",
      "Epoch: 4690 training loss: 20.91656756401062\n",
      "Epoch: 4700 training loss: 20.84575343132019\n",
      "Epoch: 4710 training loss: 20.743396997451782\n",
      "Epoch: 4720 training loss: 20.888216972351074\n",
      "Epoch: 4730 training loss: 20.759093761444092\n",
      "Epoch: 4740 training loss: 21.111423015594482\n",
      "Epoch: 4750 training loss: 20.891629219055176\n",
      "Epoch: 4760 training loss: 20.87265682220459\n",
      "Epoch: 4770 training loss: 20.95713472366333\n",
      "Epoch: 4780 training loss: 20.748977184295654\n",
      "Epoch: 4790 training loss: 20.989378213882446\n",
      "Epoch: 4800 training loss: 20.860679149627686\n",
      "Epoch: 4810 training loss: 20.960165977478027\n",
      "Epoch: 4820 training loss: 20.832414627075195\n",
      "Epoch: 4830 training loss: 21.044140338897705\n",
      "Epoch: 4840 training loss: 20.578720808029175\n",
      "Epoch: 4850 training loss: 20.881560564041138\n",
      "Epoch: 4860 training loss: 20.733468055725098\n",
      "Epoch: 4870 training loss: 20.9703848361969\n",
      "Epoch: 4880 training loss: 20.646724700927734\n",
      "Epoch: 4890 training loss: 20.711644172668457\n",
      "Epoch: 4900 training loss: 20.858187437057495\n",
      "Epoch: 4910 training loss: 20.666736364364624\n",
      "Epoch: 4920 training loss: 20.499635696411133\n",
      "Epoch: 4930 training loss: 20.88737201690674\n",
      "Epoch: 4940 training loss: 20.578951597213745\n",
      "Epoch: 4950 training loss: 20.76024842262268\n",
      "Epoch: 4960 training loss: 21.025761127471924\n",
      "Epoch: 4970 training loss: 20.76283860206604\n",
      "Epoch: 4980 training loss: 20.943050622940063\n",
      "Epoch: 4990 training loss: 20.849904537200928\n",
      "====================\n",
      "Training GraphSAGE with data_cnn\n",
      "Epoch: 0 training loss: 35.189297914505005\n",
      "Epoch: 10 training loss: 23.748029708862305\n",
      "Epoch: 20 training loss: 23.200645208358765\n",
      "Epoch: 30 training loss: 22.886302709579468\n",
      "Epoch: 40 training loss: 22.610138654708862\n",
      "Epoch: 50 training loss: 22.54480528831482\n",
      "Epoch: 60 training loss: 22.413257122039795\n",
      "Epoch: 70 training loss: 22.23423981666565\n",
      "Epoch: 80 training loss: 22.312416791915894\n",
      "Epoch: 90 training loss: 22.227274179458618\n",
      "Epoch: 100 training loss: 22.135026693344116\n",
      "Epoch: 110 training loss: 21.917958974838257\n",
      "Epoch: 120 training loss: 22.063910961151123\n",
      "Epoch: 130 training loss: 21.955780267715454\n",
      "Epoch: 140 training loss: 21.405670404434204\n",
      "Epoch: 150 training loss: 21.326629400253296\n",
      "Epoch: 160 training loss: 21.238266229629517\n",
      "Epoch: 170 training loss: 20.932918071746826\n",
      "Epoch: 180 training loss: 21.038041353225708\n",
      "Epoch: 190 training loss: 20.72478723526001\n",
      "Epoch: 200 training loss: 20.934117317199707\n",
      "Epoch: 210 training loss: 20.765775442123413\n",
      "Epoch: 220 training loss: 20.66564130783081\n",
      "Epoch: 230 training loss: 20.920018434524536\n",
      "Epoch: 240 training loss: 21.013444900512695\n",
      "Epoch: 250 training loss: 20.81214165687561\n",
      "Epoch: 260 training loss: 21.211981534957886\n",
      "Epoch: 270 training loss: 20.55310893058777\n",
      "Epoch: 280 training loss: 20.823164701461792\n",
      "Epoch: 290 training loss: 20.73091983795166\n",
      "Epoch: 300 training loss: 20.45441722869873\n",
      "Epoch: 310 training loss: 20.696364641189575\n",
      "Epoch: 320 training loss: 20.852611541748047\n",
      "Epoch: 330 training loss: 20.620224475860596\n",
      "Epoch: 340 training loss: 20.678672075271606\n",
      "Epoch: 350 training loss: 20.754197120666504\n",
      "Epoch: 360 training loss: 20.461098670959473\n",
      "Epoch: 370 training loss: 21.04521942138672\n",
      "Epoch: 380 training loss: 20.998682498931885\n",
      "Epoch: 390 training loss: 20.665853023529053\n",
      "Epoch: 400 training loss: 20.6942777633667\n",
      "Epoch: 410 training loss: 20.931234121322632\n",
      "Epoch: 420 training loss: 20.700297117233276\n",
      "Epoch: 430 training loss: 20.662260055541992\n",
      "Epoch: 440 training loss: 20.28983736038208\n",
      "Epoch: 450 training loss: 20.4244065284729\n",
      "Epoch: 460 training loss: 20.316728115081787\n",
      "Epoch: 470 training loss: 20.261592149734497\n",
      "Epoch: 480 training loss: 20.421333074569702\n",
      "Epoch: 490 training loss: 20.516260385513306\n",
      "Epoch: 500 training loss: 20.57582187652588\n",
      "Epoch: 510 training loss: 20.525074005126953\n",
      "Epoch: 520 training loss: 20.5844624042511\n",
      "Epoch: 530 training loss: 20.330764055252075\n",
      "Epoch: 540 training loss: 20.251272678375244\n",
      "Epoch: 550 training loss: 20.552926778793335\n",
      "Epoch: 560 training loss: 20.825910806655884\n",
      "Epoch: 570 training loss: 20.543691873550415\n",
      "Epoch: 580 training loss: 20.093809843063354\n",
      "Epoch: 590 training loss: 20.424464225769043\n",
      "Epoch: 600 training loss: 20.825414896011353\n",
      "Epoch: 610 training loss: 20.32234287261963\n",
      "Epoch: 620 training loss: 20.303569793701172\n",
      "Epoch: 630 training loss: 20.42465853691101\n",
      "Epoch: 640 training loss: 20.138282537460327\n",
      "Epoch: 650 training loss: 20.35966467857361\n",
      "Epoch: 660 training loss: 20.235594749450684\n",
      "Epoch: 670 training loss: 20.70190739631653\n",
      "Epoch: 680 training loss: 20.384820699691772\n",
      "Epoch: 690 training loss: 20.07213282585144\n",
      "Epoch: 700 training loss: 20.23829674720764\n",
      "Epoch: 710 training loss: 20.415632009506226\n",
      "Epoch: 720 training loss: 20.55423140525818\n",
      "Epoch: 730 training loss: 20.369488954544067\n",
      "Epoch: 740 training loss: 20.19619607925415\n",
      "Epoch: 750 training loss: 20.189168453216553\n",
      "Epoch: 760 training loss: 20.429565906524658\n",
      "Epoch: 770 training loss: 19.947483777999878\n",
      "Epoch: 780 training loss: 20.36513090133667\n",
      "Epoch: 790 training loss: 20.344751596450806\n",
      "Epoch: 800 training loss: 20.335610151290894\n",
      "Epoch: 810 training loss: 20.374448776245117\n",
      "Epoch: 820 training loss: 20.190381050109863\n",
      "Epoch: 830 training loss: 20.227476119995117\n",
      "Epoch: 840 training loss: 20.23124361038208\n",
      "Epoch: 850 training loss: 20.664340257644653\n",
      "Epoch: 860 training loss: 20.13315439224243\n",
      "Epoch: 870 training loss: 20.46268129348755\n",
      "Epoch: 880 training loss: 20.138447999954224\n",
      "Epoch: 890 training loss: 20.423094749450684\n",
      "Epoch: 900 training loss: 20.252814769744873\n",
      "Epoch: 910 training loss: 20.02494168281555\n",
      "Epoch: 920 training loss: 20.35054612159729\n",
      "Epoch: 930 training loss: 20.026387691497803\n",
      "Epoch: 940 training loss: 20.27637791633606\n",
      "Epoch: 950 training loss: 20.420559406280518\n",
      "Epoch: 960 training loss: 20.09749960899353\n",
      "Epoch: 970 training loss: 20.11500644683838\n",
      "Epoch: 980 training loss: 20.42261242866516\n",
      "Epoch: 990 training loss: 20.151017904281616\n",
      "Epoch: 1000 training loss: 20.210857391357422\n",
      "Epoch: 1010 training loss: 20.020127773284912\n",
      "Epoch: 1020 training loss: 20.23499631881714\n",
      "Epoch: 1030 training loss: 20.130411386489868\n",
      "Epoch: 1040 training loss: 20.077170610427856\n",
      "Epoch: 1050 training loss: 20.148380994796753\n",
      "Epoch: 1060 training loss: 20.188265085220337\n",
      "Epoch: 1070 training loss: 20.07094168663025\n",
      "Epoch: 1080 training loss: 20.304044723510742\n",
      "Epoch: 1090 training loss: 20.093344688415527\n",
      "Epoch: 1100 training loss: 20.316617488861084\n",
      "Epoch: 1110 training loss: 20.37560796737671\n",
      "Epoch: 1120 training loss: 20.493174076080322\n",
      "Epoch: 1130 training loss: 20.665825605392456\n",
      "Epoch: 1140 training loss: 20.14596700668335\n",
      "Epoch: 1150 training loss: 20.269465923309326\n",
      "Epoch: 1160 training loss: 19.906513214111328\n",
      "Epoch: 1170 training loss: 20.42139458656311\n",
      "Epoch: 1180 training loss: 20.398513555526733\n",
      "Epoch: 1190 training loss: 19.888408660888672\n",
      "Epoch: 1200 training loss: 20.186952352523804\n",
      "Epoch: 1210 training loss: 20.281658411026\n",
      "Epoch: 1220 training loss: 20.296929836273193\n",
      "Epoch: 1230 training loss: 20.54614543914795\n",
      "Epoch: 1240 training loss: 20.143382787704468\n",
      "Epoch: 1250 training loss: 20.4766788482666\n",
      "Epoch: 1260 training loss: 20.155580282211304\n",
      "Epoch: 1270 training loss: 20.04969310760498\n",
      "Epoch: 1280 training loss: 20.73842430114746\n",
      "Epoch: 1290 training loss: 19.942986965179443\n",
      "Epoch: 1300 training loss: 20.191343545913696\n",
      "Epoch: 1310 training loss: 20.143869638442993\n",
      "Epoch: 1320 training loss: 20.000431537628174\n",
      "Epoch: 1330 training loss: 19.921642541885376\n",
      "Epoch: 1340 training loss: 19.90671181678772\n",
      "Epoch: 1350 training loss: 19.868906021118164\n",
      "Epoch: 1360 training loss: 19.835708379745483\n",
      "Epoch: 1370 training loss: 19.910372495651245\n",
      "Epoch: 1380 training loss: 19.68622136116028\n",
      "Epoch: 1390 training loss: 19.645670175552368\n",
      "Epoch: 1400 training loss: 19.8264799118042\n",
      "Epoch: 1410 training loss: 19.94897484779358\n",
      "Epoch: 1420 training loss: 19.721741199493408\n",
      "Epoch: 1430 training loss: 19.28297472000122\n",
      "Epoch: 1440 training loss: 19.001545429229736\n",
      "Epoch: 1450 training loss: 19.319108963012695\n",
      "Epoch: 1460 training loss: 19.067549228668213\n",
      "Epoch: 1470 training loss: 18.897074460983276\n",
      "Epoch: 1480 training loss: 18.817585706710815\n",
      "Epoch: 1490 training loss: 18.959704875946045\n",
      "Epoch: 1500 training loss: 18.03327488899231\n",
      "Epoch: 1510 training loss: 17.78110146522522\n",
      "Epoch: 1520 training loss: 17.252248764038086\n",
      "Epoch: 1530 training loss: 17.777672052383423\n",
      "Epoch: 1540 training loss: 17.54916501045227\n",
      "Epoch: 1550 training loss: 17.225996017456055\n",
      "Epoch: 1560 training loss: 17.11937427520752\n",
      "Epoch: 1570 training loss: 17.426753520965576\n",
      "Epoch: 1580 training loss: 17.06355047225952\n",
      "Epoch: 1590 training loss: 17.00122880935669\n",
      "Epoch: 1600 training loss: 17.106089115142822\n",
      "Epoch: 1610 training loss: 16.71650469303131\n",
      "Epoch: 1620 training loss: 17.29013752937317\n",
      "Epoch: 1630 training loss: 17.382275819778442\n",
      "Epoch: 1640 training loss: 17.156038284301758\n",
      "Epoch: 1650 training loss: 16.93605887889862\n",
      "Epoch: 1660 training loss: 17.056325435638428\n",
      "Epoch: 1670 training loss: 17.05281662940979\n",
      "Epoch: 1680 training loss: 17.128206729888916\n",
      "Epoch: 1690 training loss: 17.60041904449463\n",
      "Epoch: 1700 training loss: 17.052481651306152\n",
      "Epoch: 1710 training loss: 16.892131805419922\n",
      "Epoch: 1720 training loss: 16.921353578567505\n",
      "Epoch: 1730 training loss: 16.52437925338745\n",
      "Epoch: 1740 training loss: 16.695375561714172\n",
      "Epoch: 1750 training loss: 17.37027072906494\n",
      "Epoch: 1760 training loss: 16.579518914222717\n",
      "Epoch: 1770 training loss: 16.45953130722046\n",
      "Epoch: 1780 training loss: 16.571937918663025\n",
      "Epoch: 1790 training loss: 16.38937485218048\n",
      "Epoch: 1800 training loss: 16.743902802467346\n",
      "Epoch: 1810 training loss: 16.503251910209656\n",
      "Epoch: 1820 training loss: 16.67050862312317\n",
      "Epoch: 1830 training loss: 16.783066630363464\n",
      "Epoch: 1840 training loss: 16.50277006626129\n",
      "Epoch: 1850 training loss: 16.689674377441406\n",
      "Epoch: 1860 training loss: 16.966901063919067\n",
      "Epoch: 1870 training loss: 16.8826162815094\n",
      "Epoch: 1880 training loss: 16.840453505516052\n",
      "Epoch: 1890 training loss: 16.548231840133667\n",
      "Epoch: 1900 training loss: 16.689027667045593\n",
      "Epoch: 1910 training loss: 16.9977525472641\n",
      "Epoch: 1920 training loss: 16.599785685539246\n",
      "Epoch: 1930 training loss: 16.559865951538086\n",
      "Epoch: 1940 training loss: 16.81221854686737\n",
      "Epoch: 1950 training loss: 17.116835951805115\n",
      "Epoch: 1960 training loss: 16.350951671600342\n",
      "Epoch: 1970 training loss: 16.529520988464355\n",
      "Epoch: 1980 training loss: 16.806275844573975\n",
      "Epoch: 1990 training loss: 16.47975468635559\n",
      "Epoch: 2000 training loss: 16.765140771865845\n",
      "Epoch: 2010 training loss: 16.678725957870483\n",
      "Epoch: 2020 training loss: 16.71632742881775\n",
      "Epoch: 2030 training loss: 16.53126072883606\n",
      "Epoch: 2040 training loss: 16.271982073783875\n",
      "Epoch: 2050 training loss: 16.88166058063507\n",
      "Epoch: 2060 training loss: 16.47676980495453\n",
      "Epoch: 2070 training loss: 16.882461071014404\n",
      "Epoch: 2080 training loss: 16.68904459476471\n",
      "Epoch: 2090 training loss: 16.794947147369385\n",
      "Epoch: 2100 training loss: 16.491230487823486\n",
      "Epoch: 2110 training loss: 16.198760747909546\n",
      "Epoch: 2120 training loss: 16.73586916923523\n",
      "Epoch: 2130 training loss: 16.56791114807129\n",
      "Epoch: 2140 training loss: 16.649394035339355\n",
      "Epoch: 2150 training loss: 16.61450171470642\n",
      "Epoch: 2160 training loss: 16.446143627166748\n",
      "Epoch: 2170 training loss: 16.590375423431396\n",
      "Epoch: 2180 training loss: 16.87391757965088\n",
      "Epoch: 2190 training loss: 16.53193759918213\n",
      "Epoch: 2200 training loss: 16.227976083755493\n",
      "Epoch: 2210 training loss: 16.96963906288147\n",
      "Epoch: 2220 training loss: 16.496490001678467\n",
      "Epoch: 2230 training loss: 16.26481866836548\n",
      "Epoch: 2240 training loss: 16.601885676383972\n",
      "Epoch: 2250 training loss: 16.558634757995605\n",
      "Epoch: 2260 training loss: 16.26969611644745\n",
      "Epoch: 2270 training loss: 16.723702430725098\n",
      "Epoch: 2280 training loss: 16.89337694644928\n",
      "Epoch: 2290 training loss: 16.11076831817627\n",
      "Epoch: 2300 training loss: 16.830098390579224\n",
      "Epoch: 2310 training loss: 16.639625668525696\n",
      "Epoch: 2320 training loss: 16.431777834892273\n",
      "Epoch: 2330 training loss: 16.302334547042847\n",
      "Epoch: 2340 training loss: 16.58200490474701\n",
      "Epoch: 2350 training loss: 16.38936424255371\n",
      "Epoch: 2360 training loss: 16.985967755317688\n",
      "Epoch: 2370 training loss: 16.56961238384247\n",
      "Epoch: 2380 training loss: 16.320280075073242\n",
      "Epoch: 2390 training loss: 16.39569914340973\n",
      "Epoch: 2400 training loss: 16.31653356552124\n",
      "Epoch: 2410 training loss: 16.54448115825653\n",
      "Epoch: 2420 training loss: 16.59910559654236\n",
      "Epoch: 2430 training loss: 16.80217695236206\n",
      "Epoch: 2440 training loss: 15.320310115814209\n",
      "Epoch: 2450 training loss: 15.740371346473694\n",
      "Epoch: 2460 training loss: 15.315955996513367\n",
      "Epoch: 2470 training loss: 15.90872347354889\n",
      "Epoch: 2480 training loss: 15.617363572120667\n",
      "Epoch: 2490 training loss: 15.408123135566711\n",
      "Epoch: 2500 training loss: 15.033442497253418\n",
      "Epoch: 2510 training loss: 15.202217102050781\n",
      "Epoch: 2520 training loss: 15.244616627693176\n",
      "Epoch: 2530 training loss: 15.03920865058899\n",
      "Epoch: 2540 training loss: 15.259320497512817\n",
      "Epoch: 2550 training loss: 15.356124997138977\n",
      "Epoch: 2560 training loss: 14.951491475105286\n",
      "Epoch: 2570 training loss: 14.907793045043945\n",
      "Epoch: 2580 training loss: 15.726593375205994\n",
      "Epoch: 2590 training loss: 14.824385285377502\n",
      "Epoch: 2600 training loss: 15.271802067756653\n",
      "Epoch: 2610 training loss: 15.162148714065552\n",
      "Epoch: 2620 training loss: 14.924504160881042\n",
      "Epoch: 2630 training loss: 15.213814377784729\n",
      "Epoch: 2640 training loss: 15.298777461051941\n",
      "Epoch: 2650 training loss: 15.628498911857605\n",
      "Epoch: 2660 training loss: 14.9922194480896\n",
      "Epoch: 2670 training loss: 14.900214672088623\n",
      "Epoch: 2680 training loss: 14.913439869880676\n",
      "Epoch: 2690 training loss: 14.990965366363525\n",
      "Epoch: 2700 training loss: 15.28042995929718\n",
      "Epoch: 2710 training loss: 14.910728693008423\n",
      "Epoch: 2720 training loss: 15.248666524887085\n",
      "Epoch: 2730 training loss: 15.26197636127472\n",
      "Epoch: 2740 training loss: 15.215431928634644\n",
      "Epoch: 2750 training loss: 15.01806926727295\n",
      "Epoch: 2760 training loss: 14.785123467445374\n",
      "Epoch: 2770 training loss: 15.735904574394226\n",
      "Epoch: 2780 training loss: 14.86125898361206\n",
      "Epoch: 2790 training loss: 14.782871961593628\n",
      "Epoch: 2800 training loss: 14.87021291255951\n",
      "Epoch: 2810 training loss: 15.101840138435364\n",
      "Epoch: 2820 training loss: 14.739688038825989\n",
      "Epoch: 2830 training loss: 15.198107600212097\n",
      "Epoch: 2840 training loss: 14.992544054985046\n",
      "Epoch: 2850 training loss: 14.873890161514282\n",
      "Epoch: 2860 training loss: 15.178579926490784\n",
      "Epoch: 2870 training loss: 14.871972918510437\n",
      "Epoch: 2880 training loss: 15.069766521453857\n",
      "Epoch: 2890 training loss: 14.572879552841187\n",
      "Epoch: 2900 training loss: 15.342479467391968\n",
      "Epoch: 2910 training loss: 14.756894588470459\n",
      "Epoch: 2920 training loss: 14.810887455940247\n",
      "Epoch: 2930 training loss: 15.241892457008362\n",
      "Epoch: 2940 training loss: 14.761164903640747\n",
      "Epoch: 2950 training loss: 15.040439486503601\n",
      "Epoch: 2960 training loss: 14.968652248382568\n",
      "Epoch: 2970 training loss: 14.65552544593811\n",
      "Epoch: 2980 training loss: 14.50177538394928\n",
      "Epoch: 2990 training loss: 14.74940049648285\n",
      "Epoch: 3000 training loss: 15.176383376121521\n",
      "Epoch: 3010 training loss: 14.808582067489624\n",
      "Epoch: 3020 training loss: 14.866654992103577\n",
      "Epoch: 3030 training loss: 14.639854431152344\n",
      "Epoch: 3040 training loss: 14.761816620826721\n",
      "Epoch: 3050 training loss: 15.433629512786865\n",
      "Epoch: 3060 training loss: 14.724197387695312\n",
      "Epoch: 3070 training loss: 14.79317021369934\n",
      "Epoch: 3080 training loss: 15.023512363433838\n",
      "Epoch: 3090 training loss: 15.160983443260193\n",
      "Epoch: 3100 training loss: 14.955136060714722\n",
      "Epoch: 3110 training loss: 14.459813952445984\n",
      "Epoch: 3120 training loss: 15.274569153785706\n",
      "Epoch: 3130 training loss: 14.396968007087708\n",
      "Epoch: 3140 training loss: 14.911920428276062\n",
      "Epoch: 3150 training loss: 14.613417029380798\n",
      "Epoch: 3160 training loss: 15.366137266159058\n",
      "Epoch: 3170 training loss: 14.698458909988403\n",
      "Epoch: 3180 training loss: 14.74700653553009\n",
      "Epoch: 3190 training loss: 14.806059718132019\n",
      "Epoch: 3200 training loss: 14.342132210731506\n",
      "Epoch: 3210 training loss: 15.115520715713501\n",
      "Epoch: 3220 training loss: 15.012654185295105\n",
      "Epoch: 3230 training loss: 14.601531386375427\n",
      "Epoch: 3240 training loss: 14.883939743041992\n",
      "Epoch: 3250 training loss: 14.803388357162476\n",
      "Epoch: 3260 training loss: 15.014906167984009\n",
      "Epoch: 3270 training loss: 14.31492292881012\n",
      "Epoch: 3280 training loss: 14.822152137756348\n",
      "Epoch: 3290 training loss: 14.521496772766113\n",
      "Epoch: 3300 training loss: 14.768116354942322\n",
      "Epoch: 3310 training loss: 14.805469036102295\n",
      "Epoch: 3320 training loss: 14.821494936943054\n",
      "Epoch: 3330 training loss: 14.77889084815979\n",
      "Epoch: 3340 training loss: 14.815659165382385\n",
      "Epoch: 3350 training loss: 14.884270787239075\n",
      "Epoch: 3360 training loss: 14.631425142288208\n",
      "Epoch: 3370 training loss: 14.35041880607605\n",
      "Epoch: 3380 training loss: 14.860169410705566\n",
      "Epoch: 3390 training loss: 14.938665866851807\n",
      "Epoch: 3400 training loss: 14.852492332458496\n",
      "Epoch: 3410 training loss: 14.386738657951355\n",
      "Epoch: 3420 training loss: 14.287034273147583\n",
      "Epoch: 3430 training loss: 14.957204341888428\n",
      "Epoch: 3440 training loss: 14.546372890472412\n",
      "Epoch: 3450 training loss: 14.592029929161072\n",
      "Epoch: 3460 training loss: 15.152758359909058\n",
      "Epoch: 3470 training loss: 14.66063380241394\n",
      "Epoch: 3480 training loss: 14.812295079231262\n",
      "Epoch: 3490 training loss: 14.41732931137085\n",
      "Epoch: 3500 training loss: 14.696241617202759\n",
      "Epoch: 3510 training loss: 14.6998872756958\n",
      "Epoch: 3520 training loss: 14.37450361251831\n",
      "Epoch: 3530 training loss: 15.015226483345032\n",
      "Epoch: 3540 training loss: 14.769588112831116\n",
      "Epoch: 3550 training loss: 14.579535603523254\n",
      "Epoch: 3560 training loss: 14.393646240234375\n",
      "Epoch: 3570 training loss: 14.799450993537903\n",
      "Epoch: 3580 training loss: 14.7996746301651\n",
      "Epoch: 3590 training loss: 15.285930871963501\n",
      "Epoch: 3600 training loss: 14.642152905464172\n",
      "Epoch: 3610 training loss: 14.65279233455658\n",
      "Epoch: 3620 training loss: 14.629065990447998\n",
      "Epoch: 3630 training loss: 14.435278296470642\n",
      "Epoch: 3640 training loss: 15.124246716499329\n",
      "Epoch: 3650 training loss: 14.993726134300232\n",
      "Epoch: 3660 training loss: 14.628626108169556\n",
      "Epoch: 3670 training loss: 14.554204106330872\n",
      "Epoch: 3680 training loss: 14.87251341342926\n",
      "Epoch: 3690 training loss: 14.661707639694214\n",
      "Epoch: 3700 training loss: 14.61506175994873\n",
      "Epoch: 3710 training loss: 14.573355197906494\n",
      "Epoch: 3720 training loss: 14.820712089538574\n",
      "Epoch: 3730 training loss: 14.754937887191772\n",
      "Epoch: 3740 training loss: 15.193798899650574\n",
      "Epoch: 3750 training loss: 14.837745308876038\n",
      "Epoch: 3760 training loss: 14.680055618286133\n",
      "Epoch: 3770 training loss: 14.90318214893341\n",
      "Epoch: 3780 training loss: 14.575903415679932\n",
      "Epoch: 3790 training loss: 14.539509177207947\n",
      "Epoch: 3800 training loss: 14.37368106842041\n",
      "Epoch: 3810 training loss: 14.962807536125183\n",
      "Epoch: 3820 training loss: 14.966343879699707\n",
      "Epoch: 3830 training loss: 15.046398162841797\n",
      "Epoch: 3840 training loss: 14.785406589508057\n",
      "Epoch: 3850 training loss: 14.918396949768066\n",
      "Epoch: 3860 training loss: 14.6410551071167\n",
      "Epoch: 3870 training loss: 14.872393488883972\n",
      "Epoch: 3880 training loss: 14.527114629745483\n",
      "Epoch: 3890 training loss: 14.450329899787903\n",
      "Epoch: 3900 training loss: 14.731236815452576\n",
      "Epoch: 3910 training loss: 14.993401885032654\n",
      "Epoch: 3920 training loss: 14.655982851982117\n",
      "Epoch: 3930 training loss: 14.737163782119751\n",
      "Epoch: 3940 training loss: 14.824050545692444\n",
      "Epoch: 3950 training loss: 14.58150851726532\n",
      "Epoch: 3960 training loss: 14.311886668205261\n",
      "Epoch: 3970 training loss: 15.093799829483032\n",
      "Epoch: 3980 training loss: 14.397660970687866\n",
      "Epoch: 3990 training loss: 14.507640719413757\n",
      "Epoch: 4000 training loss: 14.66901707649231\n",
      "Epoch: 4010 training loss: 14.881327629089355\n",
      "Epoch: 4020 training loss: 15.006669163703918\n",
      "Epoch: 4030 training loss: 14.85228979587555\n",
      "Epoch: 4040 training loss: 14.812346458435059\n",
      "Epoch: 4050 training loss: 14.632102251052856\n",
      "Epoch: 4060 training loss: 14.312989950180054\n",
      "Epoch: 4070 training loss: 14.998827457427979\n",
      "Epoch: 4080 training loss: 14.588306784629822\n",
      "Epoch: 4090 training loss: 14.643752098083496\n",
      "Epoch: 4100 training loss: 14.89868414402008\n",
      "Epoch: 4110 training loss: 14.57826840877533\n",
      "Epoch: 4120 training loss: 14.364358067512512\n",
      "Epoch: 4130 training loss: 14.103358030319214\n",
      "Epoch: 4140 training loss: 15.177848815917969\n",
      "Epoch: 4150 training loss: 14.803192734718323\n",
      "Epoch: 4160 training loss: 14.393401861190796\n",
      "Epoch: 4170 training loss: 14.633758544921875\n",
      "Epoch: 4180 training loss: 14.972518801689148\n",
      "Epoch: 4190 training loss: 14.687824845314026\n",
      "Epoch: 4200 training loss: 14.810948610305786\n",
      "Epoch: 4210 training loss: 14.8094322681427\n",
      "Epoch: 4220 training loss: 14.430898427963257\n",
      "Epoch: 4230 training loss: 14.542286992073059\n",
      "Epoch: 4240 training loss: 14.518089890480042\n",
      "Epoch: 4250 training loss: 14.732804894447327\n",
      "Epoch: 4260 training loss: 14.766561031341553\n",
      "Epoch: 4270 training loss: 14.66440749168396\n",
      "Epoch: 4280 training loss: 15.008601546287537\n",
      "Epoch: 4290 training loss: 14.777243256568909\n",
      "Epoch: 4300 training loss: 14.39204490184784\n",
      "Epoch: 4310 training loss: 14.991373777389526\n",
      "Epoch: 4320 training loss: 14.566212773323059\n",
      "Epoch: 4330 training loss: 14.759758830070496\n",
      "Epoch: 4340 training loss: 14.776072144508362\n",
      "Epoch: 4350 training loss: 14.831724524497986\n",
      "Epoch: 4360 training loss: 15.092458963394165\n",
      "Epoch: 4370 training loss: 14.388593792915344\n",
      "Epoch: 4380 training loss: 14.99741506576538\n",
      "Epoch: 4390 training loss: 14.69955325126648\n",
      "Epoch: 4400 training loss: 14.620806694030762\n",
      "Epoch: 4410 training loss: 14.503294348716736\n",
      "Epoch: 4420 training loss: 14.398727297782898\n",
      "Epoch: 4430 training loss: 14.4841810464859\n",
      "Epoch: 4440 training loss: 14.970841526985168\n",
      "Epoch: 4450 training loss: 14.723661065101624\n",
      "Epoch: 4460 training loss: 15.134690284729004\n",
      "Epoch: 4470 training loss: 14.625337719917297\n",
      "Epoch: 4480 training loss: 14.333326935768127\n",
      "Epoch: 4490 training loss: 14.31130301952362\n",
      "Epoch: 4500 training loss: 14.44521701335907\n",
      "Epoch: 4510 training loss: 14.173005104064941\n",
      "Epoch: 4520 training loss: 14.710290312767029\n",
      "Epoch: 4530 training loss: 14.9548898935318\n",
      "Epoch: 4540 training loss: 14.769790768623352\n",
      "Epoch: 4550 training loss: 14.717585444450378\n",
      "Epoch: 4560 training loss: 14.781485557556152\n",
      "Epoch: 4570 training loss: 14.603750228881836\n",
      "Epoch: 4580 training loss: 14.478540301322937\n",
      "Epoch: 4590 training loss: 14.560490250587463\n",
      "Epoch: 4600 training loss: 14.898330926895142\n",
      "Epoch: 4610 training loss: 15.153867363929749\n",
      "Epoch: 4620 training loss: 14.514689564704895\n",
      "Epoch: 4630 training loss: 14.614547848701477\n",
      "Epoch: 4640 training loss: 14.53254771232605\n",
      "Epoch: 4650 training loss: 14.699679136276245\n",
      "Epoch: 4660 training loss: 14.805032968521118\n",
      "Epoch: 4670 training loss: 14.803156733512878\n",
      "Epoch: 4680 training loss: 14.639834880828857\n",
      "Epoch: 4690 training loss: 14.482533931732178\n",
      "Epoch: 4700 training loss: 14.41389274597168\n",
      "Epoch: 4710 training loss: 14.630167007446289\n",
      "Epoch: 4720 training loss: 14.397393226623535\n",
      "Epoch: 4730 training loss: 14.7370765209198\n",
      "Epoch: 4740 training loss: 15.000162839889526\n",
      "Epoch: 4750 training loss: 14.975711822509766\n",
      "Epoch: 4760 training loss: 14.605491399765015\n",
      "Epoch: 4770 training loss: 14.429591178894043\n",
      "Epoch: 4780 training loss: 14.641199469566345\n",
      "Epoch: 4790 training loss: 14.929683566093445\n",
      "Epoch: 4800 training loss: 14.942182540893555\n",
      "Epoch: 4810 training loss: 14.420622944831848\n",
      "Epoch: 4820 training loss: 14.928233027458191\n",
      "Epoch: 4830 training loss: 15.390856981277466\n",
      "Epoch: 4840 training loss: 14.82785165309906\n",
      "Epoch: 4850 training loss: 14.76626205444336\n",
      "Epoch: 4860 training loss: 14.9253591299057\n",
      "Epoch: 4870 training loss: 14.639100670814514\n",
      "Epoch: 4880 training loss: 14.497228503227234\n",
      "Epoch: 4890 training loss: 14.702786922454834\n",
      "Epoch: 4900 training loss: 14.831056475639343\n",
      "Epoch: 4910 training loss: 14.475043654441833\n",
      "Epoch: 4920 training loss: 14.619826674461365\n",
      "Epoch: 4930 training loss: 14.461095929145813\n",
      "Epoch: 4940 training loss: 14.512287974357605\n",
      "Epoch: 4950 training loss: 14.741678357124329\n",
      "Epoch: 4960 training loss: 14.560144662857056\n",
      "Epoch: 4970 training loss: 14.534671545028687\n",
      "Epoch: 4980 training loss: 14.539220571517944\n",
      "Epoch: 4990 training loss: 14.675605416297913\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# sage = GraphSAGE(nfeat=data.x.shape[1], nhid=16, nclass=nclass)\n",
    "# train(sage, train_loader, epochs=100, device='cuda')\n",
    "\n",
    "sage_models = {}\n",
    "for name, data in dataset.items():\n",
    "    print(f\"Training GraphSAGE with {name}\")\n",
    "\n",
    "    sage = GraphSAGE(nfeat=data.x.shape[1], nhid=16, nclass=nclass)\n",
    "    train(sage, data, train_loader, lr=0.001, epochs=5000, device=device)\n",
    "    sage_models[name] = sage\n",
    "\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GraphSAGE with data_clip\n",
      "Test set results: loss= 2.7766 accuracy= 0.0855\n",
      "====================\n",
      "Testing GraphSAGE with data_cnn\n",
      "Test set results: loss= 4.4386 accuracy= 0.1391\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for name, model in sage_models.items():\n",
    "    print(f\"Testing GraphSAGE with {name}\")\n",
    "    data = dataset[name]\n",
    "\n",
    "    test(model, data)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unsupervised Graph Representation Learning with GNNs (35 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Link Prediction Setup (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricercar/miniconda3/envs/gnn/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Your answer\n",
    "import copy\n",
    "from torch_geometric.utils import negative_sampling, train_test_split_edges\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "data = copy.deepcopy(dataset[\"data_clip\"])  # I use clip emb as node features here\n",
    "data = train_test_split_edges(data, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Train a GCN Encoder for Link Prediction (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the generic link prediction model\n",
    "class LinkPredModel(nn.Module):\n",
    "    def __init__(self, encoder, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = nn.Linear(2*hidden_channels, 1)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        num_nodes = x.shape[0]\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes)\n",
    "        adj_norm = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "        z = self.encoder(x, adj_norm)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, edge_index):  # just a binary classification\n",
    "        row, col = edge_index\n",
    "        z = self.decoder(torch.cat([z[row], z[col]], dim=-1))\n",
    "        output = F.sigmoid(z)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "\n",
    "        # sample negative edges\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=edge_index,\n",
    "            num_nodes=x.size(0),\n",
    "            num_neg_samples=edge_index.size(1)\n",
    "        )\n",
    "\n",
    "        pos_out = self.decode(z, edge_index)\n",
    "        neg_out = self.decode(z, neg_edge_index)\n",
    "\n",
    "        return pos_out, neg_out \n",
    "\n",
    "    def predict(self, x, edge_index, pred_edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        output = self.decode(z, pred_edge_index)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write genertic training and testing functions\n",
    "def train(model, data, lr=0.01, weight_decay=5e-4, epochs=200, patience=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_log = []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pos_out, neg_out = model(data.x, data.train_pos_edge_index)\n",
    "\n",
    "        # calculate BCE losses\n",
    "        criterion = nn.BCELoss()\n",
    "        pos_loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "        neg_loss = criterion(neg_out, torch.zeros_like(neg_out))\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_log.append(loss.item())\n",
    "\n",
    "        # evaluate train and val acc\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc_train = acc_score(pos_out, neg_out)\n",
    "        \n",
    "        acc_val = validation(model, data)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {i}, training loss: {loss.item()}, train acc: {acc_train}, val acc: {acc_val}\")\n",
    "\n",
    "\n",
    "        # early stopping\n",
    "        if acc_val > best_val_acc:\n",
    "            best_val_acc = acc_val\n",
    "            best_val_epoch = i\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {best_val_epoch}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Best validation accuracy: {best_val_acc:4f}\")\n",
    "    \n",
    "    if best_model_state is not None: \n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_log\n",
    "\n",
    "\n",
    "def acc_score(pos_out, neg_out):\n",
    "    pos_pred = (pos_out > 0.5).float()\n",
    "    neg_pred = (neg_out > 0.5).float()\n",
    "\n",
    "    preds = torch.cat([pos_pred, neg_pred])\n",
    "    labels = torch.cat([torch.ones_like(pos_pred), torch.zeros_like(neg_pred)])\n",
    "    acc_val = preds.eq(labels).cpu().numpy().mean()\n",
    "\n",
    "    return acc_val.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(model, data):\n",
    "    model.eval()\n",
    "    pos_out, neg_out = model(data.x, data.val_pos_edge_index)\n",
    "\n",
    "    # criterion = nn.BCELoss()\n",
    "    # pos_loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "    # neg_loss = criterion(neg_out, torch.zeros_like(neg_out))\n",
    "    # loss = pos_loss + neg_loss\n",
    "\n",
    "    acc_val = acc_score(pos_out, neg_out)\n",
    "    return acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_training_curve(log, title):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(log)\n",
    "    plt.xlabel=(\"Epoch\")\n",
    "    plt.ylabel=(\"Loss\")\n",
    "    plt.title(f\"Training Loss Curve for {title}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(GCN):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        h = F.relu(self.gc1(x, adj))\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        z = self.gc2(h, adj)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for GCN\n",
      "Epoch 0, training loss: 8.761871337890625, train acc: 0.43904736275565126, val acc: 0.5615577889447236\n",
      "Epoch 10, training loss: 3.0537045001983643, train acc: 0.6068353067814855, val acc: 0.5756909547738693\n",
      "Epoch 20, training loss: 1.670668601989746, train acc: 0.5834230355220668, val acc: 0.4996859296482412\n",
      "Epoch 30, training loss: 1.3967301845550537, train acc: 0.6520452099031216, val acc: 0.5241834170854272\n",
      "Epoch 40, training loss: 1.2589995861053467, train acc: 0.6729458198780051, val acc: 0.5012562814070352\n",
      "Epoch 50, training loss: 1.2128182649612427, train acc: 0.6786419088625762, val acc: 0.5012562814070352\n",
      "Epoch 60, training loss: 1.2209107875823975, train acc: 0.6794940796555436, val acc: 0.5012562814070352\n",
      "Epoch 70, training loss: 1.2130684852600098, train acc: 0.684562253318981, val acc: 0.5012562814070352\n",
      "Epoch 80, training loss: 1.2048789262771606, train acc: 0.6916935773232867, val acc: 0.5006281407035176\n",
      "Epoch 90, training loss: 1.1990054845809937, train acc: 0.697524219590958, val acc: 0.5006281407035176\n",
      "Epoch 100, training loss: 1.1891573667526245, train acc: 0.7051489056332975, val acc: 0.5\n",
      "Epoch 110, training loss: 1.1859227418899536, train acc: 0.700843200574094, val acc: 0.5\n",
      "Epoch 120, training loss: 1.1802066564559937, train acc: 0.7018747757445282, val acc: 0.5\n",
      "Epoch 130, training loss: 1.1725674867630005, train acc: 0.7090509508432006, val acc: 0.5\n",
      "Epoch 140, training loss: 1.168046474456787, train acc: 0.7087369931826336, val acc: 0.5\n",
      "Epoch 150, training loss: 1.173546314239502, train acc: 0.7126838894869035, val acc: 0.5\n",
      "Epoch 160, training loss: 1.173437237739563, train acc: 0.7154198062432723, val acc: 0.5\n",
      "Epoch 170, training loss: 1.1657021045684814, train acc: 0.7084230355220668, val acc: 0.5\n",
      "Epoch 180, training loss: 1.1615118980407715, train acc: 0.714119124506638, val acc: 0.5\n",
      "Epoch 190, training loss: 1.1648287773132324, train acc: 0.7066738428417654, val acc: 0.5\n",
      "Epoch 200, training loss: 1.1548019647598267, train acc: 0.7118317186939361, val acc: 0.5\n",
      "Epoch 210, training loss: 1.151772141456604, train acc: 0.7127735916756369, val acc: 0.5\n",
      "Epoch 220, training loss: 1.1357736587524414, train acc: 0.7179763186221744, val acc: 0.5\n",
      "Epoch 230, training loss: 1.1388161182403564, train acc: 0.7179314675278077, val acc: 0.5\n",
      "Epoch 240, training loss: 1.141082763671875, train acc: 0.7260046645138142, val acc: 0.5\n",
      "Epoch 250, training loss: 1.131554126739502, train acc: 0.7222820236813778, val acc: 0.5\n",
      "Epoch 260, training loss: 1.1411855220794678, train acc: 0.7177072120559742, val acc: 0.5\n",
      "Epoch 270, training loss: 1.1392817497253418, train acc: 0.7194115536419089, val acc: 0.5\n",
      "Epoch 280, training loss: 1.1156750917434692, train acc: 0.721654108360244, val acc: 0.5\n",
      "Epoch 290, training loss: 1.1104421615600586, train acc: 0.7267222820236814, val acc: 0.5\n",
      "Epoch 300, training loss: 1.1198463439941406, train acc: 0.72089163975601, val acc: 0.5\n",
      "Epoch 310, training loss: 1.1106599569320679, train acc: 0.7205776820954432, val acc: 0.5\n",
      "Epoch 320, training loss: 1.107795000076294, train acc: 0.7251973448152135, val acc: 0.5\n",
      "Epoch 330, training loss: 1.11505126953125, train acc: 0.7210261930391102, val acc: 0.5\n",
      "Epoch 340, training loss: 1.1033034324645996, train acc: 0.724659131682813, val acc: 0.5\n",
      "Epoch 350, training loss: 1.1033252477645874, train acc: 0.7269016864011482, val acc: 0.5\n",
      "Epoch 360, training loss: 1.1003456115722656, train acc: 0.7273053462504485, val acc: 0.5\n",
      "Epoch 370, training loss: 1.0978529453277588, train acc: 0.7238518119842124, val acc: 0.5\n",
      "Epoch 380, training loss: 1.1228783130645752, train acc: 0.7276193039110155, val acc: 0.5\n",
      "Epoch 390, training loss: 1.1029874086380005, train acc: 0.7299964119124507, val acc: 0.5\n",
      "Epoch 400, training loss: 1.08876371383667, train acc: 0.730848582705418, val acc: 0.5\n",
      "Epoch 410, training loss: 1.07928466796875, train acc: 0.7359616074632221, val acc: 0.5\n",
      "Epoch 420, training loss: 1.0900728702545166, train acc: 0.7304449228561177, val acc: 0.5\n",
      "Epoch 430, training loss: 1.0803322792053223, train acc: 0.732059562253319, val acc: 0.5003140703517588\n",
      "Epoch 440, training loss: 1.0770237445831299, train acc: 0.736589522784356, val acc: 0.5003140703517588\n",
      "Epoch 450, training loss: 1.0662086009979248, train acc: 0.7408055256548259, val acc: 0.5003140703517588\n",
      "Epoch 460, training loss: 1.0540714263916016, train acc: 0.7442590599210621, val acc: 0.5003140703517588\n",
      "Epoch 470, training loss: 1.0599970817565918, train acc: 0.7460979547900969, val acc: 0.5021984924623115\n",
      "Epoch 480, training loss: 1.0498806238174438, train acc: 0.748340509508432, val acc: 0.5034547738693468\n",
      "Epoch 490, training loss: 1.0417969226837158, train acc: 0.7494169357732329, val acc: 0.5047110552763819\n",
      "Epoch 500, training loss: 1.0479592084884644, train acc: 0.746681019016864, val acc: 0.5043969849246231\n",
      "Epoch 510, training loss: 1.0449095964431763, train acc: 0.7474883387154646, val acc: 0.507537688442211\n",
      "Epoch 520, training loss: 1.0391181707382202, train acc: 0.7489684248295658, val acc: 0.510678391959799\n",
      "Epoch 530, training loss: 1.0319921970367432, train acc: 0.7538123430211697, val acc: 0.5135050251256281\n",
      "Epoch 540, training loss: 1.0267372131347656, train acc: 0.7557857911733046, val acc: 0.5144472361809045\n",
      "Epoch 550, training loss: 1.0250911712646484, train acc: 0.759642985288841, val acc: 0.5179020100502513\n",
      "Epoch 560, training loss: 1.0428307056427002, train acc: 0.7525116612845354, val acc: 0.5172738693467337\n",
      "Epoch 570, training loss: 1.0100178718566895, train acc: 0.7628274129888769, val acc: 0.5166457286432161\n",
      "Epoch 580, training loss: 1.007620930671692, train acc: 0.759956942949408, val acc: 0.5179020100502513\n",
      "Epoch 590, training loss: 1.0090131759643555, train acc: 0.7616612845353427, val acc: 0.5207286432160804\n",
      "Epoch 600, training loss: 1.0025956630706787, train acc: 0.7656530319339792, val acc: 0.5207286432160804\n",
      "Epoch 610, training loss: 1.0018575191497803, train acc: 0.76672945819878, val acc: 0.5204145728643216\n",
      "Epoch 620, training loss: 1.0084307193756104, train acc: 0.7698241837100825, val acc: 0.5207286432160804\n",
      "Epoch 630, training loss: 0.9916337728500366, train acc: 0.7707660566917833, val acc: 0.5257537688442211\n",
      "Epoch 640, training loss: 0.9992054104804993, train acc: 0.7657427341227125, val acc: 0.5232412060301508\n",
      "Epoch 650, training loss: 0.9989058971405029, train acc: 0.7643074991029781, val acc: 0.5254396984924623\n",
      "Epoch 660, training loss: 0.9928207397460938, train acc: 0.7679852888410478, val acc: 0.5254396984924623\n",
      "Epoch 670, training loss: 0.992975115776062, train acc: 0.7679852888410478, val acc: 0.5263819095477387\n",
      "Epoch 680, training loss: 0.9910231828689575, train acc: 0.7701381413706494, val acc: 0.5257537688442211\n",
      "Epoch 690, training loss: 0.9879505634307861, train acc: 0.7745335486185863, val acc: 0.5260678391959799\n",
      "Epoch 700, training loss: 0.9885613918304443, train acc: 0.7701829924650161, val acc: 0.5257537688442211\n",
      "Epoch 710, training loss: 0.9888235926628113, train acc: 0.772021887334051, val acc: 0.5263819095477387\n",
      "Epoch 720, training loss: 0.9812606573104858, train acc: 0.7730534625044851, val acc: 0.5345477386934674\n",
      "Epoch 730, training loss: 0.9787555932998657, train acc: 0.7749372084678866, val acc: 0.5361180904522613\n",
      "Epoch 740, training loss: 0.9864106774330139, train acc: 0.7741747398636527, val acc: 0.5345477386934674\n",
      "Epoch 750, training loss: 0.9813014268875122, train acc: 0.776686401148188, val acc: 0.5376884422110553\n",
      "Epoch 760, training loss: 0.9711408615112305, train acc: 0.775340868317187, val acc: 0.5392587939698492\n",
      "Epoch 770, training loss: 0.9899325370788574, train acc: 0.7695550771438823, val acc: 0.5392587939698492\n",
      "Epoch 780, training loss: 0.9790269136428833, train acc: 0.7751166128453534, val acc: 0.5398869346733668\n",
      "Epoch 790, training loss: 0.9604671597480774, train acc: 0.7848044492285612, val acc: 0.5402010050251256\n",
      "Epoch 800, training loss: 0.9815952777862549, train acc: 0.7716630785791173, val acc: 0.5389447236180904\n",
      "Epoch 810, training loss: 0.9857151508331299, train acc: 0.7725601004664514, val acc: 0.5398869346733668\n",
      "Epoch 820, training loss: 0.9768667221069336, train acc: 0.7796017222820237, val acc: 0.539572864321608\n",
      "Epoch 830, training loss: 0.9772045016288757, train acc: 0.7749820595622533, val acc: 0.5398869346733668\n",
      "Epoch 840, training loss: 0.9738770127296448, train acc: 0.775475421600287, val acc: 0.539572864321608\n",
      "Epoch 850, training loss: 0.9645525217056274, train acc: 0.7785252960172229, val acc: 0.5417713567839196\n",
      "Epoch 860, training loss: 0.9732614755630493, train acc: 0.7705418012199498, val acc: 0.5402010050251256\n",
      "Epoch 870, training loss: 0.9940827488899231, train acc: 0.7743092931467528, val acc: 0.5411432160804021\n",
      "Epoch 880, training loss: 0.9730544090270996, train acc: 0.7762827412988877, val acc: 0.5389447236180904\n",
      "Epoch 890, training loss: 0.965444803237915, train acc: 0.7768658055256549, val acc: 0.5411432160804021\n",
      "Epoch 900, training loss: 0.9827594757080078, train acc: 0.7722461428058844, val acc: 0.5398869346733668\n",
      "Epoch 910, training loss: 0.9595200419425964, train acc: 0.7803641908862576, val acc: 0.5405150753768844\n",
      "Epoch 920, training loss: 0.9880067706108093, train acc: 0.7607194115536419, val acc: 0.5402010050251256\n",
      "Epoch 930, training loss: 0.9753177165985107, train acc: 0.7768658055256549, val acc: 0.5398869346733668\n",
      "Epoch 940, training loss: 0.9699707627296448, train acc: 0.7686580552565483, val acc: 0.5402010050251256\n",
      "Epoch 950, training loss: 0.9734224081039429, train acc: 0.781664872622892, val acc: 0.5402010050251256\n",
      "Epoch 960, training loss: 0.9621016979217529, train acc: 0.7746232508073198, val acc: 0.5414572864321608\n",
      "Epoch 970, training loss: 0.9621187448501587, train acc: 0.778794402583423, val acc: 0.5414572864321608\n",
      "Epoch 980, training loss: 0.9672284722328186, train acc: 0.7749820595622533, val acc: 0.5414572864321608\n",
      "Epoch 990, training loss: 0.9799650311470032, train acc: 0.7734122712594187, val acc: 0.5408291457286433\n",
      "Epoch 1000, training loss: 0.9755783677101135, train acc: 0.7788392536777897, val acc: 0.5414572864321608\n",
      "Early stopping at epoch 4\n",
      "Best validation accuracy: 0.697236\n"
     ]
    }
   ],
   "source": [
    "# finally start training\n",
    "print(\"Training for GCN\")\n",
    "\n",
    "features = data.x\n",
    "\n",
    "emb_ch = 32\n",
    "gcn_encoder = GCNEncoder(nfeat=features.shape[1], nhid=16, nclass=emb_ch)  # here nclass is the embedding dimension\n",
    "gcn_lpm = LinkPredModel(gcn_encoder, hidden_channels=emb_ch)\n",
    "\n",
    "gcn_lpm.to(device)\n",
    "data.to(device)\n",
    "\n",
    "train_log = train(gcn_lpm, data, lr=0.001, epochs=5000, patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training curve\n",
    "plot_training_curve(train_log, \"GCN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Train a GAT Encoder for Link Prediction (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n",
    "class GATEncoder(GAT):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, x, adj): # different argument from GAT, but kept the same as GCNEncoder\n",
    "        edge_index = adj.coalesce().indices()\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        z = self.conv2(x, edge_index)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for GAT\n",
      "Epoch 0, training loss: 1.4302451610565186, train acc: 0.46568891280947255, val acc: 0.5\n",
      "Epoch 10, training loss: 1.3809998035430908, train acc: 0.5953982777179763, val acc: 0.5\n",
      "Epoch 20, training loss: 1.3594706058502197, train acc: 0.6088536060279871, val acc: 0.5\n",
      "Epoch 30, training loss: 1.3347548246383667, train acc: 0.6336562612127736, val acc: 0.5\n",
      "Epoch 40, training loss: 1.317178726196289, train acc: 0.626210979547901, val acc: 0.5\n",
      "Epoch 50, training loss: 1.3080469369888306, train acc: 0.6319070685324721, val acc: 0.5\n",
      "Epoch 60, training loss: 1.2890005111694336, train acc: 0.6440168640114818, val acc: 0.5\n",
      "Epoch 70, training loss: 1.2768316268920898, train acc: 0.6473358449946178, val acc: 0.5\n",
      "Epoch 80, training loss: 1.257333517074585, train acc: 0.6568442770003589, val acc: 0.5\n",
      "Epoch 90, training loss: 1.258871078491211, train acc: 0.6531664872622892, val acc: 0.5\n",
      "Epoch 100, training loss: 1.2446258068084717, train acc: 0.6579207032651597, val acc: 0.4996859296482412\n",
      "Epoch 110, training loss: 1.2463886737823486, train acc: 0.6545568711876569, val acc: 0.5028266331658291\n",
      "Epoch 120, training loss: 1.2194044589996338, train acc: 0.6702996053103696, val acc: 0.5028266331658291\n",
      "Epoch 130, training loss: 1.229053258895874, train acc: 0.6685504126300682, val acc: 0.5084798994974874\n",
      "Epoch 140, training loss: 1.2145805358886719, train acc: 0.6757265877287406, val acc: 0.5232412060301508\n",
      "Epoch 150, training loss: 1.2198412418365479, train acc: 0.665410836024399, val acc: 0.5025125628140703\n",
      "Epoch 160, training loss: 1.2199389934539795, train acc: 0.6790007176175099, val acc: 0.5113065326633166\n",
      "Epoch 170, training loss: 1.2038376331329346, train acc: 0.6865805525654826, val acc: 0.5493090452261307\n",
      "Epoch 180, training loss: 1.2082995176315308, train acc: 0.6744707570864729, val acc: 0.5675251256281407\n",
      "Epoch 190, training loss: 1.2067303657531738, train acc: 0.6698959454610692, val acc: 0.5709798994974874\n",
      "Epoch 200, training loss: 1.2238861322402954, train acc: 0.6561715105848582, val acc: 0.594535175879397\n",
      "Epoch 210, training loss: 1.202815294265747, train acc: 0.6813778256189451, val acc: 0.5612437185929648\n",
      "Epoch 220, training loss: 1.2085236310958862, train acc: 0.667518837459634, val acc: 0.5612437185929648\n",
      "Epoch 230, training loss: 1.2096843719482422, train acc: 0.6764890563329745, val acc: 0.5609296482412061\n",
      "Epoch 240, training loss: 1.205649971961975, train acc: 0.6846519555077144, val acc: 0.5207286432160804\n",
      "Epoch 250, training loss: 1.2112421989440918, train acc: 0.6844277000358808, val acc: 0.5662688442211056\n",
      "Epoch 260, training loss: 1.1877641677856445, train acc: 0.6832167204879799, val acc: 0.5606155778894473\n",
      "Epoch 270, training loss: 1.195608377456665, train acc: 0.6842931467527807, val acc: 0.5668969849246231\n",
      "Epoch 280, training loss: 1.1880321502685547, train acc: 0.6827233584499461, val acc: 0.5643844221105527\n",
      "Epoch 290, training loss: 1.186971664428711, train acc: 0.682454251883746, val acc: 0.5891959798994975\n",
      "Epoch 300, training loss: 1.2002315521240234, train acc: 0.6802116971654109, val acc: 0.5383165829145728\n",
      "Epoch 310, training loss: 1.193598985671997, train acc: 0.6851453175457481, val acc: 0.5989321608040201\n",
      "Epoch 320, training loss: 1.21921706199646, train acc: 0.6795389307499103, val acc: 0.6284547738693468\n",
      "Epoch 330, training loss: 1.187707781791687, train acc: 0.6901237890204521, val acc: 0.5376884422110553\n",
      "Epoch 340, training loss: 1.1795930862426758, train acc: 0.6894510226049516, val acc: 0.5860552763819096\n",
      "Epoch 350, training loss: 1.1777396202087402, train acc: 0.6969860064585576, val acc: 0.5932788944723618\n",
      "Epoch 360, training loss: 1.187978744506836, train acc: 0.6876569788302834, val acc: 0.612751256281407\n",
      "Epoch 370, training loss: 1.1915090084075928, train acc: 0.6750986724076068, val acc: 0.6196608040201005\n",
      "Epoch 380, training loss: 1.1749012470245361, train acc: 0.6872981700753499, val acc: 0.5885678391959799\n",
      "Epoch 390, training loss: 1.1940369606018066, train acc: 0.6724076067456046, val acc: 0.6215452261306532\n",
      "Epoch 400, training loss: 1.1885955333709717, train acc: 0.6726318622174381, val acc: 0.6271984924623115\n",
      "Epoch 410, training loss: 1.1824049949645996, train acc: 0.692859705776821, val acc: 0.6413316582914573\n",
      "Epoch 420, training loss: 1.174148440361023, train acc: 0.6867151058485828, val acc: 0.6240577889447236\n",
      "Epoch 430, training loss: 1.1779148578643799, train acc: 0.689226767133118, val acc: 0.5477386934673367\n",
      "Epoch 440, training loss: 1.1715348958969116, train acc: 0.689540724793685, val acc: 0.5995603015075377\n",
      "Epoch 450, training loss: 1.1835904121398926, train acc: 0.6852798708288482, val acc: 0.5477386934673367\n",
      "Epoch 460, training loss: 1.167554259300232, train acc: 0.699632221026193, val acc: 0.5326633165829145\n",
      "Epoch 470, training loss: 1.1915040016174316, train acc: 0.6925008970218873, val acc: 0.5581030150753769\n",
      "Epoch 480, training loss: 1.1756936311721802, train acc: 0.69586472909939, val acc: 0.5279522613065326\n",
      "Epoch 490, training loss: 1.1713446378707886, train acc: 0.689540724793685, val acc: 0.5618718592964824\n",
      "Epoch 500, training loss: 1.167109489440918, train acc: 0.6972551130247578, val acc: 0.5891959798994975\n",
      "Epoch 510, training loss: 1.1730397939682007, train acc: 0.6898995335486185, val acc: 0.5662688442211056\n",
      "Epoch 520, training loss: 1.182762861251831, train acc: 0.6893613204162181, val acc: 0.5367462311557789\n",
      "Epoch 530, training loss: 1.1627041101455688, train acc: 0.6960441334768568, val acc: 0.5725502512562815\n",
      "Epoch 540, training loss: 1.1613883972167969, train acc: 0.6920523860782203, val acc: 0.6102386934673367\n",
      "Epoch 550, training loss: 1.1784229278564453, train acc: 0.6940706853247219, val acc: 0.571608040201005\n",
      "Epoch 560, training loss: 1.169511318206787, train acc: 0.696178686759957, val acc: 0.5603015075376885\n",
      "Epoch 570, training loss: 1.1591153144836426, train acc: 0.7001255830642268, val acc: 0.5590452261306532\n",
      "Epoch 580, training loss: 1.1669447422027588, train acc: 0.6987800502332256, val acc: 0.5367462311557789\n",
      "Epoch 590, training loss: 1.1775100231170654, train acc: 0.6962683889486904, val acc: 0.5750628140703518\n",
      "Epoch 600, training loss: 1.1638003587722778, train acc: 0.6946537495514891, val acc: 0.6070979899497487\n",
      "Epoch 610, training loss: 1.1614491939544678, train acc: 0.697524219590958, val acc: 0.5449120603015075\n",
      "Epoch 620, training loss: 1.159639596939087, train acc: 0.6944743451740223, val acc: 0.5791457286432161\n",
      "Epoch 630, training loss: 1.1655333042144775, train acc: 0.69586472909939, val acc: 0.5810301507537688\n",
      "Epoch 640, training loss: 1.1677627563476562, train acc: 0.6960889845712236, val acc: 0.5785175879396985\n",
      "Epoch 650, training loss: 1.1687730550765991, train acc: 0.6959544312881234, val acc: 0.5734924623115578\n",
      "Epoch 660, training loss: 1.156409502029419, train acc: 0.6997667743092931, val acc: 0.5829145728643216\n",
      "Epoch 670, training loss: 1.1677805185317993, train acc: 0.6990491567994259, val acc: 0.5986180904522613\n",
      "Epoch 680, training loss: 1.163000226020813, train acc: 0.698735199138859, val acc: 0.5423994974874372\n",
      "Epoch 690, training loss: 1.1652601957321167, train acc: 0.6934876210979548, val acc: 0.5310929648241206\n",
      "Epoch 700, training loss: 1.154365062713623, train acc: 0.6993182633656261, val acc: 0.617462311557789\n",
      "Epoch 710, training loss: 1.1600301265716553, train acc: 0.694967707212056, val acc: 0.5461683417085427\n",
      "Epoch 720, training loss: 1.1668322086334229, train acc: 0.6899443846429852, val acc: 0.5201005025125628\n",
      "Epoch 730, training loss: 1.1641197204589844, train acc: 0.6918729817007535, val acc: 0.5094221105527639\n",
      "Epoch 740, training loss: 1.155095100402832, train acc: 0.6951471115895228, val acc: 0.5336055276381909\n",
      "Epoch 750, training loss: 1.1625502109527588, train acc: 0.6919626838894869, val acc: 0.6064698492462312\n",
      "Epoch 760, training loss: 1.1646630764007568, train acc: 0.6968963042698242, val acc: 0.5763190954773869\n",
      "Epoch 770, training loss: 1.1592618227005005, train acc: 0.6946537495514891, val acc: 0.5389447236180904\n",
      "Epoch 780, training loss: 1.1786448955535889, train acc: 0.6891370649443846, val acc: 0.5703517587939698\n",
      "Epoch 790, training loss: 1.1642484664916992, train acc: 0.6910656620021528, val acc: 0.571608040201005\n",
      "Epoch 800, training loss: 1.156121015548706, train acc: 0.6929942590599211, val acc: 0.5769472361809045\n",
      "Epoch 810, training loss: 1.165475606918335, train acc: 0.7047452457839971, val acc: 0.571608040201005\n",
      "Epoch 820, training loss: 1.1586781740188599, train acc: 0.693308216720488, val acc: 0.5738065326633166\n",
      "Epoch 830, training loss: 1.1649413108825684, train acc: 0.6967168998923574, val acc: 0.5336055276381909\n",
      "Epoch 840, training loss: 1.167358160018921, train acc: 0.694967707212056, val acc: 0.5364321608040201\n",
      "Epoch 850, training loss: 1.1612415313720703, train acc: 0.6952816648726229, val acc: 0.5477386934673367\n",
      "Epoch 860, training loss: 1.1566333770751953, train acc: 0.6998564764980265, val acc: 0.5574748743718593\n",
      "Epoch 870, training loss: 1.1601853370666504, train acc: 0.6999013275923932, val acc: 0.550251256281407\n",
      "Epoch 880, training loss: 1.1456469297409058, train acc: 0.7004843918191603, val acc: 0.530464824120603\n",
      "Epoch 890, training loss: 1.1539499759674072, train acc: 0.6983315392895587, val acc: 0.5640703517587939\n",
      "Epoch 900, training loss: 1.155529499053955, train acc: 0.6977933261571583, val acc: 0.5467964824120602\n",
      "Epoch 910, training loss: 1.1506372690200806, train acc: 0.7002601363473269, val acc: 0.530464824120603\n",
      "Epoch 920, training loss: 1.1672368049621582, train acc: 0.6923214926444206, val acc: 0.5332914572864321\n",
      "Epoch 930, training loss: 1.1647515296936035, train acc: 0.6944294940796556, val acc: 0.5888819095477387\n",
      "Epoch 940, training loss: 1.150065302848816, train acc: 0.6974345174022246, val acc: 0.535175879396985\n",
      "Epoch 950, training loss: 1.1591112613677979, train acc: 0.6865805525654826, val acc: 0.522927135678392\n",
      "Epoch 960, training loss: 1.1472818851470947, train acc: 0.6995873699318264, val acc: 0.5144472361809045\n",
      "Epoch 970, training loss: 1.1709609031677246, train acc: 0.6911105130965196, val acc: 0.5292085427135679\n",
      "Epoch 980, training loss: 1.151078701019287, train acc: 0.7052386078220308, val acc: 0.5709798994974874\n",
      "Epoch 990, training loss: 1.1507718563079834, train acc: 0.7018747757445282, val acc: 0.5326633165829145\n",
      "Epoch 1000, training loss: 1.153172254562378, train acc: 0.694519196268389, val acc: 0.5373743718592965\n",
      "Epoch 1010, training loss: 1.169039011001587, train acc: 0.6915590240401865, val acc: 0.6114949748743719\n",
      "Epoch 1020, training loss: 1.1445469856262207, train acc: 0.6981969860064585, val acc: 0.628140703517588\n",
      "Epoch 1030, training loss: 1.1629682779312134, train acc: 0.6951919626838895, val acc: 0.6140075376884422\n",
      "Epoch 1040, training loss: 1.147108554840088, train acc: 0.7038033728022963, val acc: 0.5659547738693468\n",
      "Epoch 1050, training loss: 1.180281162261963, train acc: 0.6946088984571224, val acc: 0.597675879396985\n",
      "Epoch 1060, training loss: 1.1538928747177124, train acc: 0.7068980983135988, val acc: 0.532035175879397\n",
      "Epoch 1070, training loss: 1.155621886253357, train acc: 0.704924650161464, val acc: 0.5621859296482412\n",
      "Epoch 1080, training loss: 1.1476340293884277, train acc: 0.7038482238966631, val acc: 0.5483668341708543\n",
      "Epoch 1090, training loss: 1.1382942199707031, train acc: 0.7060459275206316, val acc: 0.5260678391959799\n",
      "Epoch 1100, training loss: 1.1583924293518066, train acc: 0.6958198780050233, val acc: 0.5339195979899497\n",
      "Epoch 1110, training loss: 1.15410578250885, train acc: 0.7071223537854323, val acc: 0.5436557788944724\n",
      "Epoch 1120, training loss: 1.1409363746643066, train acc: 0.7061356297093649, val acc: 0.5521356783919598\n",
      "Epoch 1130, training loss: 1.151902675628662, train acc: 0.7072120559741658, val acc: 0.5232412060301508\n",
      "Epoch 1140, training loss: 1.1538227796554565, train acc: 0.6996770721205597, val acc: 0.5354899497487438\n",
      "Epoch 1150, training loss: 1.1473665237426758, train acc: 0.6995425188374597, val acc: 0.5219849246231156\n",
      "Epoch 1160, training loss: 1.1591808795928955, train acc: 0.6919626838894869, val acc: 0.5153894472361809\n",
      "Epoch 1170, training loss: 1.1467251777648926, train acc: 0.7000358808754934, val acc: 0.5097361809045227\n",
      "Epoch 1180, training loss: 1.1425285339355469, train acc: 0.7009329027628274, val acc: 0.5182160804020101\n",
      "Epoch 1190, training loss: 1.151933193206787, train acc: 0.709903121636168, val acc: 0.5276381909547738\n",
      "Epoch 1200, training loss: 1.1473851203918457, train acc: 0.7011123071402943, val acc: 0.5480527638190955\n",
      "Epoch 1210, training loss: 1.1541752815246582, train acc: 0.6968514531754575, val acc: 0.5201005025125628\n",
      "Epoch 1220, training loss: 1.135193943977356, train acc: 0.7078848223896663, val acc: 0.5160175879396985\n",
      "Epoch 1230, training loss: 1.1638065576553345, train acc: 0.6983763903839254, val acc: 0.5138190954773869\n",
      "Epoch 1240, training loss: 1.1567325592041016, train acc: 0.7032203085755292, val acc: 0.5222989949748744\n",
      "Epoch 1250, training loss: 1.1510846614837646, train acc: 0.6977933261571583, val acc: 0.519786432160804\n",
      "Epoch 1260, training loss: 1.1557425260543823, train acc: 0.7000807319698601, val acc: 0.5367462311557789\n",
      "Epoch 1270, training loss: 1.1389880180358887, train acc: 0.7066289917473987, val acc: 0.5182160804020101\n",
      "Epoch 1280, training loss: 1.1468918323516846, train acc: 0.7031754574811625, val acc: 0.5232412060301508\n",
      "Epoch 1290, training loss: 1.1441270112991333, train acc: 0.7055974165769645, val acc: 0.5144472361809045\n",
      "Epoch 1300, training loss: 1.158945918083191, train acc: 0.6983315392895587, val acc: 0.5411432160804021\n",
      "Epoch 1310, training loss: 1.1499803066253662, train acc: 0.7005740940078938, val acc: 0.5131909547738693\n",
      "Epoch 1320, training loss: 1.1512467861175537, train acc: 0.7028614998205956, val acc: 0.5072236180904522\n",
      "Epoch 1330, training loss: 1.1433542966842651, train acc: 0.7078848223896663, val acc: 0.5442839195979899\n",
      "Epoch 1340, training loss: 1.1480941772460938, train acc: 0.6991388589881593, val acc: 0.5207286432160804\n",
      "Epoch 1350, training loss: 1.1443545818328857, train acc: 0.7055525654825978, val acc: 0.5179020100502513\n",
      "Epoch 1360, training loss: 1.1421542167663574, train acc: 0.7082436311446, val acc: 0.5901381909547738\n",
      "Epoch 1370, training loss: 1.1388883590698242, train acc: 0.7060907786149982, val acc: 0.550251256281407\n",
      "Epoch 1380, training loss: 1.155470848083496, train acc: 0.6960441334768568, val acc: 0.5116206030150754\n",
      "Epoch 1390, training loss: 1.1429997682571411, train acc: 0.7016953713670614, val acc: 0.5210427135678392\n",
      "Epoch 1400, training loss: 1.1499229669570923, train acc: 0.7031306063867958, val acc: 0.5169597989949749\n",
      "Epoch 1410, training loss: 1.152761459350586, train acc: 0.6900340868317187, val acc: 0.5147613065326633\n",
      "Epoch 1420, training loss: 1.1400938034057617, train acc: 0.7068083961248655, val acc: 0.5015703517587939\n",
      "Epoch 1430, training loss: 1.136995553970337, train acc: 0.7089612486544672, val acc: 0.5018844221105527\n",
      "Epoch 1440, training loss: 1.1599977016448975, train acc: 0.7018747757445282, val acc: 0.5050251256281407\n",
      "Epoch 1450, training loss: 1.1518460512161255, train acc: 0.702502691065662, val acc: 0.5050251256281407\n",
      "Early stopping at epoch 453\n",
      "Best validation accuracy: 0.663945\n"
     ]
    }
   ],
   "source": [
    "print(\"Training for GAT\")\n",
    "\n",
    "features = data.x\n",
    "\n",
    "emb_ch = 32\n",
    "gat_encoder = GATEncoder(nfeat=features.shape[1], nhid=8, heads=8, nclass=emb_ch)  # here nclass is the embedding dimension\n",
    "gat_lpm = LinkPredModel(gat_encoder, hidden_channels=emb_ch)\n",
    "\n",
    "gat_lpm = gat_lpm.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "train_log = train(gat_lpm, data, lr=0.001, epochs=5000, patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training curve\n",
    "plot_training_curve(train_log, \"GAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. GCN vs. GAT Comparison (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_node_classification(embedding_matrix, labels, train_idx, val_idx,\n",
    "                                 test_idx, normalize_embedding=True, max_iter=5000):\n",
    "\n",
    "    \"\"\" Train a linear model (e.g., a fnn model) to predict the label of a node\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    preds: the predicted labels by the model\n",
    "    test_acc: the test accuracy \n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    if normalize_embedding:\n",
    "         embedding_matrix = normalize(embedding_matrix, axis=1)\n",
    "    \n",
    "    X_train = embedding_matrix[train_idx]\n",
    "    y_train = labels[train_idx]\n",
    "\n",
    "    X_val = embedding_matrix[val_idx]\n",
    "    y_val = labels[val_idx]\n",
    "\n",
    "    X_test = embedding_matrix[test_idx]\n",
    "    y_test = labels[test_idx]\n",
    "\n",
    "    model = LogisticRegression(max_iter=max_iter)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    return preds, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = graph['adj']\n",
    "edge_index = torch.tensor(adj).nonzero().t().contiguous()\n",
    "adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes)\n",
    "adj_norm = sparse_mx_to_torch_sparse_tensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN Test Accuracy: 0.037181409295352325\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gcn_lpm.eval()\n",
    "    emb_gcn = gcn_lpm.encoder(data.x, adj_norm)\n",
    "\n",
    "preds, test_acc = evaluate_node_classification(emb_gcn, label, train_index, val_index, test_index)\n",
    "print(f\"GCN Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT Test Accuracy: 0.06446776611694154\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gat_lpm.eval()\n",
    "    emb_gat = gat_lpm.encoder(data.x, adj_norm)\n",
    "\n",
    "preds, test_acc = evaluate_node_classification(emb_gat, label, train_index, val_index, test_index)\n",
    "print(f\"GAT Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bonus question (10 points)\n",
    "## Use the best techniques youâ€™ve learned in class to improve the classification performance of GCN/GAT/GraphSAGE. Report your code and results below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your best model\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class GCNPro(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True, negative_slope=0.2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.nfeat = nfeat\n",
    "        self.nclass = nclass\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid, with_bias=with_bias)\n",
    "        self.bn1 = BatchNorm(nhid) # we add a batch norm\n",
    "        self.gc2 = GraphConvolution(nhid, nclass, with_bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        ######################################\n",
    "        # write your code here \n",
    "        # hint: you can use F.dropout() function to define dropout \n",
    "        h = self.gc1(x, adj)\n",
    "        h = self.bn1(h)\n",
    "        h = F.leaky_relu(h, negative_slope=self.negative_slope) # we use leaky relu\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        predicts = self.gc2(h, adj)\n",
    "\n",
    "        ######################################\n",
    "        return F.log_softmax(predicts, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new train function with different optimizer\n",
    "def train(model, data, adj, lr=0.01, weight_decay=5e-4, epochs=200, patience=5):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) # use adamw than adam\n",
    "    labels = data.y\n",
    "    train_mask = data.train_mask \n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_val_epoch = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, adj)\n",
    "\n",
    "        loss = F.nll_loss(output[train_mask], labels[train_mask]) # compute training loss\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validate each epoch\n",
    "        _, _, acc_val, loss_val = validation(model, data, adj, verbose=False)\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {i}, training loss: {loss.item()}, val loss: {loss_val}, val_acc: {acc_val}\")\n",
    "            # print('Epoch {}, training loss: {}, '.format(i, loss.item()))\n",
    "\n",
    "        # early stopping\n",
    "        if acc_val > best_val_acc:\n",
    "            best_val_acc = acc_val\n",
    "            best_val_epoch = i\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {best_val_epoch} with validation accuracy {best_val_acc:4f}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None: \n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, adj):\n",
    "    \"\"\"test the performance\"\"\"\n",
    "    model.eval() # eval() \n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model(data.x, adj) #  \n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) #  \n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() #  \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(model, data, adj, verbose=True):\n",
    "    \"\"\"add a validation for early stopping\"\"\"\n",
    "    model.eval() # eval() \n",
    "    val_mask = data.val_mask\n",
    "    labels = data.y \n",
    "    output = model(data.x, adj) #  \n",
    "    loss_val = F.nll_loss(output[val_mask], labels[val_mask])\n",
    "    preds = output[val_mask].argmax(1) #  \n",
    "    acc_val= preds.eq(labels[val_mask]).cpu().numpy().mean() #  \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Val set results:\",\n",
    "            \"loss= {:.4f}\".format(loss_val.item()),\n",
    "            \"accuracy= {:.4f}\".format(acc_val))\n",
    "    return preds, output, acc_val.item(), loss_val.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_clip': Data(x=[5146, 512], edge_index=[2, 15924], y=[5146], train_mask=[986], val_mask=[825], test_mask=[3335]),\n",
       " 'data_cnn': Data(x=[5146, 256], edge_index=[2, 15924], y=[5146], train_mask=[986], val_mask=[825], test_mask=[3335])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCNPro with data_clip\n",
      "Epoch 0, training loss: 6.8644795417785645, val loss: 15.570046424865723, val_acc: 0.06060606060606061\n",
      "Epoch 10, training loss: 5.586727142333984, val loss: 5.232656955718994, val_acc: 0.06787878787878789\n",
      "Epoch 20, training loss: 4.64836311340332, val loss: 3.940659523010254, val_acc: 0.055757575757575756\n",
      "Epoch 30, training loss: 3.9385101795196533, val loss: 3.663271188735962, val_acc: 0.05696969696969697\n",
      "Epoch 40, training loss: 3.6463119983673096, val loss: 3.36875057220459, val_acc: 0.06909090909090909\n",
      "Epoch 50, training loss: 3.4768364429473877, val loss: 3.246875047683716, val_acc: 0.07757575757575758\n",
      "Epoch 60, training loss: 3.245399236679077, val loss: 3.2284910678863525, val_acc: 0.06545454545454546\n",
      "Epoch 70, training loss: 3.2208173274993896, val loss: 3.1910808086395264, val_acc: 0.07393939393939394\n",
      "Epoch 80, training loss: 3.2024238109588623, val loss: 3.099337339401245, val_acc: 0.08121212121212121\n",
      "Epoch 90, training loss: 3.1084117889404297, val loss: 3.0939486026763916, val_acc: 0.09696969696969697\n",
      "Epoch 100, training loss: 3.1317148208618164, val loss: 3.0500478744506836, val_acc: 0.09454545454545454\n",
      "Epoch 110, training loss: 2.988865613937378, val loss: 3.0598981380462646, val_acc: 0.08363636363636363\n",
      "Epoch 120, training loss: 2.9888808727264404, val loss: 3.0206429958343506, val_acc: 0.09818181818181818\n",
      "Epoch 130, training loss: 2.9742136001586914, val loss: 2.9808366298675537, val_acc: 0.1006060606060606\n",
      "Epoch 140, training loss: 3.0303003787994385, val loss: 2.9468913078308105, val_acc: 0.11757575757575757\n",
      "Epoch 150, training loss: 2.947800874710083, val loss: 2.9500913619995117, val_acc: 0.12242424242424242\n",
      "Epoch 160, training loss: 2.958390712738037, val loss: 3.0433590412139893, val_acc: 0.11393939393939394\n",
      "Epoch 170, training loss: 2.9193623065948486, val loss: 2.9643237590789795, val_acc: 0.12\n",
      "Epoch 180, training loss: 2.9084455966949463, val loss: 2.958946466445923, val_acc: 0.11151515151515151\n",
      "Epoch 190, training loss: 2.918086528778076, val loss: 2.996699333190918, val_acc: 0.11515151515151516\n",
      "Epoch 200, training loss: 2.857671022415161, val loss: 2.909209966659546, val_acc: 0.12121212121212122\n",
      "Epoch 210, training loss: 2.8792524337768555, val loss: 2.8895010948181152, val_acc: 0.13333333333333333\n",
      "Epoch 220, training loss: 2.7977874279022217, val loss: 2.918755292892456, val_acc: 0.12484848484848485\n",
      "Epoch 230, training loss: 2.8551366329193115, val loss: 2.886791944503784, val_acc: 0.12\n",
      "Epoch 240, training loss: 2.8869175910949707, val loss: 2.955784797668457, val_acc: 0.13696969696969696\n",
      "Epoch 250, training loss: 2.8062922954559326, val loss: 2.88594126701355, val_acc: 0.1296969696969697\n",
      "Epoch 260, training loss: 2.809473991394043, val loss: 2.846588134765625, val_acc: 0.13818181818181818\n",
      "Epoch 270, training loss: 2.767622947692871, val loss: 2.861229658126831, val_acc: 0.1406060606060606\n",
      "Epoch 280, training loss: 2.8059818744659424, val loss: 2.9745495319366455, val_acc: 0.1406060606060606\n",
      "Epoch 290, training loss: 2.7580583095550537, val loss: 2.8977832794189453, val_acc: 0.1393939393939394\n",
      "Epoch 300, training loss: 2.7741575241088867, val loss: 2.8797266483306885, val_acc: 0.14545454545454545\n",
      "Epoch 310, training loss: 2.749114990234375, val loss: 2.891050338745117, val_acc: 0.14545454545454545\n",
      "Epoch 320, training loss: 2.7403159141540527, val loss: 2.8747446537017822, val_acc: 0.15393939393939393\n",
      "Epoch 330, training loss: 2.742626905441284, val loss: 2.9121077060699463, val_acc: 0.14666666666666667\n",
      "Epoch 340, training loss: 2.7363874912261963, val loss: 2.844679117202759, val_acc: 0.15636363636363637\n",
      "Epoch 350, training loss: 2.7187399864196777, val loss: 2.814229726791382, val_acc: 0.16242424242424242\n",
      "Epoch 360, training loss: 2.7434005737304688, val loss: 2.7891905307769775, val_acc: 0.16242424242424242\n",
      "Epoch 370, training loss: 2.6913866996765137, val loss: 2.825950860977173, val_acc: 0.14909090909090908\n",
      "Epoch 380, training loss: 2.6743273735046387, val loss: 2.8108878135681152, val_acc: 0.15151515151515152\n",
      "Epoch 390, training loss: 2.7397537231445312, val loss: 2.8020200729370117, val_acc: 0.1709090909090909\n",
      "Epoch 400, training loss: 2.711513042449951, val loss: 2.78767728805542, val_acc: 0.16727272727272727\n",
      "Epoch 410, training loss: 2.6874148845672607, val loss: 2.8115360736846924, val_acc: 0.16363636363636364\n",
      "Epoch 420, training loss: 2.6928062438964844, val loss: 2.7748100757598877, val_acc: 0.1684848484848485\n",
      "Epoch 430, training loss: 2.7116456031799316, val loss: 2.7951226234436035, val_acc: 0.15151515151515152\n",
      "Epoch 440, training loss: 2.667426347732544, val loss: 2.778014898300171, val_acc: 0.17454545454545456\n",
      "Epoch 450, training loss: 2.6897125244140625, val loss: 2.7698724269866943, val_acc: 0.18303030303030304\n",
      "Epoch 460, training loss: 2.649441719055176, val loss: 2.8413286209106445, val_acc: 0.16\n",
      "Epoch 470, training loss: 2.6308045387268066, val loss: 2.834988832473755, val_acc: 0.16484848484848486\n",
      "Epoch 480, training loss: 2.67763352394104, val loss: 2.8095247745513916, val_acc: 0.18181818181818182\n",
      "Epoch 490, training loss: 2.668368339538574, val loss: 2.7628257274627686, val_acc: 0.2012121212121212\n",
      "Epoch 500, training loss: 2.601977586746216, val loss: 2.833050012588501, val_acc: 0.15151515151515152\n",
      "Epoch 510, training loss: 2.645368814468384, val loss: 2.7603657245635986, val_acc: 0.18666666666666668\n",
      "Epoch 520, training loss: 2.5985820293426514, val loss: 2.757645845413208, val_acc: 0.18424242424242424\n",
      "Epoch 530, training loss: 2.6517131328582764, val loss: 2.7791950702667236, val_acc: 0.19272727272727272\n",
      "Epoch 540, training loss: 2.6499836444854736, val loss: 2.7649505138397217, val_acc: 0.17696969696969697\n",
      "Epoch 550, training loss: 2.6087088584899902, val loss: 2.750711441040039, val_acc: 0.19272727272727272\n",
      "Epoch 560, training loss: 2.6059467792510986, val loss: 2.7332870960235596, val_acc: 0.19393939393939394\n",
      "Epoch 570, training loss: 2.6205596923828125, val loss: 2.774113178253174, val_acc: 0.18424242424242424\n",
      "Epoch 580, training loss: 2.5796961784362793, val loss: 2.7408792972564697, val_acc: 0.19393939393939394\n",
      "Epoch 590, training loss: 2.6075613498687744, val loss: 2.751397132873535, val_acc: 0.18666666666666668\n",
      "Epoch 600, training loss: 2.6143736839294434, val loss: 2.693335771560669, val_acc: 0.21333333333333335\n",
      "Epoch 610, training loss: 2.6258347034454346, val loss: 2.716914415359497, val_acc: 0.19515151515151516\n",
      "Epoch 620, training loss: 2.605456590652466, val loss: 2.7325432300567627, val_acc: 0.2012121212121212\n",
      "Epoch 630, training loss: 2.606585741043091, val loss: 2.780421495437622, val_acc: 0.18666666666666668\n",
      "Epoch 640, training loss: 2.593812942504883, val loss: 2.7374775409698486, val_acc: 0.21333333333333335\n",
      "Epoch 650, training loss: 2.562807083129883, val loss: 2.719264268875122, val_acc: 0.2109090909090909\n",
      "Epoch 660, training loss: 2.5773489475250244, val loss: 2.71274995803833, val_acc: 0.20606060606060606\n",
      "Epoch 670, training loss: 2.5638296604156494, val loss: 2.7363345623016357, val_acc: 0.19515151515151516\n",
      "Epoch 680, training loss: 2.541443109512329, val loss: 2.7278192043304443, val_acc: 0.20484848484848484\n",
      "Epoch 690, training loss: 2.5322530269622803, val loss: 2.718898057937622, val_acc: 0.20727272727272728\n",
      "Epoch 700, training loss: 2.5468361377716064, val loss: 2.7186737060546875, val_acc: 0.20363636363636364\n",
      "Epoch 710, training loss: 2.5555379390716553, val loss: 2.6910269260406494, val_acc: 0.2096969696969697\n",
      "Epoch 720, training loss: 2.5335309505462646, val loss: 2.688600778579712, val_acc: 0.21212121212121213\n",
      "Epoch 730, training loss: 2.549278736114502, val loss: 2.6710734367370605, val_acc: 0.2193939393939394\n",
      "Epoch 740, training loss: 2.545869827270508, val loss: 2.7272250652313232, val_acc: 0.1987878787878788\n",
      "Epoch 750, training loss: 2.5270280838012695, val loss: 2.7082455158233643, val_acc: 0.2193939393939394\n",
      "Epoch 760, training loss: 2.5320727825164795, val loss: 2.706897735595703, val_acc: 0.2096969696969697\n",
      "Epoch 770, training loss: 2.5074851512908936, val loss: 2.7259511947631836, val_acc: 0.19636363636363635\n",
      "Epoch 780, training loss: 2.510139226913452, val loss: 2.6946144104003906, val_acc: 0.2096969696969697\n",
      "Epoch 790, training loss: 2.5471692085266113, val loss: 2.6820948123931885, val_acc: 0.21212121212121213\n",
      "Epoch 800, training loss: 2.552795171737671, val loss: 2.699589252471924, val_acc: 0.20606060606060606\n",
      "Epoch 810, training loss: 2.5607123374938965, val loss: 2.6990597248077393, val_acc: 0.2\n",
      "Epoch 820, training loss: 2.524458408355713, val loss: 2.681746244430542, val_acc: 0.20242424242424242\n",
      "Epoch 830, training loss: 2.5387704372406006, val loss: 2.715683698654175, val_acc: 0.20606060606060606\n",
      "Epoch 840, training loss: 2.5247933864593506, val loss: 2.7097551822662354, val_acc: 0.20242424242424242\n",
      "Epoch 850, training loss: 2.5203235149383545, val loss: 2.695672035217285, val_acc: 0.20606060606060606\n",
      "Epoch 860, training loss: 2.4954116344451904, val loss: 2.7030651569366455, val_acc: 0.2096969696969697\n",
      "Epoch 870, training loss: 2.519585371017456, val loss: 2.736178398132324, val_acc: 0.20606060606060606\n",
      "Epoch 880, training loss: 2.5086588859558105, val loss: 2.702040910720825, val_acc: 0.19636363636363635\n",
      "Epoch 890, training loss: 2.491353750228882, val loss: 2.662015199661255, val_acc: 0.20363636363636364\n",
      "Epoch 900, training loss: 2.480684757232666, val loss: 2.662172317504883, val_acc: 0.22545454545454546\n",
      "Epoch 910, training loss: 2.491255760192871, val loss: 2.676443576812744, val_acc: 0.21696969696969698\n",
      "Epoch 920, training loss: 2.497408628463745, val loss: 2.6661667823791504, val_acc: 0.2096969696969697\n",
      "Epoch 930, training loss: 2.498685359954834, val loss: 2.689913034439087, val_acc: 0.21454545454545454\n",
      "Epoch 940, training loss: 2.4770162105560303, val loss: 2.678468942642212, val_acc: 0.2109090909090909\n",
      "Epoch 950, training loss: 2.4765806198120117, val loss: 2.666848659515381, val_acc: 0.21575757575757576\n",
      "Epoch 960, training loss: 2.4862210750579834, val loss: 2.6711647510528564, val_acc: 0.20363636363636364\n",
      "Epoch 970, training loss: 2.484281301498413, val loss: 2.7274229526519775, val_acc: 0.19393939393939394\n",
      "Epoch 980, training loss: 2.475365161895752, val loss: 2.6816012859344482, val_acc: 0.22181818181818183\n",
      "Epoch 990, training loss: 2.4587724208831787, val loss: 2.7282936573028564, val_acc: 0.1987878787878788\n",
      "Epoch 1000, training loss: 2.462890625, val loss: 2.7023229598999023, val_acc: 0.20363636363636364\n",
      "Epoch 1010, training loss: 2.504865884780884, val loss: 2.693011999130249, val_acc: 0.2\n",
      "Epoch 1020, training loss: 2.4608302116394043, val loss: 2.7020936012268066, val_acc: 0.19272727272727272\n",
      "Epoch 1030, training loss: 2.446403980255127, val loss: 2.6823863983154297, val_acc: 0.1987878787878788\n",
      "Epoch 1040, training loss: 2.4901962280273438, val loss: 2.6711103916168213, val_acc: 0.21454545454545454\n",
      "Epoch 1050, training loss: 2.46411395072937, val loss: 2.676485538482666, val_acc: 0.1987878787878788\n",
      "Epoch 1060, training loss: 2.4824836254119873, val loss: 2.6848206520080566, val_acc: 0.20727272727272728\n",
      "Epoch 1070, training loss: 2.4675052165985107, val loss: 2.6751410961151123, val_acc: 0.2084848484848485\n",
      "Epoch 1080, training loss: 2.4789786338806152, val loss: 2.673516273498535, val_acc: 0.21454545454545454\n",
      "Epoch 1090, training loss: 2.469087839126587, val loss: 2.6715176105499268, val_acc: 0.21333333333333335\n",
      "Epoch 1100, training loss: 2.4986138343811035, val loss: 2.7160146236419678, val_acc: 0.20484848484848484\n",
      "Epoch 1110, training loss: 2.4960601329803467, val loss: 2.6687910556793213, val_acc: 0.21333333333333335\n",
      "Epoch 1120, training loss: 2.4602859020233154, val loss: 2.679586887359619, val_acc: 0.19151515151515153\n",
      "Epoch 1130, training loss: 2.4605021476745605, val loss: 2.654844284057617, val_acc: 0.20727272727272728\n",
      "Epoch 1140, training loss: 2.476498603820801, val loss: 2.661161184310913, val_acc: 0.21454545454545454\n",
      "Epoch 1150, training loss: 2.4723477363586426, val loss: 2.7000882625579834, val_acc: 0.21818181818181817\n",
      "Epoch 1160, training loss: 2.4822723865509033, val loss: 2.7168407440185547, val_acc: 0.19151515151515153\n",
      "Epoch 1170, training loss: 2.4622528553009033, val loss: 2.683608055114746, val_acc: 0.2012121212121212\n",
      "Epoch 1180, training loss: 2.469773769378662, val loss: 2.6825737953186035, val_acc: 0.21212121212121213\n",
      "Epoch 1190, training loss: 2.458653688430786, val loss: 2.697573184967041, val_acc: 0.19757575757575757\n",
      "Epoch 1200, training loss: 2.470048189163208, val loss: 2.7172963619232178, val_acc: 0.2109090909090909\n",
      "Epoch 1210, training loss: 2.4327924251556396, val loss: 2.672677516937256, val_acc: 0.20242424242424242\n",
      "Epoch 1220, training loss: 2.452047824859619, val loss: 2.6802823543548584, val_acc: 0.21696969696969698\n",
      "Epoch 1230, training loss: 2.4540207386016846, val loss: 2.670001268386841, val_acc: 0.21454545454545454\n",
      "Epoch 1240, training loss: 2.4245617389678955, val loss: 2.668856620788574, val_acc: 0.20484848484848484\n",
      "Epoch 1250, training loss: 2.4248533248901367, val loss: 2.682271957397461, val_acc: 0.22181818181818183\n",
      "Epoch 1260, training loss: 2.4383416175842285, val loss: 2.6241118907928467, val_acc: 0.22666666666666666\n",
      "Epoch 1270, training loss: 2.425933599472046, val loss: 2.6587746143341064, val_acc: 0.2206060606060606\n",
      "Epoch 1280, training loss: 2.466158390045166, val loss: 2.678394317626953, val_acc: 0.22181818181818183\n",
      "Epoch 1290, training loss: 2.418729543685913, val loss: 2.66064715385437, val_acc: 0.21212121212121213\n",
      "Epoch 1300, training loss: 2.4098947048187256, val loss: 2.678577423095703, val_acc: 0.2012121212121212\n",
      "Epoch 1310, training loss: 2.419511318206787, val loss: 2.7060794830322266, val_acc: 0.19757575757575757\n",
      "Epoch 1320, training loss: 2.4529201984405518, val loss: 2.692009687423706, val_acc: 0.2206060606060606\n",
      "Epoch 1330, training loss: 2.4190802574157715, val loss: 2.6492652893066406, val_acc: 0.2193939393939394\n",
      "Epoch 1340, training loss: 2.425767660140991, val loss: 2.668463945388794, val_acc: 0.20363636363636364\n",
      "Epoch 1350, training loss: 2.422952175140381, val loss: 2.7104220390319824, val_acc: 0.1987878787878788\n",
      "Epoch 1360, training loss: 2.4250309467315674, val loss: 2.649784564971924, val_acc: 0.21333333333333335\n",
      "Epoch 1370, training loss: 2.414303779602051, val loss: 2.7055749893188477, val_acc: 0.21818181818181817\n",
      "Epoch 1380, training loss: 2.44525408744812, val loss: 2.6536238193511963, val_acc: 0.21212121212121213\n",
      "Epoch 1390, training loss: 2.4454355239868164, val loss: 2.656054973602295, val_acc: 0.21575757575757576\n",
      "Epoch 1400, training loss: 2.4248225688934326, val loss: 2.6712825298309326, val_acc: 0.2193939393939394\n",
      "Epoch 1410, training loss: 2.4372661113739014, val loss: 2.6540040969848633, val_acc: 0.2096969696969697\n",
      "Epoch 1420, training loss: 2.4706990718841553, val loss: 2.6558473110198975, val_acc: 0.21818181818181817\n",
      "Epoch 1430, training loss: 2.442321538925171, val loss: 2.640820026397705, val_acc: 0.21575757575757576\n",
      "Epoch 1440, training loss: 2.4172146320343018, val loss: 2.6663944721221924, val_acc: 0.20606060606060606\n",
      "Epoch 1450, training loss: 2.411155939102173, val loss: 2.658278703689575, val_acc: 0.2109090909090909\n",
      "Epoch 1460, training loss: 2.405170202255249, val loss: 2.662036657333374, val_acc: 0.2193939393939394\n",
      "Epoch 1470, training loss: 2.408130645751953, val loss: 2.652754306793213, val_acc: 0.2096969696969697\n",
      "Epoch 1480, training loss: 2.4405179023742676, val loss: 2.7067131996154785, val_acc: 0.22181818181818183\n",
      "Epoch 1490, training loss: 2.427588701248169, val loss: 2.658799171447754, val_acc: 0.22181818181818183\n",
      "Epoch 1500, training loss: 2.430856227874756, val loss: 2.6789417266845703, val_acc: 0.2109090909090909\n",
      "Epoch 1510, training loss: 2.3878774642944336, val loss: 2.6449201107025146, val_acc: 0.21818181818181817\n",
      "Epoch 1520, training loss: 2.4376564025878906, val loss: 2.668384552001953, val_acc: 0.22181818181818183\n",
      "Epoch 1530, training loss: 2.414245128631592, val loss: 2.669227123260498, val_acc: 0.21696969696969698\n",
      "Epoch 1540, training loss: 2.4070322513580322, val loss: 2.688549280166626, val_acc: 0.21454545454545454\n",
      "Epoch 1550, training loss: 2.4249014854431152, val loss: 2.6897342205047607, val_acc: 0.21212121212121213\n",
      "Epoch 1560, training loss: 2.4014856815338135, val loss: 2.660627841949463, val_acc: 0.2109090909090909\n",
      "Epoch 1570, training loss: 2.4029550552368164, val loss: 2.6578314304351807, val_acc: 0.20606060606060606\n",
      "Epoch 1580, training loss: 2.420719861984253, val loss: 2.659830331802368, val_acc: 0.22545454545454546\n",
      "Epoch 1590, training loss: 2.432568073272705, val loss: 2.6821250915527344, val_acc: 0.2096969696969697\n",
      "Epoch 1600, training loss: 2.4013867378234863, val loss: 2.6298201084136963, val_acc: 0.2290909090909091\n",
      "Epoch 1610, training loss: 2.4017019271850586, val loss: 2.697890281677246, val_acc: 0.23030303030303031\n",
      "Epoch 1620, training loss: 2.4041786193847656, val loss: 2.6656250953674316, val_acc: 0.22666666666666666\n",
      "Epoch 1630, training loss: 2.418121814727783, val loss: 2.6896324157714844, val_acc: 0.20484848484848484\n",
      "Epoch 1640, training loss: 2.3961265087127686, val loss: 2.660346269607544, val_acc: 0.22545454545454546\n",
      "Epoch 1650, training loss: 2.412482976913452, val loss: 2.6597025394439697, val_acc: 0.22424242424242424\n",
      "Epoch 1660, training loss: 2.378539562225342, val loss: 2.639211654663086, val_acc: 0.21818181818181817\n",
      "Epoch 1670, training loss: 2.38154935836792, val loss: 2.671900987625122, val_acc: 0.21454545454545454\n",
      "Epoch 1680, training loss: 2.379563570022583, val loss: 2.6800363063812256, val_acc: 0.22303030303030302\n",
      "Epoch 1690, training loss: 2.4252400398254395, val loss: 2.665030002593994, val_acc: 0.21333333333333335\n",
      "Epoch 1700, training loss: 2.3883774280548096, val loss: 2.654261350631714, val_acc: 0.2193939393939394\n",
      "Epoch 1710, training loss: 2.3937273025512695, val loss: 2.6437580585479736, val_acc: 0.22181818181818183\n",
      "Epoch 1720, training loss: 2.4166808128356934, val loss: 2.640467882156372, val_acc: 0.2290909090909091\n",
      "Epoch 1730, training loss: 2.4152581691741943, val loss: 2.648754358291626, val_acc: 0.23030303030303031\n",
      "Epoch 1740, training loss: 2.3827712535858154, val loss: 2.6350739002227783, val_acc: 0.22545454545454546\n",
      "Epoch 1750, training loss: 2.383185386657715, val loss: 2.6752099990844727, val_acc: 0.22181818181818183\n",
      "Epoch 1760, training loss: 2.3880417346954346, val loss: 2.7187933921813965, val_acc: 0.21696969696969698\n",
      "Epoch 1770, training loss: 2.4023280143737793, val loss: 2.7117230892181396, val_acc: 0.21818181818181817\n",
      "Epoch 1780, training loss: 2.3924131393432617, val loss: 2.656219720840454, val_acc: 0.21696969696969698\n",
      "Epoch 1790, training loss: 2.3887338638305664, val loss: 2.6426146030426025, val_acc: 0.21575757575757576\n",
      "Epoch 1800, training loss: 2.387507915496826, val loss: 2.7074193954467773, val_acc: 0.21212121212121213\n",
      "Epoch 1810, training loss: 2.4072790145874023, val loss: 2.6617136001586914, val_acc: 0.23272727272727273\n",
      "Epoch 1820, training loss: 2.3736939430236816, val loss: 2.6396260261535645, val_acc: 0.21575757575757576\n",
      "Epoch 1830, training loss: 2.382246494293213, val loss: 2.652738571166992, val_acc: 0.22787878787878788\n",
      "Epoch 1840, training loss: 2.3780362606048584, val loss: 2.6362979412078857, val_acc: 0.21696969696969698\n",
      "Epoch 1850, training loss: 2.366224527359009, val loss: 2.685314893722534, val_acc: 0.22181818181818183\n",
      "Epoch 1860, training loss: 2.3727757930755615, val loss: 2.7119998931884766, val_acc: 0.2206060606060606\n",
      "Epoch 1870, training loss: 2.3870668411254883, val loss: 2.674612522125244, val_acc: 0.20363636363636364\n",
      "Epoch 1880, training loss: 2.3702175617218018, val loss: 2.7483372688293457, val_acc: 0.20727272727272728\n",
      "Epoch 1890, training loss: 2.3693976402282715, val loss: 2.6708433628082275, val_acc: 0.2206060606060606\n",
      "Epoch 1900, training loss: 2.385376453399658, val loss: 2.6648499965667725, val_acc: 0.22545454545454546\n",
      "Epoch 1910, training loss: 2.3777098655700684, val loss: 2.679447889328003, val_acc: 0.22424242424242424\n",
      "Epoch 1920, training loss: 2.3885929584503174, val loss: 2.6610944271087646, val_acc: 0.22181818181818183\n",
      "Epoch 1930, training loss: 2.3881888389587402, val loss: 2.6669163703918457, val_acc: 0.21818181818181817\n",
      "Epoch 1940, training loss: 2.358487129211426, val loss: 2.6807007789611816, val_acc: 0.22666666666666666\n",
      "Epoch 1950, training loss: 2.3543574810028076, val loss: 2.6525216102600098, val_acc: 0.22787878787878788\n",
      "Epoch 1960, training loss: 2.3937389850616455, val loss: 2.6593480110168457, val_acc: 0.21333333333333335\n",
      "Epoch 1970, training loss: 2.3671202659606934, val loss: 2.683396577835083, val_acc: 0.22424242424242424\n",
      "Epoch 1980, training loss: 2.346627950668335, val loss: 2.6321489810943604, val_acc: 0.2290909090909091\n",
      "Epoch 1990, training loss: 2.3630173206329346, val loss: 2.703458547592163, val_acc: 0.21575757575757576\n",
      "Epoch 2000, training loss: 2.383960008621216, val loss: 2.668215036392212, val_acc: 0.21575757575757576\n",
      "Epoch 2010, training loss: 2.331584930419922, val loss: 2.627300262451172, val_acc: 0.23030303030303031\n",
      "Epoch 2020, training loss: 2.346402645111084, val loss: 2.620203971862793, val_acc: 0.23515151515151514\n",
      "Epoch 2030, training loss: 2.337541103363037, val loss: 2.6838579177856445, val_acc: 0.22787878787878788\n",
      "Epoch 2040, training loss: 2.357407808303833, val loss: 2.627120018005371, val_acc: 0.2109090909090909\n",
      "Epoch 2050, training loss: 2.3477542400360107, val loss: 2.6956863403320312, val_acc: 0.2290909090909091\n",
      "Epoch 2060, training loss: 2.3883817195892334, val loss: 2.6467370986938477, val_acc: 0.22424242424242424\n",
      "Epoch 2070, training loss: 2.336730480194092, val loss: 2.6863772869110107, val_acc: 0.22181818181818183\n",
      "Epoch 2080, training loss: 2.3235676288604736, val loss: 2.6670632362365723, val_acc: 0.22545454545454546\n",
      "Epoch 2090, training loss: 2.3422470092773438, val loss: 2.6411468982696533, val_acc: 0.22181818181818183\n",
      "Epoch 2100, training loss: 2.3217062950134277, val loss: 2.6361496448516846, val_acc: 0.22545454545454546\n",
      "Epoch 2110, training loss: 2.3625285625457764, val loss: 2.6200175285339355, val_acc: 0.22666666666666666\n",
      "Epoch 2120, training loss: 2.3487489223480225, val loss: 2.6361382007598877, val_acc: 0.23393939393939395\n",
      "Epoch 2130, training loss: 2.356783151626587, val loss: 2.671147346496582, val_acc: 0.2193939393939394\n",
      "Epoch 2140, training loss: 2.3223659992218018, val loss: 2.6635255813598633, val_acc: 0.22787878787878788\n",
      "Epoch 2150, training loss: 2.36706280708313, val loss: 2.6522767543792725, val_acc: 0.22666666666666666\n",
      "Epoch 2160, training loss: 2.330050230026245, val loss: 2.694960594177246, val_acc: 0.21818181818181817\n",
      "Epoch 2170, training loss: 2.3248534202575684, val loss: 2.6651813983917236, val_acc: 0.23272727272727273\n",
      "Epoch 2180, training loss: 2.3369011878967285, val loss: 2.6507086753845215, val_acc: 0.23272727272727273\n",
      "Epoch 2190, training loss: 2.320427179336548, val loss: 2.604144811630249, val_acc: 0.2315151515151515\n",
      "Epoch 2200, training loss: 2.3496780395507812, val loss: 2.639251708984375, val_acc: 0.2206060606060606\n",
      "Epoch 2210, training loss: 2.3249406814575195, val loss: 2.693727493286133, val_acc: 0.22303030303030302\n",
      "Epoch 2220, training loss: 2.3341596126556396, val loss: 2.6740517616271973, val_acc: 0.23515151515151514\n",
      "Epoch 2230, training loss: 2.292983055114746, val loss: 2.6934523582458496, val_acc: 0.22303030303030302\n",
      "Epoch 2240, training loss: 2.3404834270477295, val loss: 2.7125823497772217, val_acc: 0.2206060606060606\n",
      "Epoch 2250, training loss: 2.3526453971862793, val loss: 2.6858978271484375, val_acc: 0.23272727272727273\n",
      "Epoch 2260, training loss: 2.29331636428833, val loss: 2.629408359527588, val_acc: 0.24242424242424243\n",
      "Epoch 2270, training loss: 2.341426372528076, val loss: 2.6468558311462402, val_acc: 0.23030303030303031\n",
      "Epoch 2280, training loss: 2.3343889713287354, val loss: 2.657959461212158, val_acc: 0.23030303030303031\n",
      "Epoch 2290, training loss: 2.343571186065674, val loss: 2.6874284744262695, val_acc: 0.22303030303030302\n",
      "Epoch 2300, training loss: 2.3140363693237305, val loss: 2.6776154041290283, val_acc: 0.22545454545454546\n",
      "Epoch 2310, training loss: 2.3301048278808594, val loss: 2.637795925140381, val_acc: 0.2387878787878788\n",
      "Epoch 2320, training loss: 2.30102801322937, val loss: 2.6734485626220703, val_acc: 0.23030303030303031\n",
      "Epoch 2330, training loss: 2.2837085723876953, val loss: 2.6583869457244873, val_acc: 0.22787878787878788\n",
      "Epoch 2340, training loss: 2.3383901119232178, val loss: 2.7182421684265137, val_acc: 0.21454545454545454\n",
      "Epoch 2350, training loss: 2.354449987411499, val loss: 2.6801373958587646, val_acc: 0.22666666666666666\n",
      "Epoch 2360, training loss: 2.3530867099761963, val loss: 2.674820899963379, val_acc: 0.22303030303030302\n",
      "Epoch 2370, training loss: 2.372474431991577, val loss: 2.7223541736602783, val_acc: 0.20606060606060606\n",
      "Epoch 2380, training loss: 2.356768846511841, val loss: 2.669123649597168, val_acc: 0.22303030303030302\n",
      "Epoch 2390, training loss: 2.328824758529663, val loss: 2.6594841480255127, val_acc: 0.2290909090909091\n",
      "Epoch 2400, training loss: 2.314608097076416, val loss: 2.675330638885498, val_acc: 0.23636363636363636\n",
      "Epoch 2410, training loss: 2.3204946517944336, val loss: 2.650296449661255, val_acc: 0.23636363636363636\n",
      "Epoch 2420, training loss: 2.3330564498901367, val loss: 2.624986410140991, val_acc: 0.23030303030303031\n",
      "Epoch 2430, training loss: 2.331608295440674, val loss: 2.674288034439087, val_acc: 0.21454545454545454\n",
      "Epoch 2440, training loss: 2.330392837524414, val loss: 2.7565300464630127, val_acc: 0.20363636363636364\n",
      "Epoch 2450, training loss: 2.34059739112854, val loss: 2.6918561458587646, val_acc: 0.22424242424242424\n",
      "Epoch 2460, training loss: 2.335145950317383, val loss: 2.652026891708374, val_acc: 0.24\n",
      "Epoch 2470, training loss: 2.326641082763672, val loss: 2.639115571975708, val_acc: 0.23030303030303031\n",
      "Epoch 2480, training loss: 2.295328140258789, val loss: 2.6523513793945312, val_acc: 0.23030303030303031\n",
      "Epoch 2490, training loss: 2.327177047729492, val loss: 2.6651155948638916, val_acc: 0.24363636363636362\n",
      "Epoch 2500, training loss: 2.3034169673919678, val loss: 2.653048276901245, val_acc: 0.23757575757575758\n",
      "Epoch 2510, training loss: 2.318610429763794, val loss: 2.666347026824951, val_acc: 0.23636363636363636\n",
      "Epoch 2520, training loss: 2.332864284515381, val loss: 2.6252331733703613, val_acc: 0.23272727272727273\n",
      "Epoch 2530, training loss: 2.313095808029175, val loss: 2.647536039352417, val_acc: 0.2315151515151515\n",
      "Epoch 2540, training loss: 2.3185062408447266, val loss: 2.6520681381225586, val_acc: 0.24242424242424243\n",
      "Epoch 2550, training loss: 2.3328986167907715, val loss: 2.62391996383667, val_acc: 0.23272727272727273\n",
      "Epoch 2560, training loss: 2.2990450859069824, val loss: 2.647348403930664, val_acc: 0.23636363636363636\n",
      "Epoch 2570, training loss: 2.312220335006714, val loss: 2.6317930221557617, val_acc: 0.2193939393939394\n",
      "Epoch 2580, training loss: 2.2874929904937744, val loss: 2.6436290740966797, val_acc: 0.23757575757575758\n",
      "Epoch 2590, training loss: 2.3472864627838135, val loss: 2.614386558532715, val_acc: 0.24242424242424243\n",
      "Epoch 2600, training loss: 2.316741704940796, val loss: 2.6131184101104736, val_acc: 0.23636363636363636\n",
      "Epoch 2610, training loss: 2.289252281188965, val loss: 2.6174590587615967, val_acc: 0.24363636363636362\n",
      "Epoch 2620, training loss: 2.3109347820281982, val loss: 2.7155063152313232, val_acc: 0.22787878787878788\n",
      "Epoch 2630, training loss: 2.3056914806365967, val loss: 2.6559605598449707, val_acc: 0.22787878787878788\n",
      "Epoch 2640, training loss: 2.3424410820007324, val loss: 2.6134636402130127, val_acc: 0.2387878787878788\n",
      "Epoch 2650, training loss: 2.296704053878784, val loss: 2.613865613937378, val_acc: 0.22181818181818183\n",
      "Epoch 2660, training loss: 2.3036086559295654, val loss: 2.7372889518737793, val_acc: 0.2315151515151515\n",
      "Epoch 2670, training loss: 2.2758355140686035, val loss: 2.6555752754211426, val_acc: 0.2290909090909091\n",
      "Epoch 2680, training loss: 2.2857930660247803, val loss: 2.6543099880218506, val_acc: 0.23636363636363636\n",
      "Epoch 2690, training loss: 2.274243116378784, val loss: 2.7299349308013916, val_acc: 0.2193939393939394\n",
      "Epoch 2700, training loss: 2.294609785079956, val loss: 2.6684224605560303, val_acc: 0.22303030303030302\n",
      "Epoch 2710, training loss: 2.3119866847991943, val loss: 2.67065167427063, val_acc: 0.22545454545454546\n",
      "Epoch 2720, training loss: 2.3080577850341797, val loss: 2.6622393131256104, val_acc: 0.23757575757575758\n",
      "Epoch 2730, training loss: 2.2958199977874756, val loss: 2.6869311332702637, val_acc: 0.22303030303030302\n",
      "Epoch 2740, training loss: 2.3411865234375, val loss: 2.677239179611206, val_acc: 0.2206060606060606\n",
      "Epoch 2750, training loss: 2.2804155349731445, val loss: 2.6477949619293213, val_acc: 0.2412121212121212\n",
      "Epoch 2760, training loss: 2.28974986076355, val loss: 2.6734278202056885, val_acc: 0.22181818181818183\n",
      "Epoch 2770, training loss: 2.3131868839263916, val loss: 2.638061046600342, val_acc: 0.23515151515151514\n",
      "Epoch 2780, training loss: 2.3089725971221924, val loss: 2.6343047618865967, val_acc: 0.24\n",
      "Epoch 2790, training loss: 2.2858169078826904, val loss: 2.6175665855407715, val_acc: 0.24242424242424243\n",
      "Epoch 2800, training loss: 2.264397382736206, val loss: 2.664837121963501, val_acc: 0.23272727272727273\n",
      "Epoch 2810, training loss: 2.3065643310546875, val loss: 2.6756374835968018, val_acc: 0.2206060606060606\n",
      "Epoch 2820, training loss: 2.331590414047241, val loss: 2.6475369930267334, val_acc: 0.22424242424242424\n",
      "Epoch 2830, training loss: 2.259706974029541, val loss: 2.6604814529418945, val_acc: 0.22424242424242424\n",
      "Epoch 2840, training loss: 2.3088955879211426, val loss: 2.64620304107666, val_acc: 0.2387878787878788\n",
      "Epoch 2850, training loss: 2.27125883102417, val loss: 2.657287120819092, val_acc: 0.24242424242424243\n",
      "Epoch 2860, training loss: 2.3290796279907227, val loss: 2.6074905395507812, val_acc: 0.24727272727272728\n",
      "Epoch 2870, training loss: 2.281050443649292, val loss: 2.6172871589660645, val_acc: 0.24242424242424243\n",
      "Epoch 2880, training loss: 2.28574275970459, val loss: 2.6232144832611084, val_acc: 0.24484848484848484\n",
      "Epoch 2890, training loss: 2.323991298675537, val loss: 2.6515066623687744, val_acc: 0.22545454545454546\n",
      "Epoch 2900, training loss: 2.3196663856506348, val loss: 2.7269797325134277, val_acc: 0.23636363636363636\n",
      "Epoch 2910, training loss: 2.3003785610198975, val loss: 2.615466356277466, val_acc: 0.2193939393939394\n",
      "Epoch 2920, training loss: 2.285367012023926, val loss: 2.6973817348480225, val_acc: 0.23272727272727273\n",
      "Epoch 2930, training loss: 2.270564079284668, val loss: 2.6083226203918457, val_acc: 0.23515151515151514\n",
      "Epoch 2940, training loss: 2.2661235332489014, val loss: 2.64513897895813, val_acc: 0.24\n",
      "Epoch 2950, training loss: 2.2795729637145996, val loss: 2.6549623012542725, val_acc: 0.2290909090909091\n",
      "Epoch 2960, training loss: 2.291597604751587, val loss: 2.668792247772217, val_acc: 0.24242424242424243\n",
      "Epoch 2970, training loss: 2.2941300868988037, val loss: 2.604539155960083, val_acc: 0.2315151515151515\n",
      "Epoch 2980, training loss: 2.255004405975342, val loss: 2.6624913215637207, val_acc: 0.23515151515151514\n",
      "Epoch 2990, training loss: 2.2985305786132812, val loss: 2.6473593711853027, val_acc: 0.23272727272727273\n",
      "Epoch 3000, training loss: 2.303337812423706, val loss: 2.60863995552063, val_acc: 0.23757575757575758\n",
      "Epoch 3010, training loss: 2.2993431091308594, val loss: 2.6385457515716553, val_acc: 0.2387878787878788\n",
      "Epoch 3020, training loss: 2.3063313961029053, val loss: 2.6568620204925537, val_acc: 0.22424242424242424\n",
      "Epoch 3030, training loss: 2.2717294692993164, val loss: 2.609952926635742, val_acc: 0.2315151515151515\n",
      "Epoch 3040, training loss: 2.274385929107666, val loss: 2.622095823287964, val_acc: 0.24727272727272728\n",
      "Epoch 3050, training loss: 2.2990124225616455, val loss: 2.659442663192749, val_acc: 0.2387878787878788\n",
      "Epoch 3060, training loss: 2.2669849395751953, val loss: 2.6583616733551025, val_acc: 0.24\n",
      "Epoch 3070, training loss: 2.2408406734466553, val loss: 2.6813721656799316, val_acc: 0.2084848484848485\n",
      "Epoch 3080, training loss: 2.278223752975464, val loss: 2.6233532428741455, val_acc: 0.23393939393939395\n",
      "Epoch 3090, training loss: 2.315721035003662, val loss: 2.6658661365509033, val_acc: 0.2290909090909091\n",
      "Epoch 3100, training loss: 2.2783124446868896, val loss: 2.6671948432922363, val_acc: 0.22424242424242424\n",
      "Epoch 3110, training loss: 2.2636806964874268, val loss: 2.672468662261963, val_acc: 0.24363636363636362\n",
      "Epoch 3120, training loss: 2.2753121852874756, val loss: 2.6150062084198, val_acc: 0.25575757575757574\n",
      "Epoch 3130, training loss: 2.256427764892578, val loss: 2.5924155712127686, val_acc: 0.24242424242424243\n",
      "Epoch 3140, training loss: 2.244324207305908, val loss: 2.6956632137298584, val_acc: 0.23757575757575758\n",
      "Epoch 3150, training loss: 2.2412564754486084, val loss: 2.632134437561035, val_acc: 0.24727272727272728\n",
      "Epoch 3160, training loss: 2.2722225189208984, val loss: 2.6290841102600098, val_acc: 0.24242424242424243\n",
      "Epoch 3170, training loss: 2.261249303817749, val loss: 2.661181688308716, val_acc: 0.23272727272727273\n",
      "Epoch 3180, training loss: 2.273529052734375, val loss: 2.6137826442718506, val_acc: 0.23757575757575758\n",
      "Epoch 3190, training loss: 2.284491777420044, val loss: 2.696532964706421, val_acc: 0.23272727272727273\n",
      "Epoch 3200, training loss: 2.258464813232422, val loss: 2.6128411293029785, val_acc: 0.22545454545454546\n",
      "Epoch 3210, training loss: 2.2779898643493652, val loss: 2.684811592102051, val_acc: 0.22303030303030302\n",
      "Epoch 3220, training loss: 2.269390106201172, val loss: 2.6634395122528076, val_acc: 0.23515151515151514\n",
      "Epoch 3230, training loss: 2.277902841567993, val loss: 2.679844379425049, val_acc: 0.2315151515151515\n",
      "Epoch 3240, training loss: 2.2583987712860107, val loss: 2.6406266689300537, val_acc: 0.22666666666666666\n",
      "Epoch 3250, training loss: 2.2557599544525146, val loss: 2.63767409324646, val_acc: 0.23393939393939395\n",
      "Epoch 3260, training loss: 2.246324300765991, val loss: 2.6693038940429688, val_acc: 0.23636363636363636\n",
      "Epoch 3270, training loss: 2.255678415298462, val loss: 2.6197874546051025, val_acc: 0.24848484848484848\n",
      "Epoch 3280, training loss: 2.328972816467285, val loss: 2.6404223442077637, val_acc: 0.23272727272727273\n",
      "Epoch 3290, training loss: 2.3196864128112793, val loss: 2.6511096954345703, val_acc: 0.2290909090909091\n",
      "Epoch 3300, training loss: 2.349020481109619, val loss: 2.6148948669433594, val_acc: 0.23030303030303031\n",
      "Epoch 3310, training loss: 2.2767794132232666, val loss: 2.629962921142578, val_acc: 0.23515151515151514\n",
      "Epoch 3320, training loss: 2.311875343322754, val loss: 2.6431634426116943, val_acc: 0.23030303030303031\n",
      "Epoch 3330, training loss: 2.24310564994812, val loss: 2.6217944622039795, val_acc: 0.2193939393939394\n",
      "Epoch 3340, training loss: 2.2749767303466797, val loss: 2.7013156414031982, val_acc: 0.2206060606060606\n",
      "Epoch 3350, training loss: 2.254728317260742, val loss: 2.646329879760742, val_acc: 0.22787878787878788\n",
      "Epoch 3360, training loss: 2.2272794246673584, val loss: 2.647906541824341, val_acc: 0.23393939393939395\n",
      "Epoch 3370, training loss: 2.27176833152771, val loss: 2.6447458267211914, val_acc: 0.2412121212121212\n",
      "Epoch 3380, training loss: 2.230137586593628, val loss: 2.6575675010681152, val_acc: 0.23757575757575758\n",
      "Epoch 3390, training loss: 2.2524871826171875, val loss: 2.6408238410949707, val_acc: 0.23515151515151514\n",
      "Epoch 3400, training loss: 2.2487237453460693, val loss: 2.6631710529327393, val_acc: 0.23636363636363636\n",
      "Epoch 3410, training loss: 2.261521816253662, val loss: 2.6793901920318604, val_acc: 0.22666666666666666\n",
      "Epoch 3420, training loss: 2.2817769050598145, val loss: 2.615485429763794, val_acc: 0.2496969696969697\n",
      "Epoch 3430, training loss: 2.2490756511688232, val loss: 2.6663577556610107, val_acc: 0.23272727272727273\n",
      "Epoch 3440, training loss: 2.2559261322021484, val loss: 2.658968448638916, val_acc: 0.23393939393939395\n",
      "Epoch 3450, training loss: 2.261911392211914, val loss: 2.614393949508667, val_acc: 0.24727272727272728\n",
      "Epoch 3460, training loss: 2.2695767879486084, val loss: 2.630141496658325, val_acc: 0.2581818181818182\n",
      "Epoch 3470, training loss: 2.260059356689453, val loss: 2.6065213680267334, val_acc: 0.2315151515151515\n",
      "Epoch 3480, training loss: 2.239361047744751, val loss: 2.624768018722534, val_acc: 0.23515151515151514\n",
      "Epoch 3490, training loss: 2.2435050010681152, val loss: 2.684265613555908, val_acc: 0.23030303030303031\n",
      "Epoch 3500, training loss: 2.250023603439331, val loss: 2.5990054607391357, val_acc: 0.2315151515151515\n",
      "Epoch 3510, training loss: 2.2596869468688965, val loss: 2.6484408378601074, val_acc: 0.2315151515151515\n",
      "Epoch 3520, training loss: 2.2877352237701416, val loss: 2.6084632873535156, val_acc: 0.23636363636363636\n",
      "Epoch 3530, training loss: 2.2459230422973633, val loss: 2.6191565990448, val_acc: 0.24484848484848484\n",
      "Epoch 3540, training loss: 2.2354369163513184, val loss: 2.6927332878112793, val_acc: 0.2193939393939394\n",
      "Epoch 3550, training loss: 2.255760908126831, val loss: 2.641530752182007, val_acc: 0.22787878787878788\n",
      "Epoch 3560, training loss: 2.2572860717773438, val loss: 2.797294855117798, val_acc: 0.2206060606060606\n",
      "Epoch 3570, training loss: 2.2542724609375, val loss: 2.5947375297546387, val_acc: 0.24484848484848484\n",
      "Epoch 3580, training loss: 2.2081527709960938, val loss: 2.630767345428467, val_acc: 0.23393939393939395\n",
      "Epoch 3590, training loss: 2.261496067047119, val loss: 2.637469530105591, val_acc: 0.24606060606060606\n",
      "Epoch 3600, training loss: 2.257894515991211, val loss: 2.679565668106079, val_acc: 0.23757575757575758\n",
      "Epoch 3610, training loss: 2.252253293991089, val loss: 2.59879732131958, val_acc: 0.25575757575757574\n",
      "Epoch 3620, training loss: 2.263977289199829, val loss: 2.6368188858032227, val_acc: 0.24\n",
      "Epoch 3630, training loss: 2.281237840652466, val loss: 2.6675968170166016, val_acc: 0.2387878787878788\n",
      "Epoch 3640, training loss: 2.3047282695770264, val loss: 2.6203339099884033, val_acc: 0.24606060606060606\n",
      "Epoch 3650, training loss: 2.2458643913269043, val loss: 2.683518886566162, val_acc: 0.2290909090909091\n",
      "Epoch 3660, training loss: 2.2310588359832764, val loss: 2.6312878131866455, val_acc: 0.24242424242424243\n",
      "Epoch 3670, training loss: 2.2224137783050537, val loss: 2.627318859100342, val_acc: 0.2545454545454545\n",
      "Epoch 3680, training loss: 2.2742440700531006, val loss: 2.574293613433838, val_acc: 0.2509090909090909\n",
      "Epoch 3690, training loss: 2.283155679702759, val loss: 2.605135440826416, val_acc: 0.23515151515151514\n",
      "Epoch 3700, training loss: 2.2525415420532227, val loss: 2.6561124324798584, val_acc: 0.24484848484848484\n",
      "Epoch 3710, training loss: 2.246561050415039, val loss: 2.6194634437561035, val_acc: 0.2387878787878788\n",
      "Epoch 3720, training loss: 2.238368272781372, val loss: 2.67501163482666, val_acc: 0.2315151515151515\n",
      "Epoch 3730, training loss: 2.251842975616455, val loss: 2.6263015270233154, val_acc: 0.24484848484848484\n",
      "Epoch 3740, training loss: 2.2467124462127686, val loss: 2.6587154865264893, val_acc: 0.23393939393939395\n",
      "Epoch 3750, training loss: 2.224437952041626, val loss: 2.649291515350342, val_acc: 0.23636363636363636\n",
      "Epoch 3760, training loss: 2.2123544216156006, val loss: 2.6172749996185303, val_acc: 0.24242424242424243\n",
      "Epoch 3770, training loss: 2.247908592224121, val loss: 2.588313102722168, val_acc: 0.24\n",
      "Epoch 3780, training loss: 2.221238374710083, val loss: 2.6414287090301514, val_acc: 0.23393939393939395\n",
      "Epoch 3790, training loss: 2.2386648654937744, val loss: 2.6386284828186035, val_acc: 0.22181818181818183\n",
      "Epoch 3800, training loss: 2.219367742538452, val loss: 2.67997670173645, val_acc: 0.23636363636363636\n",
      "Epoch 3810, training loss: 2.2236428260803223, val loss: 2.7004261016845703, val_acc: 0.25212121212121213\n",
      "Epoch 3820, training loss: 2.222550868988037, val loss: 2.594574451446533, val_acc: 0.25212121212121213\n",
      "Epoch 3830, training loss: 2.236443281173706, val loss: 2.6449363231658936, val_acc: 0.263030303030303\n",
      "Epoch 3840, training loss: 2.2238450050354004, val loss: 2.625121831893921, val_acc: 0.22666666666666666\n",
      "Epoch 3850, training loss: 2.2295448780059814, val loss: 2.6149141788482666, val_acc: 0.25333333333333335\n",
      "Epoch 3860, training loss: 2.2140636444091797, val loss: 2.628124713897705, val_acc: 0.25212121212121213\n",
      "Epoch 3870, training loss: 2.221303701400757, val loss: 2.6126458644866943, val_acc: 0.23272727272727273\n",
      "Epoch 3880, training loss: 2.219916582107544, val loss: 2.6068308353424072, val_acc: 0.2545454545454545\n",
      "Epoch 3890, training loss: 2.22640323638916, val loss: 2.6492767333984375, val_acc: 0.24606060606060606\n",
      "Epoch 3900, training loss: 2.2306928634643555, val loss: 2.8232014179229736, val_acc: 0.2096969696969697\n",
      "Epoch 3910, training loss: 2.236523151397705, val loss: 2.592911958694458, val_acc: 0.25575757575757574\n",
      "Epoch 3920, training loss: 2.250391960144043, val loss: 2.582841396331787, val_acc: 0.25212121212121213\n",
      "Epoch 3930, training loss: 2.2571310997009277, val loss: 2.660907506942749, val_acc: 0.24363636363636362\n",
      "Epoch 3940, training loss: 2.2100400924682617, val loss: 2.6196985244750977, val_acc: 0.24606060606060606\n",
      "Epoch 3950, training loss: 2.210969924926758, val loss: 2.6295034885406494, val_acc: 0.24606060606060606\n",
      "Epoch 3960, training loss: 2.194213390350342, val loss: 2.6425204277038574, val_acc: 0.24242424242424243\n",
      "Epoch 3970, training loss: 2.227526903152466, val loss: 2.6542441844940186, val_acc: 0.24\n",
      "Epoch 3980, training loss: 2.2334821224212646, val loss: 2.6068503856658936, val_acc: 0.24484848484848484\n",
      "Epoch 3990, training loss: 2.2202506065368652, val loss: 2.574537992477417, val_acc: 0.25333333333333335\n",
      "Epoch 4000, training loss: 2.229557991027832, val loss: 2.6666643619537354, val_acc: 0.23393939393939395\n",
      "Epoch 4010, training loss: 2.232379913330078, val loss: 2.715561628341675, val_acc: 0.23636363636363636\n",
      "Epoch 4020, training loss: 2.2451634407043457, val loss: 2.614161968231201, val_acc: 0.2387878787878788\n",
      "Epoch 4030, training loss: 2.2407166957855225, val loss: 2.6180901527404785, val_acc: 0.24\n",
      "Epoch 4040, training loss: 2.2211077213287354, val loss: 2.6542608737945557, val_acc: 0.22424242424242424\n",
      "Epoch 4050, training loss: 2.2083005905151367, val loss: 2.701841354370117, val_acc: 0.22303030303030302\n",
      "Epoch 4060, training loss: 2.1924569606781006, val loss: 2.6329782009124756, val_acc: 0.24606060606060606\n",
      "Epoch 4070, training loss: 2.2076070308685303, val loss: 2.6542391777038574, val_acc: 0.2387878787878788\n",
      "Epoch 4080, training loss: 2.229595422744751, val loss: 2.6164917945861816, val_acc: 0.23757575757575758\n",
      "Epoch 4090, training loss: 2.276815891265869, val loss: 2.588521957397461, val_acc: 0.2545454545454545\n",
      "Epoch 4100, training loss: 2.24147629737854, val loss: 2.7049598693847656, val_acc: 0.21818181818181817\n",
      "Epoch 4110, training loss: 2.2120864391326904, val loss: 2.662256956100464, val_acc: 0.24484848484848484\n",
      "Epoch 4120, training loss: 2.242764949798584, val loss: 2.6486573219299316, val_acc: 0.23393939393939395\n",
      "Epoch 4130, training loss: 2.209723472595215, val loss: 2.629201650619507, val_acc: 0.25212121212121213\n",
      "Epoch 4140, training loss: 2.2542905807495117, val loss: 2.682983875274658, val_acc: 0.23272727272727273\n",
      "Epoch 4150, training loss: 2.210008382797241, val loss: 2.6123087406158447, val_acc: 0.23515151515151514\n",
      "Epoch 4160, training loss: 2.225712776184082, val loss: 2.6262526512145996, val_acc: 0.24242424242424243\n",
      "Epoch 4170, training loss: 2.2021071910858154, val loss: 2.605433225631714, val_acc: 0.2509090909090909\n",
      "Epoch 4180, training loss: 2.218979835510254, val loss: 2.6326582431793213, val_acc: 0.2593939393939394\n",
      "Epoch 4190, training loss: 2.2541298866271973, val loss: 2.6176185607910156, val_acc: 0.24363636363636362\n",
      "Epoch 4200, training loss: 2.245074987411499, val loss: 2.657862424850464, val_acc: 0.24727272727272728\n",
      "Epoch 4210, training loss: 2.2537450790405273, val loss: 2.6201236248016357, val_acc: 0.24242424242424243\n",
      "Epoch 4220, training loss: 2.178556442260742, val loss: 2.65466046333313, val_acc: 0.23515151515151514\n",
      "Epoch 4230, training loss: 2.2289559841156006, val loss: 2.6537952423095703, val_acc: 0.25575757575757574\n",
      "Epoch 4240, training loss: 2.2245683670043945, val loss: 2.6156673431396484, val_acc: 0.25575757575757574\n",
      "Epoch 4250, training loss: 2.218202590942383, val loss: 2.5989675521850586, val_acc: 0.2703030303030303\n",
      "Epoch 4260, training loss: 2.218693733215332, val loss: 2.5810508728027344, val_acc: 0.25575757575757574\n",
      "Epoch 4270, training loss: 2.2069499492645264, val loss: 2.5898232460021973, val_acc: 0.24727272727272728\n",
      "Epoch 4280, training loss: 2.2284083366394043, val loss: 2.6662614345550537, val_acc: 0.23757575757575758\n",
      "Epoch 4290, training loss: 2.2420918941497803, val loss: 2.619788408279419, val_acc: 0.26666666666666666\n",
      "Epoch 4300, training loss: 2.229527235031128, val loss: 2.5918283462524414, val_acc: 0.2496969696969697\n",
      "Epoch 4310, training loss: 2.2322802543640137, val loss: 2.6309781074523926, val_acc: 0.23515151515151514\n",
      "Epoch 4320, training loss: 2.250112295150757, val loss: 2.6482763290405273, val_acc: 0.25575757575757574\n",
      "Epoch 4330, training loss: 2.24141526222229, val loss: 2.639092445373535, val_acc: 0.22545454545454546\n",
      "Epoch 4340, training loss: 2.2270660400390625, val loss: 2.609177827835083, val_acc: 0.2412121212121212\n",
      "Epoch 4350, training loss: 2.244842290878296, val loss: 2.5918633937835693, val_acc: 0.2509090909090909\n",
      "Epoch 4360, training loss: 2.204892873764038, val loss: 2.633925199508667, val_acc: 0.263030303030303\n",
      "Epoch 4370, training loss: 2.2053513526916504, val loss: 2.62898850440979, val_acc: 0.24606060606060606\n",
      "Epoch 4380, training loss: 2.1783041954040527, val loss: 2.6371495723724365, val_acc: 0.26181818181818184\n",
      "Epoch 4390, training loss: 2.1945433616638184, val loss: 2.702970266342163, val_acc: 0.21454545454545454\n",
      "Epoch 4400, training loss: 2.244210720062256, val loss: 2.6559836864471436, val_acc: 0.23636363636363636\n",
      "Epoch 4410, training loss: 2.251070976257324, val loss: 2.589648485183716, val_acc: 0.25696969696969696\n",
      "Epoch 4420, training loss: 2.234297513961792, val loss: 2.6247718334198, val_acc: 0.25333333333333335\n",
      "Epoch 4430, training loss: 2.2222163677215576, val loss: 2.6525588035583496, val_acc: 0.25212121212121213\n",
      "Epoch 4440, training loss: 2.2278456687927246, val loss: 2.6302714347839355, val_acc: 0.25333333333333335\n",
      "Epoch 4450, training loss: 2.197995901107788, val loss: 2.6486966609954834, val_acc: 0.24606060606060606\n",
      "Epoch 4460, training loss: 2.196115016937256, val loss: 2.594876527786255, val_acc: 0.25575757575757574\n",
      "Epoch 4470, training loss: 2.179710626602173, val loss: 2.5874760150909424, val_acc: 0.2545454545454545\n",
      "Epoch 4480, training loss: 2.216657876968384, val loss: 2.6314189434051514, val_acc: 0.23636363636363636\n",
      "Epoch 4490, training loss: 2.171940803527832, val loss: 2.6907835006713867, val_acc: 0.24484848484848484\n",
      "Epoch 4500, training loss: 2.2570157051086426, val loss: 2.6667802333831787, val_acc: 0.21454545454545454\n",
      "Epoch 4510, training loss: 2.2215816974639893, val loss: 2.6551125049591064, val_acc: 0.2412121212121212\n",
      "Epoch 4520, training loss: 2.2399375438690186, val loss: 2.58422589302063, val_acc: 0.2509090909090909\n",
      "Epoch 4530, training loss: 2.217684745788574, val loss: 2.648512601852417, val_acc: 0.2387878787878788\n",
      "Epoch 4540, training loss: 2.202009916305542, val loss: 2.6620898246765137, val_acc: 0.25575757575757574\n",
      "Epoch 4550, training loss: 2.203878164291382, val loss: 2.634018898010254, val_acc: 0.23636363636363636\n",
      "Epoch 4560, training loss: 2.2355289459228516, val loss: 2.653097152709961, val_acc: 0.2593939393939394\n",
      "Epoch 4570, training loss: 2.2142090797424316, val loss: 2.6361348628997803, val_acc: 0.24\n",
      "Epoch 4580, training loss: 2.2146873474121094, val loss: 2.7098147869110107, val_acc: 0.24727272727272728\n",
      "Epoch 4590, training loss: 2.185990571975708, val loss: 2.6297290325164795, val_acc: 0.24363636363636362\n",
      "Epoch 4600, training loss: 2.1862354278564453, val loss: 2.6353883743286133, val_acc: 0.2581818181818182\n",
      "Epoch 4610, training loss: 2.1756391525268555, val loss: 2.616133689880371, val_acc: 0.24848484848484848\n",
      "Epoch 4620, training loss: 2.204827308654785, val loss: 2.6631555557250977, val_acc: 0.24242424242424243\n",
      "Epoch 4630, training loss: 2.2374227046966553, val loss: 2.603900909423828, val_acc: 0.24606060606060606\n",
      "Epoch 4640, training loss: 2.242286205291748, val loss: 2.6771671772003174, val_acc: 0.23515151515151514\n",
      "Epoch 4650, training loss: 2.2615389823913574, val loss: 2.5902016162872314, val_acc: 0.25696969696969696\n",
      "Epoch 4660, training loss: 2.2216200828552246, val loss: 2.5983638763427734, val_acc: 0.2545454545454545\n",
      "Epoch 4670, training loss: 2.186084508895874, val loss: 2.6278951168060303, val_acc: 0.25212121212121213\n",
      "Epoch 4680, training loss: 2.1921372413635254, val loss: 2.600907325744629, val_acc: 0.24484848484848484\n",
      "Epoch 4690, training loss: 2.203260660171509, val loss: 2.6129586696624756, val_acc: 0.2496969696969697\n",
      "Epoch 4700, training loss: 2.1979072093963623, val loss: 2.6309452056884766, val_acc: 0.26181818181818184\n",
      "Epoch 4710, training loss: 2.225175142288208, val loss: 2.663121461868286, val_acc: 0.23757575757575758\n",
      "Epoch 4720, training loss: 2.1888654232025146, val loss: 2.6346383094787598, val_acc: 0.25212121212121213\n",
      "Epoch 4730, training loss: 2.1987853050231934, val loss: 2.6883411407470703, val_acc: 0.23757575757575758\n",
      "Epoch 4740, training loss: 2.185983657836914, val loss: 2.626347064971924, val_acc: 0.25212121212121213\n",
      "Epoch 4750, training loss: 2.203301429748535, val loss: 2.622419834136963, val_acc: 0.25212121212121213\n",
      "Epoch 4760, training loss: 2.236044406890869, val loss: 2.628051996231079, val_acc: 0.24606060606060606\n",
      "Epoch 4770, training loss: 2.21040940284729, val loss: 2.609511375427246, val_acc: 0.24363636363636362\n",
      "Epoch 4780, training loss: 2.1829426288604736, val loss: 2.6596593856811523, val_acc: 0.25696969696969696\n",
      "Epoch 4790, training loss: 2.197951555252075, val loss: 2.6160049438476562, val_acc: 0.24606060606060606\n",
      "Epoch 4800, training loss: 2.229036569595337, val loss: 2.6119534969329834, val_acc: 0.2593939393939394\n",
      "Epoch 4810, training loss: 2.191201686859131, val loss: 2.6030375957489014, val_acc: 0.26181818181818184\n",
      "Epoch 4820, training loss: 2.23315691947937, val loss: 2.6116859912872314, val_acc: 0.26545454545454544\n",
      "Epoch 4830, training loss: 2.198186159133911, val loss: 2.6812758445739746, val_acc: 0.2412121212121212\n",
      "Epoch 4840, training loss: 2.1924009323120117, val loss: 2.630535125732422, val_acc: 0.26545454545454544\n",
      "Epoch 4850, training loss: 2.194122314453125, val loss: 2.648529291152954, val_acc: 0.23393939393939395\n",
      "Epoch 4860, training loss: 2.195908308029175, val loss: 2.601886510848999, val_acc: 0.26181818181818184\n",
      "Epoch 4870, training loss: 2.18415904045105, val loss: 2.660262107849121, val_acc: 0.2581818181818182\n",
      "Epoch 4880, training loss: 2.192690134048462, val loss: 2.5963916778564453, val_acc: 0.25212121212121213\n",
      "Epoch 4890, training loss: 2.1964094638824463, val loss: 2.5492377281188965, val_acc: 0.2606060606060606\n",
      "Epoch 4900, training loss: 2.2038369178771973, val loss: 2.634748935699463, val_acc: 0.24848484848484848\n",
      "Epoch 4910, training loss: 2.187558174133301, val loss: 2.602557897567749, val_acc: 0.24484848484848484\n",
      "Epoch 4920, training loss: 2.2180466651916504, val loss: 2.6367950439453125, val_acc: 0.24242424242424243\n",
      "Epoch 4930, training loss: 2.2089009284973145, val loss: 2.6604347229003906, val_acc: 0.24848484848484848\n",
      "Epoch 4940, training loss: 2.2454910278320312, val loss: 2.619295358657837, val_acc: 0.25575757575757574\n",
      "Epoch 4950, training loss: 2.1980559825897217, val loss: 2.7088167667388916, val_acc: 0.25333333333333335\n",
      "Epoch 4960, training loss: 2.1738123893737793, val loss: 2.669447422027588, val_acc: 0.24606060606060606\n",
      "Epoch 4970, training loss: 2.1462936401367188, val loss: 2.5923848152160645, val_acc: 0.2496969696969697\n",
      "Epoch 4980, training loss: 2.1956024169921875, val loss: 2.6618382930755615, val_acc: 0.24242424242424243\n",
      "Epoch 4990, training loss: 2.183535575866699, val loss: 2.604538917541504, val_acc: 0.2593939393939394\n",
      "====================\n",
      "Training GCNPro with data_cnn\n",
      "Epoch 0, training loss: 5.504458904266357, val loss: 12.044161796569824, val_acc: 0.06787878787878789\n",
      "Epoch 10, training loss: 4.973062515258789, val loss: 4.449937343597412, val_acc: 0.06666666666666667\n",
      "Epoch 20, training loss: 4.287208557128906, val loss: 3.871809244155884, val_acc: 0.052121212121212124\n",
      "Epoch 30, training loss: 4.289487361907959, val loss: 3.7323877811431885, val_acc: 0.06060606060606061\n",
      "Epoch 40, training loss: 3.975864887237549, val loss: 3.46958065032959, val_acc: 0.06909090909090909\n",
      "Epoch 50, training loss: 3.9930672645568848, val loss: 3.3663549423217773, val_acc: 0.07878787878787878\n",
      "Epoch 60, training loss: 3.6357614994049072, val loss: 3.278581380844116, val_acc: 0.10303030303030303\n",
      "Epoch 70, training loss: 3.449359893798828, val loss: 3.2331418991088867, val_acc: 0.09333333333333334\n",
      "Epoch 80, training loss: 3.4607901573181152, val loss: 3.2518651485443115, val_acc: 0.09454545454545454\n",
      "Epoch 90, training loss: 3.2275447845458984, val loss: 3.2025275230407715, val_acc: 0.10303030303030303\n",
      "Epoch 100, training loss: 3.200033187866211, val loss: 3.059156894683838, val_acc: 0.11515151515151516\n",
      "Epoch 110, training loss: 3.091831922531128, val loss: 3.0095231533050537, val_acc: 0.1296969696969697\n",
      "Epoch 120, training loss: 2.9193360805511475, val loss: 3.012965679168701, val_acc: 0.1406060606060606\n",
      "Epoch 130, training loss: 3.018969774246216, val loss: 2.9742767810821533, val_acc: 0.14909090909090908\n",
      "Epoch 140, training loss: 2.8889684677124023, val loss: 2.9722750186920166, val_acc: 0.16\n",
      "Epoch 150, training loss: 2.9325931072235107, val loss: 2.923156261444092, val_acc: 0.16\n",
      "Epoch 160, training loss: 2.718151569366455, val loss: 2.90608286857605, val_acc: 0.16363636363636364\n",
      "Epoch 170, training loss: 2.7550482749938965, val loss: 2.8637325763702393, val_acc: 0.17939393939393938\n",
      "Epoch 180, training loss: 2.7449121475219727, val loss: 2.90236234664917, val_acc: 0.1781818181818182\n",
      "Epoch 190, training loss: 2.6646676063537598, val loss: 2.8260140419006348, val_acc: 0.19515151515151516\n",
      "Epoch 200, training loss: 2.6437466144561768, val loss: 2.8443822860717773, val_acc: 0.19151515151515153\n",
      "Epoch 210, training loss: 2.615744113922119, val loss: 2.8295857906341553, val_acc: 0.2\n",
      "Epoch 220, training loss: 2.5539679527282715, val loss: 2.779299736022949, val_acc: 0.2012121212121212\n",
      "Epoch 230, training loss: 2.583827257156372, val loss: 2.7782444953918457, val_acc: 0.2096969696969697\n",
      "Epoch 240, training loss: 2.5531067848205566, val loss: 2.7620372772216797, val_acc: 0.21333333333333335\n",
      "Epoch 250, training loss: 2.479290008544922, val loss: 2.7473537921905518, val_acc: 0.21333333333333335\n",
      "Epoch 260, training loss: 2.4913594722747803, val loss: 2.7469422817230225, val_acc: 0.21212121212121213\n",
      "Epoch 270, training loss: 2.443194627761841, val loss: 2.73356556892395, val_acc: 0.21454545454545454\n",
      "Epoch 280, training loss: 2.4829978942871094, val loss: 2.729602575302124, val_acc: 0.2096969696969697\n",
      "Epoch 290, training loss: 2.4098498821258545, val loss: 2.6918234825134277, val_acc: 0.23030303030303031\n",
      "Epoch 300, training loss: 2.43245792388916, val loss: 2.7191147804260254, val_acc: 0.2315151515151515\n",
      "Epoch 310, training loss: 2.4232373237609863, val loss: 2.707214832305908, val_acc: 0.22545454545454546\n",
      "Epoch 320, training loss: 2.392829656600952, val loss: 2.693244457244873, val_acc: 0.23030303030303031\n",
      "Epoch 330, training loss: 2.375551223754883, val loss: 2.6873819828033447, val_acc: 0.22787878787878788\n",
      "Epoch 340, training loss: 2.3537497520446777, val loss: 2.6652231216430664, val_acc: 0.23515151515151514\n",
      "Epoch 350, training loss: 2.3228867053985596, val loss: 2.6783573627471924, val_acc: 0.23636363636363636\n",
      "Epoch 360, training loss: 2.3019118309020996, val loss: 2.6691088676452637, val_acc: 0.23636363636363636\n",
      "Epoch 370, training loss: 2.3440563678741455, val loss: 2.665703535079956, val_acc: 0.23636363636363636\n",
      "Epoch 380, training loss: 2.2540972232818604, val loss: 2.6527674198150635, val_acc: 0.24727272727272728\n",
      "Epoch 390, training loss: 2.2901928424835205, val loss: 2.653186559677124, val_acc: 0.24484848484848484\n",
      "Epoch 400, training loss: 2.2996468544006348, val loss: 2.63785457611084, val_acc: 0.24848484848484848\n",
      "Epoch 410, training loss: 2.2550196647644043, val loss: 2.652522563934326, val_acc: 0.24484848484848484\n",
      "Epoch 420, training loss: 2.2760326862335205, val loss: 2.632974624633789, val_acc: 0.2606060606060606\n",
      "Epoch 430, training loss: 2.2494425773620605, val loss: 2.656996250152588, val_acc: 0.25212121212121213\n",
      "Epoch 440, training loss: 2.208927631378174, val loss: 2.6278254985809326, val_acc: 0.2496969696969697\n",
      "Epoch 450, training loss: 2.2323849201202393, val loss: 2.634209394454956, val_acc: 0.25696969696969696\n",
      "Epoch 460, training loss: 2.2257080078125, val loss: 2.6153550148010254, val_acc: 0.2509090909090909\n",
      "Epoch 470, training loss: 2.188904285430908, val loss: 2.624530553817749, val_acc: 0.2593939393939394\n",
      "Epoch 480, training loss: 2.2212839126586914, val loss: 2.610045909881592, val_acc: 0.2593939393939394\n",
      "Epoch 490, training loss: 2.217650890350342, val loss: 2.6207971572875977, val_acc: 0.25575757575757574\n",
      "Epoch 500, training loss: 2.1701531410217285, val loss: 2.6056995391845703, val_acc: 0.24727272727272728\n",
      "Epoch 510, training loss: 2.184805154800415, val loss: 2.5902795791625977, val_acc: 0.2606060606060606\n",
      "Epoch 520, training loss: 2.1860008239746094, val loss: 2.5997984409332275, val_acc: 0.24727272727272728\n",
      "Epoch 530, training loss: 2.161914825439453, val loss: 2.607792615890503, val_acc: 0.2509090909090909\n",
      "Epoch 540, training loss: 2.1884117126464844, val loss: 2.6017842292785645, val_acc: 0.25575757575757574\n",
      "Epoch 550, training loss: 2.157883405685425, val loss: 2.596221685409546, val_acc: 0.2593939393939394\n",
      "Epoch 560, training loss: 2.157320976257324, val loss: 2.589132308959961, val_acc: 0.26545454545454544\n",
      "Epoch 570, training loss: 2.127694845199585, val loss: 2.5863423347473145, val_acc: 0.26181818181818184\n",
      "Epoch 580, training loss: 2.1311397552490234, val loss: 2.5977957248687744, val_acc: 0.26181818181818184\n",
      "Epoch 590, training loss: 2.1145708560943604, val loss: 2.581967830657959, val_acc: 0.2690909090909091\n",
      "Epoch 600, training loss: 2.090486526489258, val loss: 2.5980889797210693, val_acc: 0.26181818181818184\n",
      "Epoch 610, training loss: 2.1070947647094727, val loss: 2.5916659832000732, val_acc: 0.26181818181818184\n",
      "Epoch 620, training loss: 2.077031373977661, val loss: 2.581050157546997, val_acc: 0.2739393939393939\n",
      "Epoch 630, training loss: 2.0960693359375, val loss: 2.6089601516723633, val_acc: 0.2581818181818182\n",
      "Epoch 640, training loss: 2.0936617851257324, val loss: 2.576455593109131, val_acc: 0.2727272727272727\n",
      "Epoch 650, training loss: 2.0849740505218506, val loss: 2.5811221599578857, val_acc: 0.26666666666666666\n",
      "Epoch 660, training loss: 2.0873706340789795, val loss: 2.5772969722747803, val_acc: 0.2690909090909091\n",
      "Epoch 670, training loss: 2.084554672241211, val loss: 2.59831166267395, val_acc: 0.26666666666666666\n",
      "Epoch 680, training loss: 2.1157848834991455, val loss: 2.580667495727539, val_acc: 0.2678787878787879\n",
      "Epoch 690, training loss: 2.0665106773376465, val loss: 2.592578411102295, val_acc: 0.2642424242424242\n",
      "Epoch 700, training loss: 2.051546573638916, val loss: 2.5869364738464355, val_acc: 0.2678787878787879\n",
      "Epoch 710, training loss: 2.038982629776001, val loss: 2.5855751037597656, val_acc: 0.2727272727272727\n",
      "Epoch 720, training loss: 2.035968542098999, val loss: 2.571882963180542, val_acc: 0.27515151515151515\n",
      "Epoch 730, training loss: 2.0335428714752197, val loss: 2.580299139022827, val_acc: 0.2690909090909091\n",
      "Epoch 740, training loss: 2.0444283485412598, val loss: 2.5865719318389893, val_acc: 0.2678787878787879\n",
      "Epoch 750, training loss: 2.018768787384033, val loss: 2.618952989578247, val_acc: 0.26666666666666666\n",
      "Epoch 760, training loss: 2.0397586822509766, val loss: 2.5844924449920654, val_acc: 0.2703030303030303\n",
      "Epoch 770, training loss: 2.0305979251861572, val loss: 2.5852749347686768, val_acc: 0.2703030303030303\n",
      "Epoch 780, training loss: 2.0123531818389893, val loss: 2.5944199562072754, val_acc: 0.2739393939393939\n",
      "Epoch 790, training loss: 2.025989532470703, val loss: 2.5919976234436035, val_acc: 0.27636363636363637\n",
      "Epoch 800, training loss: 2.017943859100342, val loss: 2.583796977996826, val_acc: 0.2787878787878788\n",
      "Epoch 810, training loss: 2.040191411972046, val loss: 2.595170259475708, val_acc: 0.2678787878787879\n",
      "Epoch 820, training loss: 2.0303988456726074, val loss: 2.6186227798461914, val_acc: 0.27151515151515154\n",
      "Epoch 830, training loss: 2.017702579498291, val loss: 2.578813076019287, val_acc: 0.2787878787878788\n",
      "Epoch 840, training loss: 2.014744281768799, val loss: 2.579101800918579, val_acc: 0.27636363636363637\n",
      "Epoch 850, training loss: 2.000265121459961, val loss: 2.5929009914398193, val_acc: 0.27515151515151515\n",
      "Epoch 860, training loss: 1.986336350440979, val loss: 2.5859546661376953, val_acc: 0.2824242424242424\n",
      "Epoch 870, training loss: 1.9685580730438232, val loss: 2.5807723999023438, val_acc: 0.28\n",
      "Epoch 880, training loss: 1.994273066520691, val loss: 2.6066880226135254, val_acc: 0.2787878787878788\n",
      "Epoch 890, training loss: 1.967023253440857, val loss: 2.588365077972412, val_acc: 0.28\n",
      "Epoch 900, training loss: 1.9832299947738647, val loss: 2.622262716293335, val_acc: 0.2775757575757576\n",
      "Epoch 910, training loss: 1.9617738723754883, val loss: 2.6224241256713867, val_acc: 0.2703030303030303\n",
      "Epoch 920, training loss: 1.9608080387115479, val loss: 2.5996687412261963, val_acc: 0.2921212121212121\n",
      "Epoch 930, training loss: 1.9695576429367065, val loss: 2.6034398078918457, val_acc: 0.27636363636363637\n",
      "Epoch 940, training loss: 1.9889583587646484, val loss: 2.606201410293579, val_acc: 0.2739393939393939\n",
      "Epoch 950, training loss: 1.9770643711090088, val loss: 2.594200611114502, val_acc: 0.2824242424242424\n",
      "Epoch 960, training loss: 1.970813274383545, val loss: 2.583993911743164, val_acc: 0.28\n",
      "Epoch 970, training loss: 1.943861484527588, val loss: 2.6089107990264893, val_acc: 0.28363636363636363\n",
      "Epoch 980, training loss: 1.9538894891738892, val loss: 2.6058878898620605, val_acc: 0.2812121212121212\n",
      "Epoch 990, training loss: 1.9469521045684814, val loss: 2.6352267265319824, val_acc: 0.28\n",
      "Epoch 1000, training loss: 1.9552751779556274, val loss: 2.621621608734131, val_acc: 0.28\n",
      "Epoch 1010, training loss: 1.9462485313415527, val loss: 2.6062700748443604, val_acc: 0.28\n",
      "Epoch 1020, training loss: 1.9331276416778564, val loss: 2.6236724853515625, val_acc: 0.2812121212121212\n",
      "Epoch 1030, training loss: 1.9479230642318726, val loss: 2.60620379447937, val_acc: 0.28484848484848485\n",
      "Epoch 1040, training loss: 1.9241392612457275, val loss: 2.6527678966522217, val_acc: 0.2727272727272727\n",
      "Epoch 1050, training loss: 1.9152095317840576, val loss: 2.6208627223968506, val_acc: 0.27636363636363637\n",
      "Epoch 1060, training loss: 1.9580838680267334, val loss: 2.618867874145508, val_acc: 0.2824242424242424\n",
      "Epoch 1070, training loss: 1.8611724376678467, val loss: 2.622572422027588, val_acc: 0.28606060606060607\n",
      "Epoch 1080, training loss: 1.910915493965149, val loss: 2.616619110107422, val_acc: 0.2824242424242424\n",
      "Epoch 1090, training loss: 1.9284074306488037, val loss: 2.6560938358306885, val_acc: 0.2787878787878788\n",
      "Epoch 1100, training loss: 1.9125468730926514, val loss: 2.6226489543914795, val_acc: 0.2812121212121212\n",
      "Epoch 1110, training loss: 1.9053394794464111, val loss: 2.654205083847046, val_acc: 0.2872727272727273\n",
      "Epoch 1120, training loss: 1.8934804201126099, val loss: 2.618773937225342, val_acc: 0.2775757575757576\n",
      "Epoch 1130, training loss: 1.8820830583572388, val loss: 2.639214038848877, val_acc: 0.2775757575757576\n",
      "Epoch 1140, training loss: 1.9196362495422363, val loss: 2.6424620151519775, val_acc: 0.2775757575757576\n",
      "Epoch 1150, training loss: 1.9613101482391357, val loss: 2.648308277130127, val_acc: 0.28484848484848485\n",
      "Epoch 1160, training loss: 1.925757646560669, val loss: 2.644526720046997, val_acc: 0.2909090909090909\n",
      "Epoch 1170, training loss: 1.8690751791000366, val loss: 2.643805742263794, val_acc: 0.28\n",
      "Epoch 1180, training loss: 1.872698426246643, val loss: 2.6246581077575684, val_acc: 0.28606060606060607\n",
      "Epoch 1190, training loss: 1.872291088104248, val loss: 2.6345229148864746, val_acc: 0.2812121212121212\n",
      "Epoch 1200, training loss: 1.847726821899414, val loss: 2.667898654937744, val_acc: 0.2775757575757576\n",
      "Epoch 1210, training loss: 1.9000012874603271, val loss: 2.674926996231079, val_acc: 0.2775757575757576\n",
      "Epoch 1220, training loss: 1.859209656715393, val loss: 2.6601204872131348, val_acc: 0.28\n",
      "Epoch 1230, training loss: 1.8962011337280273, val loss: 2.637934446334839, val_acc: 0.2921212121212121\n",
      "Epoch 1240, training loss: 1.8757596015930176, val loss: 2.6720566749572754, val_acc: 0.28484848484848485\n",
      "Epoch 1250, training loss: 1.9031566381454468, val loss: 2.67498779296875, val_acc: 0.2884848484848485\n",
      "Epoch 1260, training loss: 1.8447579145431519, val loss: 2.664684772491455, val_acc: 0.28363636363636363\n",
      "Epoch 1270, training loss: 1.8949962854385376, val loss: 2.665189504623413, val_acc: 0.2824242424242424\n",
      "Epoch 1280, training loss: 1.8850418329238892, val loss: 2.6804347038269043, val_acc: 0.28363636363636363\n",
      "Epoch 1290, training loss: 1.847091794013977, val loss: 2.660770893096924, val_acc: 0.2909090909090909\n",
      "Epoch 1300, training loss: 1.8531514406204224, val loss: 2.65850830078125, val_acc: 0.2909090909090909\n",
      "Epoch 1310, training loss: 1.8317921161651611, val loss: 2.6572482585906982, val_acc: 0.2921212121212121\n",
      "Epoch 1320, training loss: 1.840959072113037, val loss: 2.638270616531372, val_acc: 0.28606060606060607\n",
      "Epoch 1330, training loss: 1.852496862411499, val loss: 2.669856071472168, val_acc: 0.2872727272727273\n",
      "Epoch 1340, training loss: 1.8656305074691772, val loss: 2.6710543632507324, val_acc: 0.2824242424242424\n",
      "Epoch 1350, training loss: 1.8471652269363403, val loss: 2.682455062866211, val_acc: 0.2884848484848485\n",
      "Epoch 1360, training loss: 1.8419040441513062, val loss: 2.678507089614868, val_acc: 0.28606060606060607\n",
      "Epoch 1370, training loss: 1.8553422689437866, val loss: 2.6736907958984375, val_acc: 0.2787878787878788\n",
      "Epoch 1380, training loss: 1.8217060565948486, val loss: 2.721101760864258, val_acc: 0.2812121212121212\n",
      "Epoch 1390, training loss: 1.8137767314910889, val loss: 2.687096118927002, val_acc: 0.2884848484848485\n",
      "Epoch 1400, training loss: 1.8687738180160522, val loss: 2.6667590141296387, val_acc: 0.2872727272727273\n",
      "Epoch 1410, training loss: 1.832611322402954, val loss: 2.7031874656677246, val_acc: 0.28363636363636363\n",
      "Epoch 1420, training loss: 1.8645520210266113, val loss: 2.6782517433166504, val_acc: 0.28363636363636363\n",
      "Epoch 1430, training loss: 1.8480339050292969, val loss: 2.6761724948883057, val_acc: 0.2787878787878788\n",
      "Epoch 1440, training loss: 1.8065695762634277, val loss: 2.706780195236206, val_acc: 0.28606060606060607\n",
      "Epoch 1450, training loss: 1.8183951377868652, val loss: 2.6866047382354736, val_acc: 0.2909090909090909\n",
      "Epoch 1460, training loss: 1.8376801013946533, val loss: 2.683013677597046, val_acc: 0.2896969696969697\n",
      "Epoch 1470, training loss: 1.8234039545059204, val loss: 2.7122788429260254, val_acc: 0.28484848484848485\n",
      "Epoch 1480, training loss: 1.8197388648986816, val loss: 2.725597381591797, val_acc: 0.2812121212121212\n",
      "Epoch 1490, training loss: 1.8171213865280151, val loss: 2.7131271362304688, val_acc: 0.28484848484848485\n",
      "Epoch 1500, training loss: 1.810083031654358, val loss: 2.70385479927063, val_acc: 0.2896969696969697\n",
      "Epoch 1510, training loss: 1.8186476230621338, val loss: 2.7167677879333496, val_acc: 0.29454545454545455\n",
      "Epoch 1520, training loss: 1.7894113063812256, val loss: 2.710268020629883, val_acc: 0.28\n",
      "Epoch 1530, training loss: 1.7956825494766235, val loss: 2.6990764141082764, val_acc: 0.2896969696969697\n",
      "Epoch 1540, training loss: 1.790737509727478, val loss: 2.695061206817627, val_acc: 0.29333333333333333\n",
      "Epoch 1550, training loss: 1.833996057510376, val loss: 2.7057783603668213, val_acc: 0.29818181818181816\n",
      "Epoch 1560, training loss: 1.804735541343689, val loss: 2.720461845397949, val_acc: 0.2921212121212121\n",
      "Epoch 1570, training loss: 1.8411355018615723, val loss: 2.718177556991577, val_acc: 0.2921212121212121\n",
      "Epoch 1580, training loss: 1.7998929023742676, val loss: 2.724813938140869, val_acc: 0.2872727272727273\n",
      "Epoch 1590, training loss: 1.8158708810806274, val loss: 2.7038803100585938, val_acc: 0.28606060606060607\n",
      "Epoch 1600, training loss: 1.809161901473999, val loss: 2.735703706741333, val_acc: 0.28484848484848485\n",
      "Epoch 1610, training loss: 1.7673689126968384, val loss: 2.7187693119049072, val_acc: 0.2909090909090909\n",
      "Epoch 1620, training loss: 1.8068040609359741, val loss: 2.7378077507019043, val_acc: 0.2872727272727273\n",
      "Epoch 1630, training loss: 1.8385688066482544, val loss: 2.768829107284546, val_acc: 0.28606060606060607\n",
      "Epoch 1640, training loss: 1.7986425161361694, val loss: 2.712148427963257, val_acc: 0.2957575757575758\n",
      "Epoch 1650, training loss: 1.8264166116714478, val loss: 2.7837283611297607, val_acc: 0.29454545454545455\n",
      "Epoch 1660, training loss: 1.7913470268249512, val loss: 2.745094060897827, val_acc: 0.28484848484848485\n",
      "Epoch 1670, training loss: 1.7904176712036133, val loss: 2.746286153793335, val_acc: 0.2872727272727273\n",
      "Epoch 1680, training loss: 1.780763864517212, val loss: 2.720499277114868, val_acc: 0.2957575757575758\n",
      "Epoch 1690, training loss: 1.7975091934204102, val loss: 2.7563562393188477, val_acc: 0.29333333333333333\n",
      "Epoch 1700, training loss: 1.802331805229187, val loss: 2.72344708442688, val_acc: 0.2872727272727273\n",
      "Epoch 1710, training loss: 1.7622418403625488, val loss: 2.816575527191162, val_acc: 0.2787878787878788\n",
      "Epoch 1720, training loss: 1.796648621559143, val loss: 2.739635944366455, val_acc: 0.2896969696969697\n",
      "Epoch 1730, training loss: 1.7758474349975586, val loss: 2.756903886795044, val_acc: 0.28363636363636363\n",
      "Epoch 1740, training loss: 1.7867460250854492, val loss: 2.782529830932617, val_acc: 0.2812121212121212\n",
      "Epoch 1750, training loss: 1.7369352579116821, val loss: 2.7497787475585938, val_acc: 0.28\n",
      "Epoch 1760, training loss: 1.7882425785064697, val loss: 2.742302417755127, val_acc: 0.2957575757575758\n",
      "Epoch 1770, training loss: 1.7840173244476318, val loss: 2.7791571617126465, val_acc: 0.28484848484848485\n",
      "Epoch 1780, training loss: 1.780068039894104, val loss: 2.768462896347046, val_acc: 0.29454545454545455\n",
      "Epoch 1790, training loss: 1.7680394649505615, val loss: 2.729048252105713, val_acc: 0.2909090909090909\n",
      "Epoch 1800, training loss: 1.7909572124481201, val loss: 2.7730777263641357, val_acc: 0.2872727272727273\n",
      "Epoch 1810, training loss: 1.7636480331420898, val loss: 2.7802579402923584, val_acc: 0.28606060606060607\n",
      "Epoch 1820, training loss: 1.7704719305038452, val loss: 2.7887885570526123, val_acc: 0.2824242424242424\n",
      "Epoch 1830, training loss: 1.7860463857650757, val loss: 2.78313946723938, val_acc: 0.2896969696969697\n",
      "Epoch 1840, training loss: 1.794784426689148, val loss: 2.765237331390381, val_acc: 0.2921212121212121\n",
      "Epoch 1850, training loss: 1.749419093132019, val loss: 2.8119027614593506, val_acc: 0.2896969696969697\n",
      "Epoch 1860, training loss: 1.778232216835022, val loss: 2.7746684551239014, val_acc: 0.28484848484848485\n",
      "Epoch 1870, training loss: 1.7743823528289795, val loss: 2.7717902660369873, val_acc: 0.29333333333333333\n",
      "Epoch 1880, training loss: 1.745143175125122, val loss: 2.7637603282928467, val_acc: 0.2872727272727273\n",
      "Epoch 1890, training loss: 1.7881488800048828, val loss: 2.7946102619171143, val_acc: 0.29333333333333333\n",
      "Epoch 1900, training loss: 1.7319823503494263, val loss: 2.8220374584198, val_acc: 0.2812121212121212\n",
      "Epoch 1910, training loss: 1.7655456066131592, val loss: 2.8012282848358154, val_acc: 0.2824242424242424\n",
      "Epoch 1920, training loss: 1.7119619846343994, val loss: 2.812525749206543, val_acc: 0.2872727272727273\n",
      "Epoch 1930, training loss: 1.7391269207000732, val loss: 2.7987823486328125, val_acc: 0.2909090909090909\n",
      "Epoch 1940, training loss: 1.734960913658142, val loss: 2.7843191623687744, val_acc: 0.2909090909090909\n",
      "Epoch 1950, training loss: 1.732701063156128, val loss: 2.776496171951294, val_acc: 0.2896969696969697\n",
      "Epoch 1960, training loss: 1.7228490114212036, val loss: 2.80007004737854, val_acc: 0.29454545454545455\n",
      "Epoch 1970, training loss: 1.769921064376831, val loss: 2.813689947128296, val_acc: 0.2896969696969697\n",
      "Epoch 1980, training loss: 1.7265676259994507, val loss: 2.797067165374756, val_acc: 0.2921212121212121\n",
      "Epoch 1990, training loss: 1.753538727760315, val loss: 2.7743237018585205, val_acc: 0.29818181818181816\n",
      "Epoch 2000, training loss: 1.693520426750183, val loss: 2.7945210933685303, val_acc: 0.28363636363636363\n",
      "Epoch 2010, training loss: 1.7216928005218506, val loss: 2.8144285678863525, val_acc: 0.2884848484848485\n",
      "Epoch 2020, training loss: 1.7525957822799683, val loss: 2.8071954250335693, val_acc: 0.2921212121212121\n",
      "Epoch 2030, training loss: 1.7287640571594238, val loss: 2.8165464401245117, val_acc: 0.28484848484848485\n",
      "Epoch 2040, training loss: 1.7262111902236938, val loss: 2.788316488265991, val_acc: 0.28484848484848485\n",
      "Epoch 2050, training loss: 1.750309705734253, val loss: 2.810541868209839, val_acc: 0.2909090909090909\n",
      "Epoch 2060, training loss: 1.724877119064331, val loss: 2.829249143600464, val_acc: 0.29333333333333333\n",
      "Epoch 2070, training loss: 1.7404142618179321, val loss: 2.8413991928100586, val_acc: 0.28606060606060607\n",
      "Epoch 2080, training loss: 1.7346655130386353, val loss: 2.794809341430664, val_acc: 0.2884848484848485\n",
      "Epoch 2090, training loss: 1.723047137260437, val loss: 2.8375556468963623, val_acc: 0.2884848484848485\n",
      "Epoch 2100, training loss: 1.6979122161865234, val loss: 2.8107240200042725, val_acc: 0.2884848484848485\n",
      "Epoch 2110, training loss: 1.7069205045700073, val loss: 2.839573860168457, val_acc: 0.2872727272727273\n",
      "Epoch 2120, training loss: 1.7100613117218018, val loss: 2.8211538791656494, val_acc: 0.2993939393939394\n",
      "Epoch 2130, training loss: 1.7194602489471436, val loss: 2.8392367362976074, val_acc: 0.2896969696969697\n",
      "Epoch 2140, training loss: 1.7193821668624878, val loss: 2.8119218349456787, val_acc: 0.2921212121212121\n",
      "Epoch 2150, training loss: 1.7339590787887573, val loss: 2.875930070877075, val_acc: 0.2824242424242424\n",
      "Epoch 2160, training loss: 1.7201690673828125, val loss: 2.844116449356079, val_acc: 0.2787878787878788\n",
      "Epoch 2170, training loss: 1.7101216316223145, val loss: 2.903204917907715, val_acc: 0.28606060606060607\n",
      "Epoch 2180, training loss: 1.7274545431137085, val loss: 2.859842300415039, val_acc: 0.2896969696969697\n",
      "Epoch 2190, training loss: 1.6977016925811768, val loss: 2.890084743499756, val_acc: 0.2787878787878788\n",
      "Epoch 2200, training loss: 1.73140549659729, val loss: 2.8409292697906494, val_acc: 0.28606060606060607\n",
      "Epoch 2210, training loss: 1.7127528190612793, val loss: 2.872680187225342, val_acc: 0.28\n",
      "Epoch 2220, training loss: 1.748894214630127, val loss: 2.8371505737304688, val_acc: 0.2872727272727273\n",
      "Epoch 2230, training loss: 1.748181700706482, val loss: 2.8559865951538086, val_acc: 0.2957575757575758\n",
      "Epoch 2240, training loss: 1.7328325510025024, val loss: 2.8752028942108154, val_acc: 0.2872727272727273\n",
      "Epoch 2250, training loss: 1.726987600326538, val loss: 2.8829691410064697, val_acc: 0.2872727272727273\n",
      "Epoch 2260, training loss: 1.7078113555908203, val loss: 2.8522117137908936, val_acc: 0.29454545454545455\n",
      "Epoch 2270, training loss: 1.7001299858093262, val loss: 2.8926141262054443, val_acc: 0.2872727272727273\n",
      "Epoch 2280, training loss: 1.6866369247436523, val loss: 2.840313673019409, val_acc: 0.2824242424242424\n",
      "Epoch 2290, training loss: 1.7051904201507568, val loss: 2.896418333053589, val_acc: 0.28606060606060607\n",
      "Epoch 2300, training loss: 1.7252634763717651, val loss: 2.8611268997192383, val_acc: 0.28484848484848485\n",
      "Epoch 2310, training loss: 1.7259150743484497, val loss: 2.8565316200256348, val_acc: 0.28606060606060607\n",
      "Epoch 2320, training loss: 1.7350596189498901, val loss: 2.868812084197998, val_acc: 0.2884848484848485\n",
      "Epoch 2330, training loss: 1.7059544324874878, val loss: 2.931652545928955, val_acc: 0.2884848484848485\n",
      "Epoch 2340, training loss: 1.7144763469696045, val loss: 2.8423259258270264, val_acc: 0.2884848484848485\n",
      "Epoch 2350, training loss: 1.7152847051620483, val loss: 2.8935163021087646, val_acc: 0.29333333333333333\n",
      "Epoch 2360, training loss: 1.6901888847351074, val loss: 2.877272367477417, val_acc: 0.2909090909090909\n",
      "Epoch 2370, training loss: 1.6752959489822388, val loss: 2.9090888500213623, val_acc: 0.2824242424242424\n",
      "Epoch 2380, training loss: 1.6960086822509766, val loss: 2.9059650897979736, val_acc: 0.2812121212121212\n",
      "Epoch 2390, training loss: 1.73421049118042, val loss: 2.8699371814727783, val_acc: 0.2787878787878788\n",
      "Epoch 2400, training loss: 1.6957218647003174, val loss: 2.8972973823547363, val_acc: 0.28363636363636363\n",
      "Epoch 2410, training loss: 1.7049494981765747, val loss: 2.8789427280426025, val_acc: 0.2896969696969697\n",
      "Epoch 2420, training loss: 1.6916104555130005, val loss: 2.901474952697754, val_acc: 0.2896969696969697\n",
      "Epoch 2430, training loss: 1.6809794902801514, val loss: 2.914273738861084, val_acc: 0.28\n",
      "Epoch 2440, training loss: 1.6892508268356323, val loss: 2.9277145862579346, val_acc: 0.2896969696969697\n",
      "Epoch 2450, training loss: 1.6322022676467896, val loss: 2.9010090827941895, val_acc: 0.2872727272727273\n",
      "Epoch 2460, training loss: 1.6784570217132568, val loss: 2.882504940032959, val_acc: 0.28363636363636363\n",
      "Epoch 2470, training loss: 1.6862248182296753, val loss: 2.934528112411499, val_acc: 0.28606060606060607\n",
      "Epoch 2480, training loss: 1.668012022972107, val loss: 2.9361581802368164, val_acc: 0.2909090909090909\n",
      "Epoch 2490, training loss: 1.6693880558013916, val loss: 2.88630747795105, val_acc: 0.2909090909090909\n",
      "Epoch 2500, training loss: 1.6882717609405518, val loss: 2.8627655506134033, val_acc: 0.2884848484848485\n",
      "Epoch 2510, training loss: 1.7231805324554443, val loss: 2.984675168991089, val_acc: 0.2739393939393939\n",
      "Epoch 2520, training loss: 1.685258150100708, val loss: 2.887990951538086, val_acc: 0.2872727272727273\n",
      "Epoch 2530, training loss: 1.6774102449417114, val loss: 2.888336658477783, val_acc: 0.2957575757575758\n",
      "Epoch 2540, training loss: 1.6826376914978027, val loss: 2.9496657848358154, val_acc: 0.28484848484848485\n",
      "Epoch 2550, training loss: 1.6983096599578857, val loss: 2.945435047149658, val_acc: 0.2872727272727273\n",
      "Epoch 2560, training loss: 1.7159795761108398, val loss: 2.8718886375427246, val_acc: 0.2909090909090909\n",
      "Epoch 2570, training loss: 1.6666842699050903, val loss: 2.9222238063812256, val_acc: 0.2921212121212121\n",
      "Epoch 2580, training loss: 1.7062023878097534, val loss: 2.894773006439209, val_acc: 0.2896969696969697\n",
      "Epoch 2590, training loss: 1.681929111480713, val loss: 2.8894379138946533, val_acc: 0.28363636363636363\n",
      "Epoch 2600, training loss: 1.7112966775894165, val loss: 2.964914083480835, val_acc: 0.2993939393939394\n",
      "Epoch 2610, training loss: 1.712486743927002, val loss: 2.981175661087036, val_acc: 0.29333333333333333\n",
      "Epoch 2620, training loss: 1.703771710395813, val loss: 2.9404408931732178, val_acc: 0.2993939393939394\n",
      "Epoch 2630, training loss: 1.6952725648880005, val loss: 2.914520502090454, val_acc: 0.2957575757575758\n",
      "Epoch 2640, training loss: 1.697681188583374, val loss: 2.9950666427612305, val_acc: 0.2872727272727273\n",
      "Epoch 2650, training loss: 1.7019630670547485, val loss: 2.9242160320281982, val_acc: 0.296969696969697\n",
      "Epoch 2660, training loss: 1.691104769706726, val loss: 2.966600179672241, val_acc: 0.2824242424242424\n",
      "Epoch 2670, training loss: 1.7001864910125732, val loss: 2.9331438541412354, val_acc: 0.2921212121212121\n",
      "Epoch 2680, training loss: 1.708818793296814, val loss: 2.8937265872955322, val_acc: 0.28484848484848485\n",
      "Epoch 2690, training loss: 1.6634016036987305, val loss: 2.9906697273254395, val_acc: 0.28484848484848485\n",
      "Epoch 2700, training loss: 1.688738465309143, val loss: 2.912260055541992, val_acc: 0.29818181818181816\n",
      "Epoch 2710, training loss: 1.6377907991409302, val loss: 3.0013978481292725, val_acc: 0.2824242424242424\n",
      "Epoch 2720, training loss: 1.681665301322937, val loss: 2.9503464698791504, val_acc: 0.2921212121212121\n",
      "Epoch 2730, training loss: 1.6974272727966309, val loss: 2.9446120262145996, val_acc: 0.29333333333333333\n",
      "Epoch 2740, training loss: 1.7194159030914307, val loss: 2.9172592163085938, val_acc: 0.28363636363636363\n",
      "Epoch 2750, training loss: 1.641722321510315, val loss: 2.9856836795806885, val_acc: 0.2812121212121212\n",
      "Epoch 2760, training loss: 1.659865140914917, val loss: 2.9262890815734863, val_acc: 0.2884848484848485\n",
      "Epoch 2770, training loss: 1.6506474018096924, val loss: 2.9694316387176514, val_acc: 0.28484848484848485\n",
      "Epoch 2780, training loss: 1.6590131521224976, val loss: 2.9877092838287354, val_acc: 0.28363636363636363\n",
      "Epoch 2790, training loss: 1.7061378955841064, val loss: 2.9843995571136475, val_acc: 0.28363636363636363\n",
      "Epoch 2800, training loss: 1.661309838294983, val loss: 2.968203067779541, val_acc: 0.2872727272727273\n",
      "Epoch 2810, training loss: 1.645263671875, val loss: 3.0098886489868164, val_acc: 0.2872727272727273\n",
      "Epoch 2820, training loss: 1.6855647563934326, val loss: 2.8838894367218018, val_acc: 0.2909090909090909\n",
      "Epoch 2830, training loss: 1.6623860597610474, val loss: 3.0167744159698486, val_acc: 0.2909090909090909\n",
      "Epoch 2840, training loss: 1.7175339460372925, val loss: 2.941244125366211, val_acc: 0.29818181818181816\n",
      "Epoch 2850, training loss: 1.6622182130813599, val loss: 2.9432990550994873, val_acc: 0.2812121212121212\n",
      "Epoch 2860, training loss: 1.6566541194915771, val loss: 2.9956448078155518, val_acc: 0.28606060606060607\n",
      "Epoch 2870, training loss: 1.6702146530151367, val loss: 2.993109941482544, val_acc: 0.29454545454545455\n",
      "Epoch 2880, training loss: 1.6414629220962524, val loss: 2.977027654647827, val_acc: 0.2896969696969697\n",
      "Epoch 2890, training loss: 1.6429213285446167, val loss: 2.9903640747070312, val_acc: 0.2872727272727273\n",
      "Epoch 2900, training loss: 1.6692214012145996, val loss: 3.00498628616333, val_acc: 0.27636363636363637\n",
      "Epoch 2910, training loss: 1.6567258834838867, val loss: 2.999797821044922, val_acc: 0.29333333333333333\n",
      "Epoch 2920, training loss: 1.6804496049880981, val loss: 3.000485897064209, val_acc: 0.29818181818181816\n",
      "Epoch 2930, training loss: 1.6473828554153442, val loss: 2.9559550285339355, val_acc: 0.2921212121212121\n",
      "Epoch 2940, training loss: 1.6340556144714355, val loss: 3.017171621322632, val_acc: 0.28484848484848485\n",
      "Epoch 2950, training loss: 1.67387056350708, val loss: 3.004108428955078, val_acc: 0.2909090909090909\n",
      "Epoch 2960, training loss: 1.679870843887329, val loss: 3.0030107498168945, val_acc: 0.2872727272727273\n",
      "Early stopping at epoch 1961 with validation accuracy 0.304242\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gcnpro_models = {}\n",
    "for name, data in dataset.items():\n",
    "    print(f\"Training GCNPro with {name}\")\n",
    "\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj = to_scipy_sparse_matrix(data.edge_index, num_nodes=num_nodes)\n",
    "    adj_norm = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    features = data.x\n",
    "    labels = data.y\n",
    "\n",
    "    nclass = labels.max().item()+1\n",
    "    gcnpro = GCNPro(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "    gcnpro = gcnpro.to(device)\n",
    "    data = data.to(device)\n",
    "    adj_norm = adj_norm.to(device)\n",
    "    train(gcnpro, data, adj_norm, lr=0.0005, epochs=5000, patience=1000)\n",
    "\n",
    "    gcnpro_models[name] = gcnpro\n",
    "\n",
    "    print(\"=\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GCNPro with data_clip\n",
      "Test set results: loss= 2.9614 accuracy= 0.1685\n",
      "====================\n",
      "Testing GCNPro with data_cnn\n",
      "Test set results: loss= 3.4490 accuracy= 0.2138\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# test models\n",
    "for name, model in gcnpro_models.items():\n",
    "    print(f\"Testing GCNPro with {name}\")\n",
    "    data = dataset[name]\n",
    "\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj = to_scipy_sparse_matrix(data.edge_index, num_nodes=num_nodes)\n",
    "    adj_norm = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    test(model, data, adj_norm)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
